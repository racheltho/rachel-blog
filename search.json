[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am cofounder of fast.ai, which created one of the most popular deep learning courses in the world. I have a PhD in mathematics and was previously the founding director of the Center for Applied Data Ethics at the University of San Francisco.\nI live in Queensland, Australia with my husband and daughter. My interests include mathematical biology, data ethics, and machine learning. You can find my old blog posts at fast.ai and medium.\n\nMathematical Biology and AI in Medicine\n\n\nA Mathematical Model of Glutathione Metabolism, Journal of Theoretical Biology and Medical Modeling\nMedicine’s Machine Learning Problem, Boston Review\nEarned a PhD in mathematics from Duke University\nHoward Hughes Medical Institute Fellowship\nkeynote speaker at Stanford’s Artificial Intelligence in Medicine symposium\n\n\nData Ethics\n\n\nProfessor of Practice at Queensland University of Technology Centre for Data Science\nFounding director of Center for Applied Data Ethics (CADE) at University of San Francisco\nReliance on Metrics is a Fundamental Challenge for AI, Patterns. Optimizing metrics is a central aspect of most current AI approaches, yet overemphasizing metrics leads to manipulation, gaming, and a myopic short-term focus.\nCreated and taught data ethics course\nWrote ethics chapter of best-selling book,\nWrote book chapters for:\n\n97 Things About Ethics Everyone in Data Science Should Know\nDeep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD\nRedesigning AI\n\n\n\nMachine Learning and Data Science\n\n\nCo-founder of course.fast.ai\nDesigned and taught graduate level courses on Natural Language Processing and Computational Linear Algebra in the USF Masters of Data Science program\nThe New Era in NLP, Keynote at SciPy (Scientific Python) Conference 2019\nKeynote at ICML AutoML workshop, based on my popular series of AutoML posts\nBeginner friendly workshop on Word Embeddings (such as Word2Vec)\nForbes 20 Incredible Women in AI\nFeatured in book Women Tech Founders on the Rise\nEarly data scientist and software engineer at Uber\n\n\n\n\n\nA few events I’ve spoken at"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "science, data ethics, education",
    "section": "",
    "text": "My family’s unlikely homeschooling journey\n\n\n\n\n\nPrior to 2020, we never expected to homeschool, and now we have committed to it long-term.\n\n\n\n\n\n\nSep 6, 2022\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nQualitative humanities research is crucial to AI\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nFollowing the thread of any quantitative issue leads to qualitative questions.\n\n\n\n\n\n\nJun 1, 2022\n\n\nLouisa Bartolow and Rachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nAI Harms are Societal, Not Just Individual\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nMuch like pollution, the harms caused by algorithmic systems are often collective and communal.\n\n\n\n\n\n\nMay 17, 2022\n\n\nRachel Thomas and Louisa Bartolo\n\n\n\n\n\n\n  \n\n\n\n\nThere’s no such thing as not a math person\n\n\n\n\n\n\n\nadvice\n\n\neducation\n\n\n\n\nMany cultural factors, misconceptions, stereotypes, and obstacles turn people off to math.\n\n\n\n\n\n\nMar 15, 2022\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nMedicine is Political\n\n\n\n\n\n\n\nscience\n\n\nethics\n\n\n\n\nFrom covid-19 to HIV research to the long history of wrongly assuming women’s illnesses are psychosomatic, we have seen again and again that medicine, like all science, is political.\n\n\n\n\n\n\nOct 12, 2021\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\n11 Short Videos About AI Ethics\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nShort videos about machine learning ethics to watch and share.\n\n\n\n\n\n\nAug 16, 2021\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nEssential Work-From-Home Advice: Cheap and Easy Ergonomic Setups\n\n\n\n\n\n\n\nadvice\n\n\nwork\n\n\n\n\nYou can permanently damage your back, neck, and wrists from working without an ergonomic setup. Learn how to create one for less.\n\n\n\n\n\n\nAug 6, 2020\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nThe problem with metrics is a big problem for AI\n\n\n\n\n\n\n\nmachine learning\n\n\nethics\n\n\n\n\nUnthinkingly optimizing metrics can lead to a variety of grave harms, and what most current AI approaches do is to optimize metrics.\n\n\n\n\n\n\nSep 24, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\n8 Things You Need to Know about Surveillance\n\n\n\n\n\n\n\nethics\n\n\n\n\nHow surveillance makes us less safe\n\n\n\n\n\n\nAug 7, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nTech’s Long Hours Are Discriminatory and Counterproductive\n\n\n\n\n\n\n\ninclusion\n\n\nwork\n\n\n\n\nWhy working longer doesn’t work.\n\n\n\n\n\n\nFeb 19, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nFive Things That Scare Me About AI\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nAn AI-powered future is already here, and some of the consequences are scarier than you may realize.\n\n\n\n\n\n\nJan 29, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nGoogle’s AutoML: Cutting Through the Hype\n\n\n\n\n\n\n\ntechnical\n\n\nmachine learning\n\n\n\n\nAnalyzing Google’s claims that we all need custom neural networks\n\n\n\n\n\n\nJul 23, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nAn Opinionated Introduction to AutoML and Neural Architecture Search\n\n\n\n\n\n\n\ntechnical\n\n\nmachine learning\n\n\n\n\nAutoML is being heavily hyped– but would AugmentedML be a better approach?\n\n\n\n\n\n\nJul 16, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nWhat do machine learning practitioners actually do?\n\n\n\n\n\n\n\nmachine learning\n\n\nadvice\n\n\nwork\n\n\n\n\nMachine learning is an in-demand field, but there are misconceptions about what the work entails in practice.\n\n\n\n\n\n\nJul 12, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Deep Learning for Tabular Data\n\n\n\n\n\n\n\nmachine learning\n\n\ntechnical\n\n\n\n\nDeep learning is not just for images and text.\n\n\n\n\n\n\nApr 29, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nHow (and why) to create a good validation set\n\n\n\n\n\n\n\nmachine learning\n\n\ntechnical\n\n\n\n\nAvoid failures from poorly chosen validation sets.\n\n\n\n\n\n\nNov 13, 2017\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nDiversity Washing Makes Things Worse\n\n\n\n\n\n\n\ninclusion\n\n\n\n\nShallow, showy diversity efforts aren’t just ineffective, they are actively harmful.\n\n\n\n\n\n\nDec 7, 2015\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nIf you think women in tech is just a pipeline problem, you haven’t been paying attention\n\n\n\n\n\n\n\ninclusion\n\n\n\n\nIt doesn’t matter how many girls you teach to code if you keep driving adult women out of the tech industry.\n\n\n\n\n\n\nJul 27, 2015\n\n\nRachel Thomas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html",
    "href": "posts/2015-07-27-not-pipeline/index.html",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "",
    "text": "This article has been translated into Spanish at Matajuegos and into Russian by Dmitry Si.\nAccording to the Harvard Business Review, 41% of women working in tech eventually end up leaving the field (compared to just 17% of men), and I can understand why…\nI first learned to code at age 16, and am now in my 30s. I have a math PhD from Duke. I still remember my pride in a “knight’s tour” algorithm that I wrote in C++ in high school; the awesome mind warp of an interpreter that can interpret itself (a Scheme course my first semester of college); my fascination with numerous types of matrix factorizations in C in grad school; and my excitement about relational databases and web scrapers in my first real job.\nOver a decade after I first learned to program, I still loved algorithms, but felt alienated and depressed by tech culture. While at a company that was a particularly poor culture fit, I was so unhappy that I hired a career counselor to discuss alternative career paths. Leaving tech would have been devastating, but staying was tough."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#work-hard-play-hard",
    "href": "posts/2015-07-27-not-pipeline/index.html#work-hard-play-hard",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Work hard, play hard",
    "text": "Work hard, play hard\nI’m not the stereotypical male programmer in his early 20s looking to “work hard, play hard”. I do work hard, but I’d rather wake up early than stay up late, and I was already thinking ahead to when my husband and I would need to coordinate our schedules with daycare drop-offs and pick-ups. Kegerators and ping pong tables don’t appeal to me. I’m not aggressive enough to thrive in a combative work environment. Talking to other female friends working in tech, I know that I’m not alone in my frustrations.\nWhen researcher Kieran Snyder interviewed 716 women who left tech after an average tenure of 7 years, almost all of them said they liked the work itself, but most were unhappy with the work environment. In NSF-funded research, Nadya Fouad surveyed 5,300 women who had earned engineering degrees (of all types) over the last 50 years, and 38% of them were no longer working as engineers. Fouad summarized her findings on why they leave with “It’s the climate, stupid!”\nThis is a huge, unnecessary, and expensive loss of talent in a field facing a supposed talent shortage. Given that tech is currently one of the major drivers of the US economy, this impacts everyone. Any tech company struggling to hire and retain as many employees as they need should particularly care about addressing this problem."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#your-company-is-not-a-meritocracy-and-you-are-not-gender-blind",
    "href": "posts/2015-07-27-not-pipeline/index.html#your-company-is-not-a-meritocracy-and-you-are-not-gender-blind",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Your company is NOT a meritocracy and you are NOT “gender-blind”",
    "text": "Your company is NOT a meritocracy and you are NOT “gender-blind”\nYou don’t know if you’re color-blind without testing either\nNobody wants to think of themselves as being sexist. However, a number of studies have shown that identical job applications or resumes are evaluated differently based on whether they are labeled with a male or female name. When men and women read identical scripts containing entrepreneurial pitches or salary negotiations, they are evaluated differently. Both men and women have been shown to have these biases. These biases occur unconsciously and without intention or malice.\nHere is a sampling of just a few of the studies on unconscious gender bias:\n\nInvestors preferred entrepreneurial ventures pitched by a man than an identical pitch from a woman by a rate of 68% to 32% in a study conducted jointly by HBS, Wharton, and MIT Sloan. “Male-narrated pitches were rated as more persuasive, logical and fact-based than were the same pitches narrated by a female voice.”\nIn a randomized, double-blind study by Yale researchers, science faculty at 6 major institutions evaluated applications for a lab manager position. Applications randomly assigned a male name were rated as significantly more competent and hirable and offered a higher starting salary and more career mentoring, compared to identical applications assigned female names.\nWhen men and women negotiated a job offer by reading identical scripts for a Harvard and CMU study, women who asked for a higher salary were rated as being more difficult to work with and less nice, but men were not perceived negatively for negotiating.\nPsychology faculty were sent CVs for an applicant (randomly assigned male or female name), and both men and women were significantly more likely to hire a male applicant than a female applicant with an identical record.\nIn 248 performance reviews of high-performers in tech, negative personality criticism (such as abrasive, strident, or irrational) showed up in 85% of reviews for women and just 2% of reviews for men. It is ridiculous to assume that 85% of women have personality problems and that only 2% of men do.\n\nMost concerningly, a study from Yale researchers shows that perceiving yourself as objective is actually correlated with showing even more bias. The mere desire to not be biased is not enough to overcome decades of cultural conditioning and can even lend more credence to post-hoc justifications. Acknowledging that you have biases that conflict with your values does not make you a bad person. It’s a natural result of our culture. The important thing is to find ways to eliminate them. Blindly believing your company is a meritocracy not only does not make it so, but will actually make it even harder to address implicit bias.\nBias is typically justified post-hoc. Our initial subconscious impression of the female applicant is negative, and then we find logical reasons to justify it. For instance, in the above study by Yale researchers if the male applicant for police chief had more street smarts and the female applicant had more formal education, evaluators decided that street smarts were the most important trait, and if the names were reversed, evaluators decided that formal education was the most important trait."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#good-news-and-bad-news",
    "href": "posts/2015-07-27-not-pipeline/index.html#good-news-and-bad-news",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Good News and Bad News",
    "text": "Good News and Bad News\n\nThe Bad News…\nBecause of the high attrition rate for women working in tech, teaching more girls and women to code is not enough to solve this problem. Because of the above well-documented differences in how men and women are perceived, training women to negotiate better and be more assertive is also not enough to solve this problem. Female voices are perceived as less logical and less persuasive than male voices. Women are perceived negatively for being too assertive. If tech culture is going to change, everyone needs to change, especially men and most especially leaders.\nThe professional and emotional costs to women for speaking out about discrimination can be large (in terms of retaliation, being perceived as less employable or difficult to work with, or companies then seeking to portray them as poor performers). I know a number of female software engineers who will privately share stories of sexism with trusted friends that we are not willing to share publicly because of the risk. This is why it is important to proactively address this issue. There is more than enough published research and personal stories from those who have chosen to publicly share to confirm that this is a widespread issue in the tech industry.\n\n\n…and the Good News\nChange is possible. Although these are schools and not tech companies, Harvey Mudd and Harvard Business School provide inspiring case studies. Strong leaders at both schools enacted sweeping changes to address previously male-centric cultures. Harvey Mudd has raised the percentage of computer science majors that are women to 40% (the national average is 18%). The top 5% of Harvard Business School graduates rose from being approximately 20% women to closer to 40% and the GPA gap between men and women closed, all within one year of making a number of comprehensive, structural changes."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#so-what-can-we-do-about-it",
    "href": "posts/2015-07-27-not-pipeline/index.html#so-what-can-we-do-about-it",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "So What Can We Do About It?",
    "text": "So What Can We Do About It?\nThese recommendations on what companies could do to improve their cultures are based on a mix of research and personal experience. My goal is to have a positive focus, and I would love it if you walked away with at least one concrete goal for making constructive change at your company.\n\nTrain managers\nIt is very common at tech start-ups to promote talented engineers to management without providing them with any management training or oversight, particularly at rapidly growing companies where existing leadership is stretched thin. These new managers are often not aware of any of the research on motivation, human psychology, or bias. Untrained, unsupervised managers cause more harm to women than men, although regardless, all employees would benefit from new managers receiving training, mentorship, and supervision.\n\n\nFormalize hiring and promotion criteria\nIn the Yale study mentioned above regarding applicants for police chief, getting participants to formalize their hiring criteria before they looked at applications (i.e. deciding if formal education or street smarts was more important) reduced bias. I was once on a team where the hiring criteria were amorphous and where the manager frequently overrode majority votes by the team because of “gut feeling”. It seemed like unconscious bias played a large role in decisions, but because of our haphazard approach to hiring, there was no way of truly knowing.\n\n\nLeaders, speak up and act in concrete ways\nLeadership sets the values and culture for a company, so the onus is on them to make it clear that they value diversity. Younger engineers and managers will follow their perceptions of what executives value. In the cases of positive change at Harvey Mudd and Harvard Business School, leadership at the top was spearheading these initiatives. Intel is going to begin tying executives’ compensation to whether they achieve diversity goals on their teams. As Kelly Shuster, director for the Denver chapter of Women Who Code has pointed out, leaders have to get rid of employees who engage in sexist or racist behavior. Otherwise, the company is at risk of losing talented employees, and is sending a message to all employees that discrimination is okay.\n\n\nDon’t rely on self-nominations or self-evaluations\nThere is a well-documented confidence gap between men and women. Don’t rely on people nominating themselves for promotions or to get the most interesting projects, since women are less likely to put themselves forward. Google relies on employees nominating themselves for promotions and data revealed that women were much less likely to do so (and thus much less likely to receive promotions). When senior women began hosting workshops encouraging women to nominate themselves, the number of women at Google receiving promotions increased. Groups are more likely to pick male leaders because of their over-confidence, compared to more qualified women who are less confident. Don’t rely heavily on self-evaluations in performance scoring. Women perceive their abilities as being worse than they are, whereas men have an inflated sense of their abilities.\n\n\nFormally audit employee data\nConfirm that men and women with the same qualifications are earning the same amount and that they are receiving promotions and raises at similar rates (and if not, explore why). Make sure that gendered criticism (such as calling a woman strident or abrasive) is not used in performance reviews. The trend of tech companies releasing their diversity statistics is a good one, but given the high industry attrition rate for women, they should also start releasing their retention rates broken down by gender. I would like to see companies release statistics on the rates at which women are given promotions or raises compared to men, and how performance evaluation scores compare between men and women. By publicly sharing data, companies can hold themselves accountable and can track changes over time.\n\n\nDon’t emphasize face time\nA culture that rewards facetime and encourages people to regularly stay late or eat dinner at the office puts employees with families at a disadvantage (particularly mothers), and research shows that working excess hours does not actually improve productivity in the long-term since workers begin to experience burn out after just a few weeks. Furthermore, when employees burn out and quit, the cost of recruiting and hiring a new employee is typically 20% of the annual salary for that position.\n\n\nCreate a collaborative environment\nStanford research studies document that women are more likely to dislike competitive environments compared to men and are more likely to select out of them, regardless of their ability. Given that women are perceived negatively for being too assertive, it is tougher for women to succeed in a highly aggressive environment as well. Men who speak up more than their peers are rewarded with 10% higher ratings, whereas women who speak up more are punished with 14% lower ratings. Creating a competitive culture where people must fight for their ideas makes it much tougher for women to succeed.\n\n\nOffer maternity leave\nOver 10% of the 716 women who left tech in Kieran Snyder’s research left because of inadequate maternity leave. Several were pressured to return from leave early or to be on call while on leave. These women did not want to be stay-at-home-parents, they just wanted to recover after giving birth. Just as you would not pressure someone to return to work without recovery time after a major surgery, women need time to physically heal after delivering a baby. When Google increased paid maternity leave from 12 weeks to 18 weeks, the number of new moms who quit Google dropped by 50%."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#some-final-thoughts",
    "href": "posts/2015-07-27-not-pipeline/index.html#some-final-thoughts",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Some final thoughts…",
    "text": "Some final thoughts…\n\nA note on racial bias\nThere is a huge amount of research on unconscious racial bias, and tech companies need to address this issue. As Nichole Sanchez, VP of Social Impact at GitHub, describes, calls for diversity are often solely about adding more white women, which is deeply problematic. Racial bias adds another intersectional dimension to the discrimination that women of color experience. In interviews with 60 women of color who work in STEM research, 100% of them had experienced discrimination, and the particular negative stereotypes they faced differed depending on their race. A resume with a traditionally African-American sounding name is less likely to be called for an interview than the same resume with a traditionally white sounding name. I do not have the personal experience to speak about this topic and instead encourage you to read these blog posts and articles by and about tech workers of color on the challenges they’ve faced: Erica Joy (Slack engineer, former Google engineer), Justin Edmund (designer, Pinterest’s 7th employee), Aston Motes (Engineer, Dropbox’s 1st employee), and Angelica Coleman (developer advocate at Zendesk, formerly at Dropbox)."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#now",
    "href": "posts/2015-07-27-not-pipeline/index.html#now",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Now",
    "text": "Now\nI’m currently teaching software development at all-women Hackbright Academy, a job that I love and that suits me perfectly. I want all women to have the opportunity (and I mean truly have the opportunity, without implicit or explicit discrimination) to learn how to program — knowing software development provides so many career and financial possibilities; it’s intellectually rewarding and fun; and being a creator is deeply satisfying. Although I know many women with frustrating experiences of sexism, I also know women who have found companies where they’re happily thriving. I’m glad for the attention tech’s diversity problem has been receiving and I am hopeful about continued change.\nThanks for review, edits, and discussion to: Jeremy Howard and Angie Chang.\nI do more research and further develop the ideas in this post in my later posts: on how showy, shallow diversity strategies make things worse; on bullshit diversity initiatives and some better ideas; and the research on how women are leaving tech because they can’t advance in their careers."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html",
    "href": "posts/2015-12-07-diversity-washing/index.html",
    "title": "Diversity Washing Makes Things Worse",
    "section": "",
    "text": "It is painful watching tech companies known to be bad environments for women and people of color make shallow, showy attempts to rebrand themselves as valuing diversity. Perhaps you’re thinking, Any effort towards diversity is a good thing, and there’s no harm in trying, right? Wrong. This is not just a triumph of style over substance; these efforts can harm the people they are purporting to help. For instance, research shows that many diversity programs reduce the number of Black women and Black men in management; diversity structures cause people to be less likely to believe women and people of color; and some forms of unconscious bias training increase bias.\nSince the following points have been covered thoroughly elsewhere, I’m going to take it as a given that:\n\nTech has a diversity problem\nIt’s not just the pipeline (really)\nMeritocracy is a myth\nDiversity is a good thing\n\n(If you’re unfamiliar with these ideas, please read the linked articles; I highly recommend them.) I very much want to see these problems fixed, but they need more than just a coat of PR-friendly paint. Any successful effort towards diversity and inclusion will need to involve comprehensive changes, ongoing self-reflection, and tackling hard problems, not just superficial, high-publicity, quick fixes.\nThe rest of this post will dig into what the research shows about ways that diversity programs can backfire. My next article will suggest some ideas for effective programs."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#the-danger-of-diversity-washing",
    "href": "posts/2015-12-07-diversity-washing/index.html#the-danger-of-diversity-washing",
    "title": "Diversity Washing Makes Things Worse",
    "section": "The danger of diversity washing",
    "text": "The danger of diversity washing\nResearchers from U of Washington, UCLA, and UCSB showed that the mere presence of diversity policies, diversity training, and diversity awards cause white people to be less likely to believe racial discrimination exists and cause men to be less likely to believe gender discrimination exists, despite other data and evidence. Participants in one study read a New York Times article published as a class action lawsuit for gender discrimination against pharmaceutical giant Novartis went to trial. The twist was that half the participants were shown an article that included a sentence stating that Working Mother magazine had chosen Novartis as one of the 100 best companies in the USA. This sentence was omitted from the article for the other half. Those that read the sentence about the_Working Mother_ accolade were less likely to believe that the female employees had a valid case against Novartis, even though the rest of the article remained the same.\nThe researchers conducted 6 variations of the study. In one version, white people read either a diversity statement, or a mission statement, for a fictional company. They were then shown data on comparative promotion rates by race, as well as an article about a Black employee filing suit for racial discrimination. Participants who had read the diversity statement were less likely to believe that discrimination had occurred and rated the Black employee more negatively (compared to those that read the mission statement, which did not mention diversity), even when the data showed clear racial differences. Other versions of the study provided differing data on hiring rates and salaries. In all versions, the presence of a diversity structure (such as diversity policies, diversity training, or diversity awards) caused white people to be less likely to believe racial discrimination and caused men to be less likely to believe gender discrimination.\nThis shows that the presence of diversity programs can hurt women and people of color by creating what the study’s authors call an illusion of fairness. Because of the “diversity branding”, people are less likely to believe that discrimination exists at that company, regardless of what the data shows. So the next time you see a tech company announce their shiny new diversity initiative with much fanfare, consider one impact: that the negative accounts of women and people of color that work at these companies will be disregarded even more often. This belief that the existence of diversity initiatives equals equality is just one way that such efforts can backfire. Next, let’s look at some real world data from the outcomes of diversity programs (it’s not what you would hope)."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#what-does-data-from-diversity-programs-at-over-700-companies-reveal",
    "href": "posts/2015-12-07-diversity-washing/index.html#what-does-data-from-diversity-programs-at-over-700-companies-reveal",
    "title": "Diversity Washing Makes Things Worse",
    "section": "What does data from diversity programs at over 700 companies reveal?",
    "text": "What does data from diversity programs at over 700 companies reveal?\nHarvard professor Frank Dobbin led a team of sociologists in reviewing data from 708 companies to evaluate 7 different approaches to trying to increase the share of White women, Black women, and Black men in management, and found that many programs were ineffective and some diversity efforts even made things worse. Programs that targeted stereotypes through education and feedback, such as diversity training, were the least effective, and in some cases reduced diversity. The study found that diversity training was followed by a 7% decline in the proportion of Black women in management. Diversity evaluations of managers were followed by an 8% decline in Black men in management, although a 6% increase for White women. This is a particular issue when tech diversity efforts are often limited only to recruiting White women.\nThe most effective programs were “responsibility structures” such as diversity committees, diversity staff positions, and affirmative action plans. Professor Dobbin stated that “if no one is specifically charged with the task of increasing diversity, then the buck inevitably gets passed ad infinitum. To increase diversity, executives must treat it like any other business goal.” Networking and mentoring produced modest positive effects. Diversity training was one of the least effective approaches. Dobbin says that “even with best practices, you’re not going to get much of an effect. It doesn’t change what happens at work.”\nTogether with Alexandra Kalev of Tel Aviv University, Dobbin later expanded the research to 803 companies, and to include Asian and Hispanic employees (in addition to Black and White employees) in the aptly titled study “Try and Make Me!: Why Corporate Diversity Training Fails.” It is possible that the newer training programs currently in use by tech companies may end up being more effective than those reviewed in Dobbin’s study; however we should wait to see the evidence."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#unconscious-bias-training-can-increase-bias",
    "href": "posts/2015-12-07-diversity-washing/index.html#unconscious-bias-training-can-increase-bias",
    "title": "Diversity Washing Makes Things Worse",
    "section": "Unconscious bias training can increase bias",
    "text": "Unconscious bias training can increase bias\nUnconscious bias (for example, when people rate identical resumes with a female or traditionally African American name more negatively, as opposed to the same resume with a male or traditionally white name) is a very real problem; however just teaching people that unconscious bias exists does not eliminate it and can even increase bias. In this NYTimes article, Sheryl Sandberg and Wharton professor Adam Grant summarize research that unconscious bias training can increase bias, depending on how it is communicated.\nIn one study from UVA and Washington University, managers read a job interview transcript after either being told either that: stereotypes are rare; or being told that: many people believe stereotypes. When participants were told stereotypes were common and that the candidate was female, they were 28% less likely to hire her and judged her as 27% less likable (compared to the identical transcript with the candidate labeled as male). People may feel more comfortable believing stereotypes when they hear that they are commonly believed. Sandberg and Grant argue that the key is to instead communicate that biases are inaccurate, and that most people don’t want to discriminate."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#the-difficulty-of-self-reflection",
    "href": "posts/2015-12-07-diversity-washing/index.html#the-difficulty-of-self-reflection",
    "title": "Diversity Washing Makes Things Worse",
    "section": "The Difficulty of Self-Reflection",
    "text": "The Difficulty of Self-Reflection\nPart of the problem with efforts to raise awareness of unconscious bias is that we are great at finding post-hoc justifications for our biases so we tend to see ourselves as immune to bias. Harvard Business School professor Francesca Gino, who has taught courses on biases and decision making to executives and MBA students, states “most of my students easily recognize that their colleagues and friends are biased but generally don’t think they are themselves.” It is both easier to believe that other people discriminate, but not me, because I have good judgement, and easier to believe that other people experience discrimination, but not my coworker, because I see her flaws.\nA study from Yale researchers shows that perceiving yourself as objective is actually correlated with showing even more bias. Researchers from MIT and Indiana University found that company structures that explicitly promote meritocracy (compared to those that don’t) show greater bias against women. The 445 participants in the study all had managerial experience and were asked to evaluate employee profiles given a set of organizational core values (which included meritocracy in some cases but not others). Women were awarded smaller bonuses than men with equivalent performance reviews when the core values emphasized meritocracy.\nGender and racial biases aren’t just problems affecting the education system and other people’s companies; biases are affecting your company. It’s not just that other people need to change; we all need to change, and it’s an ongoing process."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#no-easy-fixes",
    "href": "posts/2015-12-07-diversity-washing/index.html#no-easy-fixes",
    "title": "Diversity Washing Makes Things Worse",
    "section": "No Easy Fixes",
    "text": "No Easy Fixes\nI am not the first to express similar frustrations, although I have seen little coverage by tech journalists of this aspect of the issue. Erica Joy, an engineer at Slack, wrote that she is “tired of the ‘we hired this many’ and ‘we gave this many dollars to girls coding initiatives.’ None of those numbers accurately portray what the inside of a company looks like.” Darrell Jones III, head of business development at Clef, explained, “when we allow companies to simply ‘educate’ their employees or ‘spread awareness’ by publishing dismal diversity numbers, we let them off the hook.” Cate Huston, a former Google engineer and head of mobile development at Ride, ranked classes of diversity problems from easy to extra hard, and, expressed her disappointment that Grace Hopper Conference (for women in computing) primarily focuses on “easy problems,” asking, “Is it just going to be a celebration of managing the easy things. Of crawling over that exceptionally low bar of sexist marketing materials. Of focusing on the pipeline rather than the woman who are already here.” Freada Kapor Klein, founder of the Level Playing Field Institute, has observed that “there is way too much money going to hackathons teaching privileged girls how to code without any tie-in to anything else… We’re mapping out all of the drop-off points so that as opposed to being the 400th person who funds a girls coding program, we can even out the dollars.” Indie game developer Veve Jaffe wrote of Intel’s diversity efforts, “my experience reflects a growing trend of corporations paying lip service to diversity — and collecting all of its PR benefits — while demanding unpaid work from underrepresented developers.”\nThis is not an argument against donating to girls coding initiatives, hosting hackathons for girls, or creating inclusive marketing materials (all are good things!), just a reminder that these won’t impact the biases the talented women and people of color already working at your company are currently facing. No donation is ever a substitute for the hard work of self-reflection and company-wide change. The “easy” changes are necessary, but they are not sufficient.\nSimilarly, collecting and sharing diversity data is a necessary step in determining if your current approach is working, but not something to celebrate on its own. It is just a means towards the end goal of creating an equitable and inclusive work environment. If the data shows your approach is not working, you need to change what you’re doing.\nOne of the results of the research on data from 708 companies is that “new programs decoupled from everyday practice often have no impact” and that it is more effective to rethink hiring and promotion structures entirely. This is similar to what Dr. Klein has found working on diversity issues for over a decade; Klein says that having “engineering deeply involved in company diversity and inclusion efforts is critically important to getting it right and not having it be a side annoying thing.”"
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#conclusion",
    "href": "posts/2015-12-07-diversity-washing/index.html#conclusion",
    "title": "Diversity Washing Makes Things Worse",
    "section": "Conclusion",
    "text": "Conclusion\nThis post is not an argument against diversity initiatives, but a reminder that diversity and inclusion aren’t automatically achieved when these programs are announced. We should all be on the lookout for concrete progress (not just raw hiring numbers, but also percentage of women and people of color in management and executive positions, salary parity, and retention rates), as well as for negative side effects. Any successful effort towards diversity and inclusion will need to involve ongoing self-reflection, comprehensive changes, and addressing hard problems. In my next post, I will write about examples of more substantial changes.\nMany thanks to Jeremy Howard for feedback on earlier drafts of this post.\nI further develop the ideas here in my next post about bullshit diversity strategies and some better ideas for enacting positive change. I later survey the research on why women are unable to advance in their careers and offer concrete strategies to address this problem. You may also be interested in my post debunking the pipeline myth, which shares my personal story of wanting to leave the tech industry, as well as practical tips."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html",
    "href": "posts/2017-11-13-validation-sets/index.html",
    "title": "How (and why) to create a good validation set",
    "section": "",
    "text": "An all-too-common scenario: a seemingly impressive machine learning model is a complete failure when implemented in production. The fallout includes leaders who are now skeptical of machine learning and reluctant to try it again. How can this happen?\nOne of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a train_test_split method, this method takes a random subset of the data, which is a poor choice for many real-world problems.\nThe definitions of training, validation, and test sets can be fairly nuanced, and the terms are sometimes inconsistently used. In the deep learning community, “test-time inference” is often used to refer to evaluating on data in production, which is not the technical definition of a test set. As mentioned above, sklearn has a train_test_split method, but no train_validation_test_split. Kaggle only provides training and test sets, yet to do well, you will need to split their training set into your own validation and training sets. Also, it turns out that Kaggle’s test set is actually sub-divided into two sets. It’s no suprise that many beginners may be confused! I will address these subtleties below."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#first-what-is-a-validation-set",
    "href": "posts/2017-11-13-validation-sets/index.html#first-what-is-a-validation-set",
    "title": "How (and why) to create a good validation set",
    "section": "First, what is a “validation set”?",
    "text": "First, what is a “validation set”?\nWhen creating a machine learning model, the ultimate goal is for it to be accurate on new data, not just the data you are using to build it. Consider the below example of 3 different models for a set of data:\n\n\n\nunder-fitting and over-fitting (Source: Andrew Ng’s Machine Learning Coursera class)\n\n\nThe error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it’s not the best choice. Why is that? If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\nThe underlying idea is that:\n\nthe training set is used to train a given model\nthe validation set is used to choose between models (for instance, does a random forest or a neural net work better for your problem? do you want a random forest with 40 trees or 50 trees?)\nthe test set tells you how you’ve done. If you’ve tried out a lot of different models, you may get one that does well on your validation set just by chance, and having a test set helps make sure that is not the case.\n\nA key property of the validation and test sets is that they must be representative of the new data you will see in the future. This may sound like an impossible order! By definition, you haven’t seen this data yet. But there are still a few things you know about it."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#when-is-a-random-subset-not-good-enough",
    "href": "posts/2017-11-13-validation-sets/index.html#when-is-a-random-subset-not-good-enough",
    "title": "How (and why) to create a good validation set",
    "section": "When is a random subset not good enough?",
    "text": "When is a random subset not good enough?\nIt’s instructive to look at a few examples. Although many of these examples come from Kaggle competitions, they are representative of problems you would see in the workplace.\n\nTime series\nIf your data is a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates your are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future). If your data includes the date and you are building a model to use in the future, you will want to choose a continuous section with the latest dates as your validation set (for instance, the last two weeks or last month of the available data).\nSuppose you want to split the time series data below into training and validation sets:\n\n\n\nTime series data\n\n\nA random subset is a poor choice (too easy to fill in the gaps, and not indicative of what you’ll need in production):\n\n\n\na poor choice for your training set\n\n\nUse the earlier data as your training set (and the later data for the validation set):\n\n\n\na better choice for your training set\n\n\nKaggle currently has a competition to predict the sales in a chain of Ecuadorian grocery stores. Kaggle’s “training data” runs from Jan 1 2013 to Aug 15 2017 and the test data spans Aug 16 2017 to Aug 31 2017. A good approach would be to use Aug 1 to Aug 15 2017 as your validation set, and all the earlier data as your training set.\n\n\nNew people, new boats, new…\nYou also need to think about what ways the data you will be making predictions for in production may be qualitatively different from the data you have to train your model with.\nIn the Kaggle distracted driver competition, the independent data are pictures of drivers at the wheel of a car, and the dependent variable is a category such as texting, eating, or safely looking ahead. If you were the insurance company building a model from this data, note that you would be most interested in how the model performs on drivers you haven’t seen before (since you would likely have training data only for a small group of people). This is true of the Kaggle competition as well: the test data consists of people that weren’t used in the training set.\n\n\n\nTwo images of the same person drinking a soda while driving\n\n\nIf you put one of the above images in your training set and one in the validation set, your model will seem to be performing better than it would on new people. Another perspective is that if you used all the people in training your model, your model may be overfitting to particularities of those specific people, and not just learning the states (texting, eating, etc).\nA similar dynamic was at work in the Kaggle fisheries competition to identify the species of fish caught by fishing boats in order to reduce illegal fishing of endangered populations. The test set consisted of boats that didn’t appear in the training data. This means that you’d want your validation set to include boats that are not in the training set.\nSometimes it may not be clear how your test data will differ. For instance, for a problem using satellite imagery, you’d need to gather more information on whether the training set just contained certain geographic locations, or if it came from geographically scattered data."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#the-dangers-of-cross-validation",
    "href": "posts/2017-11-13-validation-sets/index.html#the-dangers-of-cross-validation",
    "title": "How (and why) to create a good validation set",
    "section": "The dangers of cross-validation",
    "text": "The dangers of cross-validation\nThe reason that sklearn doesn’t have a train_validation_test split is that it is assumed you will often be using cross-validation, in which different subsets of the training set serve as the validation set. For example, for a 3-fold cross validation, the data is divided into 3 sets: A, B, and C. A model is first trained on A and B combined as the training set, and evaluated on the validation set C. Next, a model is trained on A and C combined as the training set, and evaluated on validation set B. And so on, with the model performance from the 3 folds being averaged in the end.\nHowever, the problem with cross-validation is that it is rarely applicable to real world problems, for all the reasons describedin the above sections. Cross-validation only works in the same cases where you can randomly shuffle your data to choose a validation set."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#kaggles-training-set-your-training-validation-sets",
    "href": "posts/2017-11-13-validation-sets/index.html#kaggles-training-set-your-training-validation-sets",
    "title": "How (and why) to create a good validation set",
    "section": "Kaggle’s “training set” = your training + validation sets",
    "text": "Kaggle’s “training set” = your training + validation sets\nOne great thing about Kaggle competitions is that they force you to think about validation sets more rigorously (in order to do well). For those who are new to Kaggle, it is a platform that hosts machine learning competitions. Kaggle typically breaks the data into two sets you can download:\n\na training set, which includes the independent variables, as well as the dependent variable (what you are trying to predict). For the example of an Ecuadorian grocery store trying to predict sales, the independent variables include the store id, item id, and date; the dependent variable is the number sold. For the example of trying to determine whether a driver is engaging in dangerous behaviors behind the wheel, the independent variable could be a picture of the driver, and the dependent variable is a category (such as texting, eating, or safely looking forward).\na test set, which just has the independent variables. You will make predictions for the test set, which you can submit to Kaggle and get back a score of how well you did.\n\nThis is the basic idea needed to get started with machine learning, but to do well, there is a bit more complexity to understand. You will want to create your own training and validation sets (by splitting the Kaggle “training” data). You will just use your smaller training set (a subset of Kaggle’s training data) for building your model, and you can evaluate it on your validation set (also a subset of Kaggle’s training data) before you submit to Kaggle.\nThe most important reason for this is that Kaggle has split the test data into two sets: for the public and private leaderboards. The score you see on the public leaderboard is just for a subset of your predictions (and you don’t know which subset!). How your predictions fare on the private leaderboard won’t be revealed until the end of the competition. The reason this is important is that you could end up overfitting to the public leaderboard and you wouldn’t realize it until the very end when you did poorly on the private leaderboard. Using a good validation set can prevent this. You can check if your validation set is any good by seeing if your model has similar scores on it to compared with on the Kaggle test set.\nAnother reason it’s important to create your own validation set is that Kaggle limits you to two submissions per day, and you will likely want to experiment more than that. Thirdly, it can be instructive to see exactly what you’re getting wrong on the validation set, and Kaggle doesn’t tell you the right answers for the test set or even which data points you’re getting wrong, just your overall score.\nUnderstanding these distinctions is not just useful for Kaggle. In any predictive machine learning project, you want your model to be able to perform well on new data."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html",
    "href": "posts/2018-04-29-categorical-embeddings/index.html",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "",
    "text": "There is a powerful technique that is winning Kaggle competitions and is widely used at Google (according to Jeff Dean), Pinterest, and Instacart, yet that many people don’t even realize is possible: the use of deep learning for tabular data, and in particular, the creation of embeddings for categorical variables.\nDespite what you may have heard, you can use deep learning for the type of data you might keep in a SQL database, a Pandas DataFrame, or an Excel spreadsheet (including time-series data). I will refer to this as tabular data, although it can also be known as relational data, structured data, or other terms (see my twitter poll and comments for more discussion).\nTabular data is the most commonly used type of data in industry, but deep learning on tabular data receives far less attention than deep learning for computer vision and natural language processing. This post covers some key concepts from applying neural networks to tabular data, in particular the idea of creating embeddings for categorical variables, and highlights 2 relevant modules of the fastai library:\nThe material from this post is covered in much more detail starting around 1:59:45 in the Lesson 3 video and continuing in Lesson 4 of our free, online Practical Deep Learning for Coders course. To see example code of how this approach can be used in practice, check out our Lesson 3 jupyter notebook."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#embeddings-for-categorical-variables",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#embeddings-for-categorical-variables",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Embeddings for Categorical Variables",
    "text": "Embeddings for Categorical Variables\nA key technique to making the most of deep learning for tabular data is to use embeddings for your categorical variables. This approach allows for relationships between categories to be captured. Perhaps Saturday and Sunday have similar behavior, and maybe Friday behaves like an average of a weekend and a weekday. Similarly, for zip codes, there may be patterns for zip codes that are geographically near each other, and for zip codes that are of similar socio-economic status.\n\nTaking Inspiration from Word Embeddings\nA way to capture these multi-dimensional relationships between categories is to use embeddings. This is the same idea as is used with word embeddings, such as Word2Vec. For instance, a 3-dimensional version of a word embedding might look like:\n\n\n\n\n\n\n\npuppy\n[0.9, 1.0, 0.0]\n\n\ndog\n[1.0, 0.2, 0.0]\n\n\nkitten\n[0.0, 1.0, 0.9]\n\n\ncat\n[0.0, 0.2, 1.0]\n\n\n\nNotice that the first dimension is capturing something related to being a dog, and the second dimension captures youthfulness. This example was made up by hand, but in practice you would use machine learning to find the best representations (while semantic values such as dogginess and youth would be captured, they might not line up with a single dimension so cleanly). You can check out my workshop on word embeddings for more details about how word embeddings work.\n\n\n\nillustration from my word embeddings workshop: vectors for baby animal words are closer together, and an unrelated word like ‘avalanche’ is further away\n\n\n\n\nApplying Embeddings for Categorical Variables\nSimilarly, when working with categorical variables, we will represent each category by a vector of floating point numbers (the values of this representation are learned as the network is trained).\nFor instance, a 4-dimensional version of an embedding for day of week could look like:\n\n\n\n\n\n\n\nSunday\n[.8, .2, .1, .1]\n\n\nMonday\n[.1, .2, .9, .9]\n\n\nTuesday\n[.2, .1, .9, .8]\n\n\n\nHere, Monday and Tuesday are fairly similar, yet they are both quite different from Sunday. Again, this is a toy example. In practice, our neural network would learn the best representations for each category while it is training, and each dimension (or direction, which doesn’t necessarily line up with ordinal dimensions) could have multiple meanings. Rich relationships can be captured in these distributed representations.\n\n\nReusing Pretrained Categorical Embeddings\nEmbeddings capture richer relationships and complexities than the raw categories. Once you have learned embeddings for a category which you commonly use in your business (e.g. product, store id, or zip code), you can use these pre-trained embeddings for other models. For instance, Pinterest has created 128-dimensional embeddings for its pins in a library called Pin2Vec, and Instacart has embeddings for its grocery items, stores, and customers.\n\n\n\nFrom the Instacart blog post ‘Deep Learning with Emojis (not Math)’\n\n\nThe fastai library contains an implementation for categorical variables, which work with Pytorch’s nn.Embedding module, so this is not something you need to code from hand each time you want to use it."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#treating-some-continuous-variables-as-categorical",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#treating-some-continuous-variables-as-categorical",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Treating some Continuous Variables as Categorical",
    "text": "Treating some Continuous Variables as Categorical\nWe generally recommend treating month, year, day of week, and some other variables as categorical, even though they could be treated as continuous. Often for variables with a relatively small number of categories, this results in better performance. This is a modeling decision that the data scientist makes. Generally, we want to keep continuous variables represented by floating point numbers as continuous.\nAlthough we can choose to treat continuous variables as categorical, the reverse is not true: any variables that are categorical must be treated as categorical."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#time-series-data",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#time-series-data",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Time Series Data",
    "text": "Time Series Data\nThe approach of using neural networks together with categorical embeddings can be applied to time series data as well. In fact, this was the model used by students of Yoshua Bengio to win 1st place in the Kaggle Taxi competition(paper here), using a trajectory of GPS points and timestamps to predict the length of a taxi ride. It was also used by the 3rd place winners in the Kaggle Rossmann Competition, which involved using time series data from a chain of stores to predict future sales. The 1st and 2nd place winners of this competition used complicated ensembles that relied on specialist knowledge, while the 3rd place entry was a single model with no domain-specific feature engineering.\n\n\n\nThe winning architecture from the Kaggle Taxi Trajectory Competition\n\n\nIn our Lesson 3 jupyter notebook we walk through a solution for the Kaggle Rossmann Competition. This data set (like many data sets) includes both categorical data (such as the state the store is located in, or being one of 3 different store types) and continuous data (such as the distance to the nearest competitor or the temperature of the local weather). The fastai library lets you enter both categorical and continuous variables as input to a neural network.\nWhen applying machine learning to time-series data, you nearly always want to choose a validation set that is a continuous selection with the latest available dates that you have data for. As I wrote in a previous post, “If your data is a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates your are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future).”\nOne key to successfully using deep learning with time series data is to split the date into multiple categorical variables (year, month, week, day of week, day of month, and Booleans for whether it’s the start/end of a month/quarter/year). The fastai library has implemented a method to handle this for you, as described below."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#modules-to-know-in-the-fastai-library",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#modules-to-know-in-the-fastai-library",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Modules to Know in the Fastai Library",
    "text": "Modules to Know in the Fastai Library\nWe will be releasing more documentation for the fastai library in coming months, but it is already available on pip and on github, and it is used in the Practical Deep Learning for Coders course. The fastai library is built on top of Pytorch and encodes best practices and helpful high-level abstractions for using neural networks. The fastai library achieves state-of-the-art results and was recently used to win the Stanford DAWNBench competition (fastest CIFAR10 training).\n\nfastai.column_data\nfastai.column_data.ColumnarModelData takes a Pandas DataFrame as input and creates a type of ModelData object (an object which contains data loaders for the training, validation, and test sets, and which is the fundamental way of keeping track of your data while training models).\n\n\nfastai.structured\nThe fastai.structured module of the fastai library is built on top of Pandas, and includes methods to transform DataFrames in a number of ways, improving the performance of machine learning models by pre-processing the data appropriately and creating the right types of variables.\nFor instance, fastai.structured.add_datepart converts dates (e.g. 2000-03-11) into a number of variables (year, month, week, day of week, day of month, and booleans for whether it’s the start/end of a month/quarter/year.)\nOther useful methods in the module allow you to:\n\nFill in missing values with the median whilst adding a boolean indicator variable (fix_missing)\nChange any columns of strings in a Pandas DataFrame to a column of categorical values (train_cats)"
  },
  {
    "objectID": "posts/2018-07-12-automl1/index.html",
    "href": "posts/2018-07-12-automl1/index.html",
    "title": "What do machine learning practitioners actually do?",
    "section": "",
    "text": "This post is part 1 of a series. Part 2 is an opinionated introduction to AutoML and neural architecture search, and Part 3 looks at Google’s AutoML in particular.\nThere are frequent media headlines about both the scarcity of machine learning talent (see here, here, and here) and about the promises of companies claiming their products automate machine learning and eliminate the need for ML expertise altogether (see here, here, and here). In his keynote at the TensorFlow DevSummit, Google’s head of AI Jeff Dean estimated that there are tens of millions of organizations that have electronic data that could be used for machine learning but lack the necessary expertise and skills. I follow these issues closely since my work at fast.ai focuses on enabling more people to use machine learning and on making it easier to use.\nIn thinking about how we can automate some of the work of machine learning, as well as how to make it more accessible to people with a wider variety of backgrounds, it’s first necessary to ask, what is it that machine learning practitioners do? Any solution to the shortage of machine learning expertise requires answering this question: whether it’s so we know what skills to teach, what tools to build, or what processes to automate.\n\n\n\nWhat do machine learning practitioners do? (Image Source: #WOCinTech Chat)\n\n\nThis post is the first in a 3-part series. It will address what it is that machine learning practitioners do, with Part 2 explaining AutoML and neural architecture search (which several high profile figures have suggested will be key to decreasing the need for data scientists) and Part 3 will cover Google’s heavily hyped AutoML product in particular.\n\nBuilding Data Products is Complex Work\n\nWhile many academic machine learning sources focus almost exclusively on predictive modeling, that is just one piece of what machine learning practitioners do in the wild. The processes of appropriately framing a business problem, collecting and cleaning the data, building the model, implementing the result, and then monitoring for changes are interconnected in many ways that often make it hard to silo off just a single piece (without at least being aware of what the other pieces entail). As Jeremy Howard et al. wrote in Designing great data products, Great predictive modeling is an important part of the solution, but it no longer stands on its own; as products become more sophisticated, it disappears into the plumbing.\n\n\n\nBuilding Data Products is Complex Work (Source: Wikimedia Commons)\n\n\nA team from Google, D. Sculley et al., wrote the classic Machine Learning: The High-Interest Credit Card of Technical Debt about the code complexity and technical debt often created when using machine learning in practice. The authors identify a number of system-level interactions, risks, and anti-patterns, including:\n\nglue code: massive amount of supporting code written to get data into and out of general-purpose packages\npipeline jungles: the system for preparing data in an ML-friendly format may become a jungle of scrapes, joins, and sampling steps, often with intermediate files output\nre-use input signals in ways that create unintended tight coupling of otherwise disjoint systems\nrisk that changes in the external world may make models or input signals change behavior in unintended ways, and these can be difficult to monitor\n\nThe authors write, A remarkable portion of real-world “machine learning” work is devoted to tackling issues of this form… It’s worth noting that glue code and pipeline jungles are symptomatic of integration issues that may have a root cause in overly separated “research” and “engineering” roles… It may be surprising to the academic community to know that only a tiny fraction of the code in many machine learning systems is actually doing “machine learning”. (emphasis mine)\n\nWhen machine learning projects fail\n\nIn a previous post, I identified some failure modes in which machine learning projects are not effective in the workplace:\n\nThe data science team builds really cool stuff that never gets used. There’s no buy-in from the rest of the organization for what they’re working on, and some of the data scientists don’t have a good sense of what can realistically be put into production.\nThere is a backlog with data scientists producing models much faster than there is engineering support to put them in production.\nThe data infrastructure engineers are separate from the data scientists. The pipelines don’t have the data the data scientists are asking for now, and the data scientists are under-utilizing the data sources the infrastructure engineers have collected.\nThe company has definitely decided on feature/product X. They need a data scientist to gather some data that supports this decision. The data scientist feels like the PM is ignoring data that contradicts the decision; the PM feels that the data scientist is ignoring other business logic.\nThe data science team interviews a candidate with impressive math modeling and engineering skills. Once hired, the candidate is embedded in a vertical product team that needs simple business analytics. The data scientist is bored and not utilizing their skills.\n\nI framed these as organizational failures in my original post, but they can also be described as various participants being overly focused on just one slice of the complex system that makes up a full data product. These are failures of communication and goal alignment between different parts of the data product pipeline.\n\nSo, what do machine learning practitioners do?\n\nAs suggested above, building a machine learning product is a multi-faceted and complex task. Here are some of the things that machine learning practitioners may need to do during the process:\nUnderstanding the context:\n\nidentify areas of the business that could benefit from machine learning\ncommunicate with other stakeholders about what machine learning is and is not capable of (there are often many misconceptions)\ndevelop understanding of business strategy, risks, and goals to make sure everyone is on the same page\nidentify what kind of data the organization has\nappropriately frame and scope the task\nunderstand operational constraints (e.g. what data is actually available at inference time)\nproactively identify ethical risks, including how your work could be mis-used by harassers, trolls, authoritarian governments, or for propaganda/disinformation campaigns (and plan how to reduce these risks)\nidentify potential biases and potential negative feedback loops\n\nData: - make plans to collect more of different data (if needed and if possible) - stitch together data from many different sources: often this data has been collected in different formats or with inconsistent conventions - deal with missing or corrupted data - visualize the data - create appropriate training, validation, and test sets\nModeling: - choose which model to use - fit model resource needs into constraints (e.g. will the completed model need to run on an edge device, in a low memory or high latency environment, etc) - choose hyperparameters (e.g. in the case of deep learning, this includes choosing an architecture, loss function, and optimizer) - train the model (and debug why it’s not training). This can involve: - adjusting hyperparmeters (e.g. such as the learning rate) - outputing intermediate results to see how the loss, training error, and validation error are changing with time - inspecting the data the model is wrong on to look for patterns - identifying underlying errors or issues with the data - realizing you need to change how you clean and pre-process the data - realizing you need more or different data augmentation - realizing you need more or different data - trying out different models - identifying if you are under- or over-fitting\nProductionize: - creating an API or web app with your model as an endpoint in order to productionize - exporting your model into the needed format - plan for how often your model will need to be retrained with updated data (e.g. perhaps you will retrain nightly or weekly)\nMonitor: - track model performance over time - monitor the input data, to identify if it changes with time in a way that would invalidate your model - communicate your results to the rest of the organization - have a plan in place for how you will monitor and respond to mistakes or unexpected consequences\nCertainly, not every machine learning practitioner needs to do all of the above steps, but components of this process will be a part of many machine learning applications. Even if you are working on just a subset of these steps, a familiarity with the rest of the process will help ensure that you are not overlooking considerations that would keep your project from being successful!\n\nTwo of the hardest parts of Machine Learning\n\nFor myself and many others I know, I would highlight two of the most time-consuming and frustrating aspects of machine learning (in particular, deep learning) as:\n\nDealing with data formatting, inconsistencies, and errors is often a messy and tedious process.\nTraining deep learning models is a notoriously brittle process right now.\n\n\nIs cleaning data really part of ML? Yes.\n\nDealing with data formatting, inconsistencies, and errors is often a messy and tedious process. People will sometimes describe machine learning as separate from data science, as though for machine learning, you can just begin with your nicely cleaned, formatted data set. However, in my experience, the process of cleaning a data set and training a model are usually interwoven: I frequently find issues in the model training that cause me to go back and change the pre-processing for the input data.\n\n\n\nDealing with messy and inconsistent data is necessary\n\n\n\nTraining Deep Learning Models is Brittle and Finicky (for now)\n\nThe difficulty of getting models to train deters many beginners, who often wind up feeling discouraged. Even experts frequently complain of how frustrating and fickle the training process can be. One AI researcher at Stanford told me, I taught a course on deep learning and had all the students do their own projects. It was so hard. The students couldn’t get their models to train, and we were like “well, that’s deep learning”. Ali Rahimi, an AI researcher with over a decade of experience and winner of the NIPS 2017 Test of Time Award, complained about the brittleness of training in his NeurIPS award speech. How many of you have designed a deep net from scratch, built it from the ground up, architecture and all, and when it didn’t work, you felt bad about yourself? Rahimi asked the audience of AI researchers, and many raised their hands. Rahimi continued, This happens to me about every 3 months.\nThe fact that even AI experts sometimes have trouble training new models implies that the process has yet to be automated in a way where it could be incorporated into a general-purpose product. Some of the biggest advances in deep learning will come through discovering more robust training methods. We have already seen this some with advances like dropout, super convergence, and transfer learning, all of which make training easier. Through the power of transfer learning (to be discussed in Part 3) training can be a robust process when defined for a narrow enough problem domain; however, we still have a ways to go in making training more robust in general.\n\nFor Academic Researchers\n\nEven if you are working on theoretical machine learning research, it is useful to understand the process that machine learning practitioners working on practical problems go through, as that might provide insights on what the most relevant or high-impact areas of research are.\nAs Googler engineers D. Sculley et al. wrote, Technical debt is an issue that both engineers and researchers need to be aware of. Research solutions that provide a tiny accuracy benefit at the cost of massive increases in system complexity are rarely wise practice… Paying down technical debt is not always as exciting as proving a new theorem, but it is a critical part of consistently strong innovation. And developing holistic, elegant solutions for complex machine learning systems is deeply rewarding work. (emphasis mine)\n\nAutoML\n\nNow that we have an overview of some of the tasks that machine learning practitioners do as part of their work, we are ready to evaluate attempts to automate this work. As it’s name suggests, AutoML is one field in particular that has focused on automating machine learning, and a subfield of AutoML called neural architecture search is currently receiving a ton of attention. In part 2, I will explain what AutoML and neural architecture search are, and in part 3, look at Google’s AutoML in particular.\nBe sure to check out Part 2: An Opinionated Introduction to AutoML Neural Architecture Search, and Part 3: Google’s AutoML: Cutting Through the Hype"
  },
  {
    "objectID": "posts/2018-07-16-automl2/index.html",
    "href": "posts/2018-07-16-automl2/index.html",
    "title": "An Opinionated Introduction to AutoML and Neural Architecture Search",
    "section": "",
    "text": "This is part 2 in a series. Check out part 1 here and part 3 here.\nResearchers from CMU and DeepMind recently released an interesting new paper, called Differentiable Architecture Search (DARTS), offering an alternative approach to neural architecture search, a very hot area of machine learning right now. Neural architecture search has been heavily hyped in the last year, with Google’s CEO Sundar Pichai and Google’s Head of AI Jeff Dean promoting the idea that neural architecture search and the large amounts of computational power it requires are essential to making machine learning available to the masses. Google’s work on neural architecture search has been widely and adoringly covered by the tech media (see here, here, here, and here for examples).\n\n\n\nHeadlines from just a few of the many, many articles written about Google’s AutoML and Neural Architecture Search\n\n\nDuring his keynote (starts around 22:20) at the TensorFlow DevSummit in March 2018, Jeff Dean posited that perhaps in the future Google could replace machine learning expertise with 100x computational power. He gave computationally expensive neural architecture search as a primary example (the only example he gave) of why we need 100x computational power in order to make ML accessible to more people.\n\n\n\nSlide from Jeff Dean’s Keynote at the TensorFlow Dev Summit\n\n\nWhat is neural architecture search? Is it the key to making machine learning available to non-machine learning experts? I will dig into these questions in this post, and in my next post, I will look specifically at Google’s AutoML. Neural architecture search is a part of a broader field called AutoML, which has also been receiving a lot of hype and which we will consider first.\nPart 2 table of contents:\n\n\nWhat is AutoML?\n\n\nHow useful is AutoML?\n\n\nWhat is neural architecture search?\n\n\nWhat about DARTS?\n\n\nHow useful is Neural Architecture Search?\n\n\nHow else could we make machine learning practitioners more effective?\n\n\n\nWhat is AutoML?\n\nThe term AutoML has traditionally been used to describe automated methods for model selection and/or hyperparameter optimization. These methods exist for many types of algorithms, such as random forests, gradient boosting machines, neural networks, and more. The field of AutoML includes open-source AutoML libraries, workshops, research, and competitions. Beginners often feel like they are just guessing as they test out different hyperparameters for a model, and automating the process could make this piece of the machine learning pipeline easier, as well as speeding things up even for experienced machine learning practitioners.\nThere are a number of AutoML libraries, the oldest of which is AutoWEKA, which was first released in 2013 and automatically chooses a model and selects hyperparameters. Other notable AutoML libraries include auto-sklearn (which extends AutoWEKA to python), H2O AutoML, and TPOT. AutoML.org (formerly known as ML4AAD, Machine Learning for Automated Algorithm Design) has been organzing AutoML workshops at the academic machine learning conference ICML yearly since 2014.\n\nHow useful is AutoML?\n\nAutoML provides a way to select models and optimize hyper-parameters. It can also be useful in getting a baseline to know what level of performance is possible for a problem. So does this mean that data scientists can be replaced? Not yet, as we need to keep the context of what else it is that machine learning practitioners do.\nFor many machine learning projects, choosing a model is just one piece of the complex process of building machine learning products. As I covered in my previous post, projects can fail if participants don’t see how interconnected the various parts of the pipeline are. I thought of over 30 different steps that can be involved in the process. I highlighted two of the most time-consuming aspects of machine learning (in particular, deep learning) as cleaning data (and yes, this is an inseparable part of machine learning) and training models. While AutoML can help with selecting a model and choosing hyperparameters, it is important to keep perspective on what other data expertise is still needed and on the difficult problems remain.\nI will suggest some alternate approaches to AutoML for making machine learning practitioners more effective in the final section.\n\nWhat is neural architecture search?\n\nNow that we’ve covered some of what AutoML is, let’s look at a particularly active subset of the field: neural architecture search. Google CEO Sundar Pichai wrote that, “designing neural nets is extremely time intensive, and requires an expertise that limits its use to a smaller community of scientists and engineers. That’s why we’ve created an approach called AutoML, showing that  it’s possible for neural nets to design neural nets.”\nWhat Pichai refers to as using “neural nets to design neural nets” is known as neural architecture search; typically reinforcement learning or evolutionary algorithms are used to design the new neural net architectures. This is useful because it allows us to discover architectures far more complicated than what humans may think to try, and these architectures can be optimized for particular goals. Neural architecture search is often very computationally expensive.\nTo be precise, neural architecture search usually involves learning something like a layer (often called a “cell”) that can be assembled as a stack of repeated cells to create a neural network:\n\n\n\nDiagram from Zoph et. al. 2017. On the left is the full neural network of stacked cells, and on the right is the inside structure of a cell\n\n\nThe literature of academic papers on neural architecture search is extensive, so I will highlight just a few recent papers here:\n\nThe term AutoML jumped to “mainstream” prominence with work by Google AI researchers (paper here) Quoc Le and Barret Zoph, which was featured at Google I/O in May 2017. This work used reinforcement learning to find new architectures for the computer vision problem Cifar10 and the NLP problem Penn Tree Bank, and achieved similar results to existing architectures.\n\n\n\n\nDiagram from Le and Zoph’s blog post: the simpler architecture on the left was designed by a human and the more complicated architecture on the right was designed by a neural net.\n\n\n\nNASNet from Learning Transferable Architectures for Scalable Image Recognition (blog post here). This work searches for an architectural building block on a small data set (Cifar10) and then builds an architecture for a large data set (ImageNet). This research was very computationally intensive with it taking 1800 GPU days (the equivalent of almost 5 years for 1 GPU) to learn the architecture (the team at Google used 500 GPUs for 4 days!).\nAmoebaNet from Regularized Evolution for Image Classifier Architecture Search This research was even more computationally intensive than NASNet, with it taking the equivalent of 3150 GPU days (the equivalent of almost 9 years for 1 GPU) to learn the architecture (the team at Google used 450 K40 GPUs for 7 days!). AmoebaNet consists of “cells” learned via an evolutionary algorithm, showing that artificially-evolved architectures can match or surpass human-crafted and reinforcement learning-designed image classifiers. After incorporating advances from fast.ai such as an aggressive learning schedule and changing the image size as training progresses, AmoebaNet is now the cheapest way to train ImageNet on a single machine.\nEfficient Neural Architecture Search (ENAS): used much fewer GPU-hours than previously existing automatic model design approaches, and notably, was 1000x less expensive than standard Neural Architecture Search. This research was done using a single GPU for just 16 hours.\n\n\nWhat about DARTS?\n\nDifferentiable architecture search (DARTS). This research was recently released from a team at Carnegie Mellon University and DeepMind, and I’m excited about the idea. DARTS assumes the space of candidate architectures is continuous, not discrete, and this allows it to use gradient-based aproaches, which are vastly more efficient than the inefficient black-box search used by most neural architecture search algorithms.\n\n\n\nDiagram from DARTS, which treats the space of all possible architectures as continuous, not discrete\n\n\nTo learn a network for Cifar-10, DARTS takes just 4 GPU days, compared to 1800 GPU days for NASNet and 3150 GPU days for AmoebaNet (all learned to the same accuracy). This is a huge gain in efficiency! Although more exploration is needed, this is a promising research direction. Given how Google frequently equates neural architecture search with huge computational expense, efficient ways to do architecture search have most likely been under-explored.\n\nHow useful is Neural Architecture Search?\n\nIn his TensorFlow DevSummit keynote (starts around 22:20), Jeff Dean suggested that a significant part of deep learning work is trying out different architectures. This was the only step of machine learning that Dean highlighted in his short talk, and I was surprised by his emphasis. Sundar Pichai’s blog post contained a similar assertion.\n\n\n\nJeff Dean’s slide showing that neural architecture search can try 20 different models to find the most accurate\n\n\nHowever, choosing a model is just one piece of the complex process of building machine learning products. In most cases, architecture selection is nowhere near the hardest, most time-consuming, or most significant part of the problem. Currently, there is no evidence that each new problem would be best modeled with it’s own unique architecture, and most practitioners consider it unlikely this will ever be the case.\nOrganizations like Google working on architecture design and sharing the architectures they discover with the rest of us are providing an important and helpful service. However, the underlying architecture search method is only needed for that tiny fraction of researchers that are working on foundational neural architecture design. The rest of us can just use the architectures they find via transfer learning.\n\nHow else could we make machine learning practitioners more effective? AutoML vs. Augmented ML\n\nThe field of AutoML, including neural architecture search, has been largely focused on the question: how can we automate model selection and hyperparameter optimization? However, automation ignores the important role of human input. I’d like to propose an alternate question: how can humans and computers work together to make machine learning more effective? The focus of augmented ML is on figuring out how a human and machine can best work together to take advantage of their different strengths.\nAn example of augmented ML is Leslie Smith’s learning rate finder (paper here), which is implemented in the fastai library (a high level API that sits on top of PyTorch) and taught as a key technique in our free deep learning course. The learning rate is a hyperparameter that can determine how quickly your model trains, or even whether it successfully trains at all. The learning rate finder allows a human to find a good learning rate in a single step, by looking at a generated chart. It’s faster than AutoML approaches to the same problem, improves the data scientist’s understanding of the training process, and encourages more powerful multi-step approaches to training models.\n\n\n\nDiagram from Surmenok’s blog post on the learning rate finder, showing relationship between learning rate and loss\n\n\nThere’s another problem with the focus on automating hyperparameter selection: it overlooks the possibility that some types of model are more widely useful, have fewer hyperparameters to tune, and are less sensitive to choice of hyperparameters. For example, a key benefit of random forests over gradient boosting machines (GBMs) is that random forests are more robust, whereas GBMs tend to be fairly sensitive to minor changes in hyperparameters. As a result, random forests are widely used in industry. Researching ways to effectively remove hyperparameters (through smarter defaults, or through new models) can have a huge impact. When I first became interested in deep learning in 2013, it was overwhelming to feel that there were such a huge number of hyperparameters, and I’m happy that newer research and tools has helped eliminate many of those (especially for beginners). For instance, in the fast.ai course, beginners start by only having to choose a single hyperparameter, the learning rate, and we even give you a tool to do that!\n\nStay tuned…\n\nNow that we have an overview of what the fields of AutoML and neural architecture search are, we can take a closer look at Google’s AutoML in the next post.\nIf you haven’t already, check out Part 1: What is it that machine learning practitioners do? and Part 3: Google’s AutoML: Cutting Through the Hype of this series.\nPlease be sure to check out Part 3 of this post next week!"
  },
  {
    "objectID": "posts/2018-07-23-automl3/index.html",
    "href": "posts/2018-07-23-automl3/index.html",
    "title": "Google’s AutoML: Cutting Through the Hype",
    "section": "",
    "text": "This is part 3 in a series. Part 1 is here and Part 2 is here.\nTo announce Google’s AutoML, Google CEO Sundar Pichai wrote, “Today, designing neural nets is extremely time intensive, and requires an expertise that limits its use to a smaller community of scientists and engineers. That’s why we’ve created an approach called AutoML, showing that it’s possible for neural nets to design neural nets. We hope AutoML will take an ability that a few PhDs have today and will make it possible in three to five years for hundreds of thousands of developers to design new neural nets for their particular needs.” (emphasis mine)\n\n\n\nGoogle CEO Sundar Pichai says that we all need to design our own neural nets\n\n\nWhen Google’s Head of AI, Jeff Dean, suggested that 100x computational power could replace the need for machine learning expertise, computationally expensive neural architecture search was the only example he gave to illustrate this point. (around 23:50 in his TensorFlow DevSummit keynote)\nThis raises a number of questions: do hundreds of thousands of developers need to “design new neural nets for their particular needs” (to quote Pichai’s vision), or is there an effective way for neural nets to generalize to similar problems? Can large amounts of computational power really replace machine learning expertise?\nIn evaluating Google’s claims, it’s valuable to keep in mind Google has a vested financial interest in convincing us that the key to effective use of deep learning is more computational power, because this is an area where they clearly beat the rest of us. If true, we may all need to purchase Google products. On its own, this doesn’t mean that Google’s claims are false, but it’s good be aware of what financial motivations could underlie their statements.\nIn my previous posts, I shared an introduction to the history of AutoML, defined what neural architecture search is, and pointed out that for many machine learning projects, designing/choosing an architecture is nowhere near the hardest, most time-consuming, or most painful part of the problem. In today’s post, I want to look specifically at Google’s AutoML, a product which has received a lot of media attention, and address the following:\n\n\nWhat is Google’s AutoML?\n\n\nWhat is transfer learning?\n\n\nNeural architecture search vs. transfer learning: two opposite approaches\n\n\nIn need of more evidence\n\n\nWhy all the hype about Google’s AutoML?\n\n\nHow can we address the shortage of machine learning expertise?\n\n\n\nWhat is Google’s AutoML?\n\nAlthough the field of AutoML has been around for years (including open-source AutoML libraries, workshops, research, and competitions), in May 2017 Google co-opted the term AutoML for its neural architecture search. In blog posts accompanying announcements made at the conference Google I/O, Google CEO Sundar Pichai wrote, “That’s why we’ve created an approach called AutoML, showing that it’s possible for neural nets to design neural nets” and Google AI researchers Barret Zoph and Quoc Le wrote “In our approach (which we call”AutoML”), a controller neural net can propose a “child” model architecture…“\nGoogle’s Cloud AutoML was announced in January 2018 as a suite of machine learning products. So far it consists of one publicly available product, AutoML Vision, an API that identifies or classifies objects in pictures. According to the product page, Cloud AutoML Vision relies on two core techniques: transfer learning and neural architecture search. Since we’ve already explained neural architecture search, let’s now take a look at transfer learning, and see how it relates to neural architecture search.\n\n\n\nHeadlines from just a few of the many, many articles written about Google’s AutoML and Neural Architecture Search\n\n\nNote: Google Cloud AutoML also has a drag-and-drop ML product that is still in alpha. I applied for access to it over 2 months ago, but I have not heard back from Google yet. I plan to write a post once it’s released.\n\nWhat is transfer learning?\n\nTransfer learning is a powerful technique that lets people with smaller datasets or less computational power achieve state-of-the-art results, by taking advantage of pre-trained models that have been trained on similar, larger data sets. Because the model learned via transfer learning doesn’t have to learn from scratch, it can generally reach higher accuracy with much less data and computation time than models that don’t use transfer learning.\nTransfer learning is a core technique that we use throughout our free Practical Deep Learning for Coders course– and that our students have been applying in production in everything from their own startups to Fortune 500 companies. Although transfer learning seems to be considered “less sexy” than neural architecture search, it is being used to achieve ground-breaking academic results, such as in Jeremy Howard and Sebastian Ruder’s application of transfer learning to NLP, which achieved state-of-the-art classification on 6 datasets and is serving as a basis for further research in this area at OpenAI.\n\nNeural architecture search vs. transfer learning: two opposing approaches\n\nThe underlying idea of transfer learning is that neural net architectures will generalize for similar types of problems: for example, that many images have underlying features (such as corners, circles, dog faces, or wheels) that show up in a variety of different types of images. In contrast, the underlying idea of promoting neural architecture search for every problem is the opposite: that each dataset has a unique, highly specialized architecture it will perform best with.\n\n\n\nExamples from Matthew Zeiler and Rob Fergus of 4 features learned by image classifiers: corners, circles, dog faces, and wheels\n\n\nWhen neural architecture search discovers a new architecture, you must learn weights for that architecture from scratch, while with transfer learning, you begin with existing weights from a pre-trained model. In this sense, you you can’t use neural architecture search and transfer learning on the same problem: if you’re learning a new architecture, you would need to train new weights for it; whereas if you are using transfer learning on a pretrained model, you can’t make substantial changes to the architecture.\nOf course, you can apply transfer learning to an architecture learned through neural architecture search (which I think is a good idea!). This requires only that a few researchers use neural architecture search and open-source the models that they find. It is not necessary for all machine learning practitioners to be using neural architecture search themselves on all problems when they can instead use transfer learning. However, Jeff Dean’s keynote, Sundar Pichai’s blog post, Google Cloud’s promotional materials, and the media coverage all suggest the opposite: that everybody needs to be able to use neural architecture search directly.\n\nWhat Neural Architecture Search is good for\n\nNeural architecture search is good for finding new architectures! Google’s AmoebaNet was learned via neural architecture search, and (with the inclusion of fast.ai advances such as an aggressive learning schedule and changing the image size as training progresses) is now the cheapest way to train ImageNet on a single machine!\nAmoebaNet was not designed with a reward function that involved the ability to scale, and so it didn’t scale as well as ResNet to multiple machines, but a neural net that scales well could potentially be learned in the future, optimized for different qualities.\n\nIn need of more evidence\n\nWe haven’t seen evidence that every dataset would be best modeled with its own custom model, as opposed to instead fine-tuning an existing model. Since neural architecture search requires a larger training set, this would particularly be an issue for smaller data sets. Even some of Google’s own research uses transferable techniques instead of finding a new architecture for each data set, such as NASNet (blog post here), which learned an architectural building block on Cifar10 and then used that building block to create an architecture for ImageNet. I don’t know of any widely-entered machine learning competitions that have been won using neural architectures search yet.\nFurthermore, we don’t know that the mega-computationally expensive approach to neural architecture search that Google touts is the superior approach. For instance, more recent papers such as Efficient Neural Architecture Search (ENAS) and\nDifferentiable architecture search (DARTS) propose significantly more efficient algorithms. DARTS takes just 4 GPU days, compared to 1800 GPU days for NASNet and 3150 GPU days for AmoebaNet (all learned to the same accuracy on Cifar-10). Jeff Dean is an author on the ENAS paper, which proposed a technique that is 1000x less computationally expensive, which seems inconsistent with his emphasis at the TF DevSummit one month later on using approaches that are 100x more computationally expensive.\n\nThen why all the hype about Google’s AutoML?\n\nGiven the above limitations, why has Google AutoML’s hype been so disproportionate to its proven usefulness (at least so far)? I think there are a few explanations:\n\nGoogle’s AutoML highlights some of the dangers of having an academic research lab embedded in a for-profit corporation. There is a temptation to try to build products around interesting academic research, without assessing if they fulfill an actual need. This is also the story of many AI start-ups, such as MetaMind or Geometric Intelligence, that end up as acquihires without ever having produced a product. My advice for startup founders is to avoid productionizing your PhD thesis and to avoid hiring only academic researchers.\nGoogle excels at marketing. Artificial intelligence is seen as an inaccessible and intimidating field by many outsiders, who don’t feel that they have a way to evaluate claims, particularly from lionized companies like Google. Many journalists fall prey to this as well, and uncritically channel Google’s hype into glowing articles. I periodically talk to people that do not work in machine learning, yet are excited about various Google ML products that they’ve never used and can’t explain anything about.\nOne example of Google’s misleading coverage of its own achievements occurred when Google AI researchers released “a deep learning technology to reconstruct the true human genome”, compared their own work to Nobel prize-winning discoveries (the hubris!), and the story was picked up by Wired. However, Steven Salzberg, a distinguished professor of Biomedical Engineering, Computer Science, and Biostatistics at Johns Hopkins University debunked Google’s post. Salzberg pointed out that the research didn’t actually reconstruct the human genome and was “little more than an incremental improvement over existing software, and it might be even less than that.” A number of other genomics researchers chimed in to agree with Salzberg.\nThere is some great work happening at Google, but it would be easier to appreciate if we didn’t have to sift through so much misleading hype to figure out what is legitimate.\n\n\n\nGoogle’s DeepVariant “is little more than an incremental improvement over existing software, and it might be even less than that.” @StevenSalzberg1 What do others genomics researchers think?https://t.co/vaAECQhvSi\n\n— Rachel Thomas (@math_rachel) December 12, 2017\n\n\n\nGoogle has a vested interest in convincing us that the key to effective use of deep learning is more computational power, because this is an area where they clearly beat the rest of us. AutoML is often very computationally expensive, such as in the examples of Google using 450 K40 GPUs for 7 days (the equivalent of 3150 GPU days) to learn AmoebaNet.\nWhile engineers and the media often drool over bare-metal power and anything bigger, history has shown that innovation is often birthed instead by constraint and creativity. Google works on the biggest data possible using the most expensive computers possible; how well can this really generalize to the problems that the rest of us face living in a constrained world of limited resources?\nInnovation comes from doing things differently, not from doing things bigger. The recent success of fast.ai in the Stanford DAWNBench competition is one example of this.\n\n\n\n\nInnovation come from doings things differently, not doing things bigger. @jeremyphoward https://t.co/3TJYs8OCbr pic.twitter.com/I55a6gT1OF\n\n— Rachel Thomas (@math_rachel) May 2, 2018\n\n\n\n\nHow can we address the shortage of machine learning expertise?\n\nTo return to the issue that Jeff Dean raised in his TensorFlow DevSummit keynote about the global shortage of machine learning practitioners, a different approach is possible. We can remove the biggest obstacles to using deep learning in several ways by:\n\nmaking deep learning easier to use\ndebunking myths about what it takes to do deep learning\nincreasing access for people that lack the money or credit cards needed to use a cloud GPU\n\n\nMaking Deep Learning Easier to Use\n\nResearch to make deep learning easier to use has a huge impact, making it faster and simpler to train better networks. Examples of exciting discoveries that have now become standard practice are:\n\nDropout allows training on smaller datasets without over-fitting.\nBatch normalization allows for faster training.\nRectified linear units help avoid gradient explosions.\n\nNewer research to improve ease of use includes: - The learning rate finder makes the training process more robust. - Super convergence speeds up training, requiring fewer computational resources. - “Custom heads” for existing architectures (e.g. modifying ResNet, which was initially designed for classification, so that it can be used to find bounding boxes or perform style transfer) allow for easier architecture reuse across a range of problems.\nNone of the above discoveries involve bare-metal power; instead, all of them were creative ideas of ways to do things differently.\n\nAddress Myths About What it Takes to Do Deep Learning\n\nAnother obstacle is the many myths that cause people to believe that deep learning isn’t for them: falsely believing that their data is too small, that they don’t have the right education or background, or that their computers aren’t big enough. One such myth says that only machine learning PhDs are capable of using deep learning, and many companies that can’t afford to hire expensive experts don’t even bother trying. However, it’s not only possible for companies to train the employees they already have to become machine learning experts, it’s even preferable, because your current employees already have domain expertise for the area you work in!\nFor the vast majority of people I talk with, the barriers to entry for deep learning are far lower than they expected: one year of coding experience and access to a GPU.\n\nIncreasing Access: Google Colab Notebooks\n\nAlthough the cost of cloud GPUs (around 50 cents per hour) are within the budgets of many of us, I’m periodically contacted by students from around the world that can’t afford any GPU use at all. In some countries, rules about banking and credit cards can make it difficult for students to use services like AWS, even when they have the money. Google Colab notebooks are a solution! Colab notebooks provide a Jupyter notebook environment that requires no setup to use, runs entirely in the cloud, and gives users access to a free GPU (although long-running GPU use is not allowed). They can also be used to create documentation that contains working code samples running in an interactive environment. Google colab notebooks will do much more to democratize deep learning than Google’s AutoML will; perhaps this would be a better target for Google’s marketing machine in the future.\nIf you haven’t already, check out Part 1: What is it that machine learning practitioners do? and Part 2: An Opinionated Introduction to AutoML Neural Architecture Search of this series."
  },
  {
    "objectID": "posts/2019-01-29-five-scary-things/index.html",
    "href": "posts/2019-01-29-five-scary-things/index.html",
    "title": "Five Things That Scare Me About AI",
    "section": "",
    "text": "AI is being increasingly used to make important decisions. Many AI experts (including Jeff Dean, head of AI at Google, and Andrew Ng, founder of Coursera and deeplearning.ai) say that warnings about sentient robots are overblown, but other harms are not getting enough attention. I agree. I am an AI researcher, and I’m worried about some of the societal impacts that we’re already seeing. In particular, these 5 things scare me about AI:\n\n\nAlgorithms are often implemented without ways to address mistakes.\n\n\nAI makes it easier to not feel responsible.\n\n\nAI encodes & magnifies bias.\n\n\nOptimizing metrics above all else leads to negative outcomes.\n\n\nThere is no accountability for big tech companies.\n\n\nAt the end, I’ll briefly share some positive ways that we can try to address these.\nBefore we dive in, I need to clarify one point that is important to understand: algorithms (and the complex systems they are a part of) can make mistakes. These mistakes come from a variety of sources: bugs in the code, inaccurate or biased data, approximations we have to make (e.g. you want to measure health and you use hospital readmissions as a proxy, or you are interested in crime and use arrests as a proxy. These things are related, but not the same), misunderstandings between different stakeholders (policy makers, those collecting the data, those coding the algorithm, those deploying it), how computer systems interact with human systems, and more.\nThis article discusses a variety of algorithmic systems. I don’t find debates about definitions particularly interesting, including what counts as “AI” or if a particular algorithm qualifies as “intelligent” or not. Please note that the dynamics described in this post hold true both for simpler algorithms, as well as more complex ones.\n\n\nAlgorithms are often implemented without ways to address mistakes.\n\n\nAfter the state of Arkansas implemented software to determine people’s healthcare benefits, many people saw a drastic reduction in the amount of care they received, but were given no explanation and no way to appeal. Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week, transforming her life for the worse. Eventually, a lengthy court case uncovered errors in the software implementation, and Tammy’s hours were restored (along with those of many others who were impacted by the errors).\nObservations of 5th grade teacher Sarah Wysocki’s classroom yielded positive reviews. Her assistant principal wrote, “It is a pleasure to visit a classroom in which the elements of sound teaching, motivated students and a positive learning environment are so effectively combined.” Two months later, she was fired by an opaque algorithm, along with over 200 other teachers. The head of the PTA and a parent of one of Wyscoki’s students described her as “One of the best teachers I’ve ever come in contact with. Every time I saw her, she was attentive to the children, went over their schoolwork, she took time with them and made sure.” That people are losing needed healthcare without an explanation or being fired without explanation is truly dystopian!\n\n\n\nHeadlines from the Verge and the Washington Post\n\n\nAs I covered in a previous post, people use outputs from algorithms differently than they use decisions made by humans: - Algorithms are more likely to be implemented with no appeals process in place. - Algorithms are often used at scale. - Algorithmic systems are cheap. - People are more likely to assume algorithms are objective or error-free. As Peter Haas said, “In AI, we have Milgram’s ultimate authority figure,” referring to Stanley Milgram’s famous experiments showing that most people will obey orders from authority figures, even to the point of harming or killing other humans. How much more likely will people be to trust algorithms perceived as objective and correct?\nThere is a lot of overlap between these factors. If the main motivation for implementing an algorithm is cost-cutting, adding an appeals process (or even diligently checking for errors) may be considered an “unnecessary” expense. Cathy O’Neill, who earned her math PhD at Harvard, wrote a book Weapons of Math Destruction, in which she covers how algorithms are disproportionately impacting poor people, whereas the privileged are more likely to still have access to human attention (in hiring, education, and more).\n\n\nAI makes it easier to not feel responsible.\n\n\nLet’s return to the case of the buggy software used to determine health benefits in Arkansas. How could this have been prevented? In order to prevent severely disabled people from mistakenly losing access to needed healthcare, we need to talk about responsibility. Unfortunately, complex systems lend themselves to a dynamic in which nobody feels responsible for the outcome.\nThe creator of the algorithm for healthcare benefits, Brant Fries (who has been earning royalties off this algorithm, which is in use in over half the 50 states), blamed state policy makers. I’m sure the state policy makers could blame the implementers of the software. When asked if there should be a way to communicate how the algorithm works to the disabled people losing their healthcare, Fries callously said, “It’s probably something we should do. Yeah, I also should probably dust under my bed,” and then later clarified that he thought it was someone else’s responsibility.\nThis passing of the buck and failure to take responsibility is common in many bureaucracies. As danah boyd observed, “Bureaucracy has often been used to shift or evade responsibility. Who do you hold responsible in a complex system?” Boyd gives the examples of high-ranking bureaucrats in Nazi Germany, who did not see themselves as responsible for the Holocaust. boyd continues, “Today’s algorithmic systems are extending bureaucracy.”\nAnother example of nobody feeling responsible comes from the case of research to classify gang crime. A database of gang members assembled by the Los Angeles Police Department (and 3 other California law enforcement agencies) was found to have 42 babies who were under the age of 1 when added to the gang database (28 were said to have admitted to being gang members). Keep in mind these are just some of the most obvious errors- we don’t know how many other people were falsely included.\nI don’t bring this up for the primary purpose of pointing fingers or casting blame. However, a world of complex systems in which nobody feels responsible for the outcomes (which can include severely disabled people losing access to the healthcare they need, or innocent people being labeled as gang members) is not a pleasant place. Our work is almost always a small piece of a larger whole, yet a sense of responsibility is necessary to try to address and prevent negative outcomes.\n\n\nAI encodes & magnifies bias.\n\n\nBut isn’t algorithmic bias just a reflection of how the world is? I get asked a variation of this question every time I give a talk about bias. To which my answer is: No, our algorithms and products impact the world and are part of feedback loops. Consider an algorithm to predict crime and determine where to send police officers: sending more police to a particular neighhorhood is not just an effect, but also a cause. More police officers can lead to more arrests in a given neighborhood, which could cause the algorithm to send even more police to that neighborhood (a mechanism described in this paper on runaway feedback loops).\nBias is being encoded and even magnified in a variety of applications: - software used to decide prison sentences that has twice as high a false positive rate for Black defendents as for white defendents - computer vision software from Amazon, Microsoft, and IBM performs significantly worse on people of color\n[Research by Joy Buolamwini and Timnit Gebru found that commercial computer vision software performed significantly worse on women with dark skin. Gendershades.org]{gendershades3.png){width=60%}\n\nWord embeddings, which are a building block for language tools like Gmail’s SmartReply and Google Translate, generate useful analogies such as Rome:Italy :: Madrid:Spain, as well as biased analogies such as man:computer programmer :: woman: homemaker.\nMachine learning used in recruiting software developed at Amazon penalized applicants who attended all-women’s colleges, as well as any resumes that contained the word “women’s.”\nOver 2/3 of the images in ImageNet, the most studied image data set in the world, are from the Western world (USA, England, Spain, Italy, Australia).\n\n\n\n\nChart from ‘No Classification without Representation’ by Shankar, et. al, shows the origin of ImageNet photos: 45% US, 8% UK, 6% Italy, 3% Canada, 3% Australia, 3% Spain,…\n\n\nSince a Cambrian explosion of machine learning products is occuring, the biases that are calcified now and in the next few years may have a disproportionately huge impact for ages to come (and will be much harder to undo decades from now).\n\n\nOptimizing metrics above all else leads to negative outcomes.\n\n\nWorldwide, people watch 1 billion hours of YouTube per day (yes, that says PER DAY). A large part of YouTube’s successs has been due to its recommendation system, in which a video selected by an algorithm automatically begin playing once the previous video is over. Unfortunately, these recommendations are disproportionately for conspiracy theories promoting white supremacy, climate change denial, and denial of the mass shootings that plague the USA. What is going on? YouTube’s algorithm is trying to maximize how much time people spend watching YouTube, and conspiracy theorists watch significantly more YouTube than people who trust a variety of media sources. Unfortunately, a recommendation system trying only to maximize time spent on its own platform will incentivize content that tells you the rest of the media is lying.\n“YouTube may be one of the most powerful radicalizing instruments of the 21st century,” Professor Zeynep Tufekci wrote in the New York Times. Guillaume Chaslot is a former YouTube engineer turned whistleblower. He has been outspoken about the harms caused by YouTube, and he partnered with the Guardian and the Wall Street Journal to study the extremism and bias in YouTube’s recommendations.\n[Photo of Guillaume Chaslot from the Guardian article]{chaslot.png){width=60%}\nYouTube is owned by Google, which is earning billions of dollars by aggressively introducing vulnerable people to conspiracy theories, while the rest of society bears the externalized costs of rising authoritarian governments, a resurgence in white supremacist movements, failure to act on climate change (even as extreme weather is creating increasing numbers of refugees), growing distrust of mainstream news sources, and a failure to pass sensible gun laws.\nThis problem is an example of the tyranny of metrics: metrics are just a proxy for what you really care about, and unthinkingly optimizing a metric can lead to unexpected, negative results. One analog example is that when the UK began publishing the success rates of surgeons, heart surgeons began turning down risky (but necessary) surgeries to try to keep their scores as high as possible.\nReturning to the account of the popular 5th grade teacher who was fired by an algorithm, she suspects that the underlying reason she was fired was that her incoming students had unusually high test scores the previous year (making it seem like their scores had dropped to a more average level after her teaching), and that their former teachers may have cheated. As USA education policy began over-emphasizing student test scores as the primary way to evaluate teachers, there have been widespread scandals of teachers and principals cheating by altering students scores, in Georgia, Indiana, Massachusetts, Nevada, Virginia, Texas, and elsewhere. When metrics are given undue importance, attempts to game those metrics become common.\n\n\nThere is no accountability for big tech companies.\n\n\nMajor tech companies are the primary ones driving AI advances, and their algorithms impact billions of people. Unfortunately, these companies have zero accountability. YouTube (owned by Google) is helping to radicalize people into white supremacy. Google allowed advertisers to target people who search racist phrases like “black people ruin neighborhoods” and Facebook allowed advertisers to target groups like “jew haters”. Amazon’s facial recognition technology misidentified 28 members of congress as criminals, yet it is already in use by police departments. Palantir’s predictive policing technology was used for 6 years in New Orleans, with city council members not even knowing about the program, much less having any oversight. The newsfeed/timeline/recommendation algorithms of all the major platforms tend to reward incendiary content, prioritizing it for users.\nIn early 2018, the UN ruled that Facebook had played a “determining role” in the ongoing genocide in Myanmar. “I’m afraid that Facebook has now turned into a beast,” said the UN investigator. This result was not a surprise to anyone who had been following the situation in Myanmar. People warned Facebook executives about how the platform was being used to spread dehumanizing hate speech and incite violence against an ethnic minority as early as 2013, and again in 2014 and 2015. As early as 2014, news outlets such as Al Jazeera were covering Facebook’s role in inciting ethnic violence in Myanmar.\nOne person close to the case said, “That’s not 20/20 hindsight. The scale of this problem was significant and it was already apparent.” Facebook execs were warned in 2015 that Facebook could play the same role in Myanmar that radio broadcasts had played during the 1994 Rwandan genocide. As of 2015, Facebook only employed 4 contractors who spoke Burmese (the primary language in Myanmar).\nContrast Facebook’s inaction in Myanmar with their swift action in Germany after the passage of a new law, which could have resulted in penalties of up to 50 million euros. Facebook hired 1,200 German contractors in under a year. In 2018, five years after Facebook was first warned about how they were being used to incite violence in Myanmar, they hired “dozens” of Burmese contractors, a fraction of their response in Germany. The credible threat of a large financial penalty may be the only thing Facebook responds to.\nWhile it can be easy to focus on regulations that are misguided or ineffective, we often take for granted safety standards and regulations that have largely worked well. One major success story comes from automobile safety. Early cars had sharp metal knobs on dashboard that lodged in people’s skulls during crashes, plate glass windows that shattered dangerously, and non-collapsible steering columns that would frequently impale drivers. Beyond that, there was a widespread belief that the only issue with cars was the people driving them, and car manufactures did not want data on car safety to be collected. It took consumer safety advocates decades to push the conversation to how cars could be designed with greater safety, and to pass laws regarding seat belts, driver licenses, crash tests, and the collection of car crash data. For more on this topic, Datasheets for Datasets covers cases studies of how standardization came to the electronics, pharmaceutical, and automobile industries, and 99% Invisible has a deep dive on the history of car safety (with parallels and contrasts to the gun industry).\n\nHow We Can Do Better\n\nThe good news: none of the problems listed here are inherent to algorithms! There are ways we can do better:\n\nMake sure there is a meaningful, human appeals process. Plan for how to catch and address mistakes in advance.\nTake responsibility, even when our work is just one part of the system.\nBe on the lookout for bias. Create datasheets for data sets.\nChoose not to just optimize metrics.\nPush for thoughtful regulations and standards for the tech industry.\n\nThe problems we are facing can feel scary and complex. However, it is still very early on in this age of AI and increasing algorithmic automation. Now is a great time to take action: we can change our culture, cultivate a greater sense of responsibility for our work, seek out thoughtful accountability to counterbalance the inordinate power that major tech companies have, and choose to create more humane products and systems. Technology is just a tool, and it can be used for good or bad. Let’s work to use it for good, to improve the lives of many, rather than just generate wealth for a small number of people.\n\nRelated Posts\n\nYou may be interested in these related posts on tech and ethics:\n\nAI Ethics Resources\nWhat HBR Gets Wrong About Algorithms and Bias\nTech’s long hours are discriminatory & counter-productive\nArtificial Intelligence Needs All of Us (TEDx talk)"
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html",
    "href": "posts/2019-02-12-long-hours/index.html",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "",
    "text": "Whether you realize it or not, you are likely interacting with ill or disabled people regularly. According to recent survey data, a high portion of the U.S. workforce reports having a disability (30 percent), even though a much smaller percentage says they’ve self-identified as disabled to their employer (only 3.2 percent). Often, these illnesses and disabilities are impossible for others to observe, so many people choose to keep their conditions a secret from managers and co-workers to avoid discrimination.\nHealth is not binary; it can fluctuate and is subjective. I have experienced a number of health challenges, including having brain surgery twice (once while pregnant) and one life-threatening brain infection (which can take years to recover from). Trust me when I say that you can’t assess someone’s health based on their appearance or mood. And yet, over one-third of people with disabilities say they have experienced negative bias in their current job.\nI work in the tech industry, where there is an overt glorification — and in many cases, a requirement — of working unhealthily long hours. This is in spite of research showing that putting in longer hours doesn’t lead to greater productivity and instead is harmful. And when you’re ill or disabled and working in this field, the long hours can be not just counterproductive but discriminatory.\n\n\n\nA banner from 1856 reads, “8 hours labour, 8 hours recreation, 8 hours rest.” Source: Wikimedia"
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#fewer-hours-in-the-day",
    "href": "posts/2019-02-12-long-hours/index.html#fewer-hours-in-the-day",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "Fewer hours in the day",
    "text": "Fewer hours in the day\nMany people with chronic illnesses or disabilities simply have fewer hours in the day. We may need more sleep than comparatively healthy people — and yet still wake up feeling awful — as well as have to carefully budget limited energy. Conditions often require frequent doctor visits, blood tests, MRIs, physical therapy, and other appointments, plus there’s dealing with the administrative burden of managing scheduling, billing, and insurance claims, all of which frequently involve errors.\nIn an episode of the podcast No End in Sight, which is focused on chronic illness, a front-end software engineer named John pinpointed his experience feeling time-crunched. John has bipolar disorder and Fabry disease, a rare genetic disorder that causes reduced kidney function and chronic pain and requires him to get regular IV infusion treatments. He described being told during a job interview at Microsoft that he needed to spend more of his free time coding:\n\nI really felt looked down on as being lazy. And really, I’m not lazy. I have chronic illness, and I’m trying to do the best — like, I’m not trying to push myself too hard because I don’t want to throw myself into a bipolar tailspin. And I also don’t want to hurt my hands and have it be even worse to type… I was told by this abled person how to go about living assuming that I was abled, and it was just really frustrating. I’ve contributed at least a thousand hours to open source, and I’m supposed to just keep doing more. When does it end?\n\nNatasha Walton, who founded the Tech Disability Project, has fibromyalgia and post-traumatic stress disorder. She noted on Twitter that certain aspects of the day, like sleep and fitness routines, are not optional for her. “They account for the time I spend meeting my body’s basic needs each and every day so that I can participate in the wider world,” she explained.\nThe tech work environment is hostile even to healthy people. The “ideal worker” in tech is in perfect health, child-free, and has no other commitments. I’ve had several jobs in tech that I could do for a time, and even do quite well, but that I knew would be unsustainable for me long-term. The question was not if I would burn out, but when. Numerous co-workers have also seemed on the brink of burnout regardless of whether they had a chronic illness. I even have tech-industry friends who developed permanent chronic illnesses while in toxic work environments.\nThere are companies where people like me would not be welcome based on unreasonable employee demands. Last year, Andrew Ng’s deeplearning.ai posted a controversial job ad that not only specified that employees typically spend 70–90 hours per week working and studying (later changed to 70+ hours), but that doing so is the natural consequence of believing you can change the world. Many companies operate on this assumption, even if most are not quite so frank about it.\nElon Musk posted a declaration that to change the world, people need to work 80 hours per week, peaking above 100 at times. Uber formerly had an explicit company value to “work harder, longer, and smarter” and served dinner at 8:15 p.m. “Working seven days a week, sometimes until 1 or 2 a.m., was considered normal,” said one former employee. A New York Times article about Amazon described “marathon conference calls on Easter Sunday and Thanksgiving, criticism from bosses for spotty Internet access on vacation, and hours spent working at home most nights or weekends,” as well as employees being given low-performance ratings directly after cancer treatment, major surgeries, or giving birth to a stillborn child."
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#the-research-on-productivity",
    "href": "posts/2019-02-12-long-hours/index.html#the-research-on-productivity",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "The research on productivity",
    "text": "The research on productivity\nAs much as possible, we need to get away from the shallow idea that the quantity of time worked is what matters. The tech industry’s obsession with ridiculously long hours is not only inaccessible to many disabled people and harmful to everyone’s health and relationships, but as Olivia Goldhill pointed out for Quartz at Work, research on productivity suggests it’s just inefficient:\n\nAs countless studies have shown, this simply isn’t true. Productivity dramatically decreases with longer work hours, and completely drops off once people reach 55 hours of work a week, to the point that, on average, someone working 70 hours in a week achieves no more than a colleague working 15 fewer hours.\n\nAlex Soojung-Kim Pang’s book Rest covers the crucial role that leisure time and downtime play in our creativity, health, and productivity. Prolific, talented figures including Charles Darwin, Henri Poincaré, G.H. Hardy, mathematician Paul Halmos, Charles Dickens, and many others were known to engage in only four or five hours of highly concentrated work per day. Pang also highlights an overlooked aspect of the “rule” popularized by Malcolm Gladwell that to become an expert takes 10,000 hours of practice. Gladwell based it on psychologist K. Anders Ericsson’s study of top musical performers, but Pang observes that the top performers also slept more and took afternoon naps:\n\nWe’ve come to believe that world-class performance comes after 10,000 hours of practice. But that’s wrong. It comes after 10,000 hours of deliberate practice, 12,500 hours of deliberate rest, and 30,000 hours of sleep.\n\nMore support for rest-boosted productivity is detailed in a Harvard Business Review roundup titled “The Research Is Clear: Long Hours Backfire for People and for Companies.” It highlights a variety of other study results:\n\nManagers could not tell the difference between employees who worked 80-hour weeks and those who pretended to — though they still penalized employees who were open about working less.\nOverwork is linked to impaired sleep, and sleep deprivation has long been known to lengthen reaction time, interfere with problem-solving, and even induce an impairment equivalent to being drunk.\nDepression, heavy drinking, diabetes, memory problems, heart disease, and poorer judgment calls are all repercussions tied to being overworked.\nPredictable, required time off (like nights and weekends) make teams more productive.\n\n\n\n\nRoman timekeeping: Four clocks show the amount of night and day at times of the year. Image from Wikimedia"
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#the-importance-of-flexible-work-environments",
    "href": "posts/2019-02-12-long-hours/index.html#the-importance-of-flexible-work-environments",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "The importance of flexible work environments",
    "text": "The importance of flexible work environments\nAccommodations, even simple ones, can mean a world of difference to employees with illnesses or disabilities. Brianne Benness, founder of the No End in Sight podcast mentioned above, has written about how a flexible job with remote work helped her stay employed during her at-the-time undiagnosed illness: “When I woke up in a lot of pain, I could tell my boss I was working from home… When the pain in my neck made it too distracting to sit at my desk, I could move to a couch and lie down with my head supported.”\nBut when Brianne switched to a job with a more rigid in-office schedule, both her health and work level plummeted. With nowhere to lie down with her laptop, she would not only get distracted by the pain but put her focus on trying to seem productive. Meanwhile, she lost energy fast during the day, struggled with brain fog, and would go to bed as soon as she got home. She nailed the conundrum in explaining, “I know that when my brain is firing on all cylinders, I can get more done in five hours than I can get done in a full week when my brain is plodding. But I don’t know how to share that value with an employer.”\n\nThe tech industry problems of exclusion, discrimination, and unhealthy work habits run deep, but there is also a widespread appetite for change.\n\nSome business leaders and employers are recognizing the value on their own. A Harvard Business Review article details an experiment by Stanford professor Nicholas Bloom and Ctrip travel website cofounder James Liang in which they let half of Ctrip’s employees work from home for nine months. They found that the group working from home was both more productive and only half as likely to quit as other employees. Bloom said he was blown away by the results, and the benefits of flexible work were much greater than he expected.\nMassive employers like PricewaterhouseCoopers are experimenting, too. PwC is the second largest professional services firm in the world, and last year, it announced a new flexible work program in which potential employees can choose how many hours per week or how many months per year they are available to work. It seems to have created a valuable appeal in recruiting. When one of the company’s leaders, Anne Donovan, shared her advice on switching to a more flexible culture, she asserted that everyone deserves the same degree of flexibility and that culture comes from the top."
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#we-have-a-long-way-to-go",
    "href": "posts/2019-02-12-long-hours/index.html#we-have-a-long-way-to-go",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "We have a long way to go",
    "text": "We have a long way to go\nIn her keynote at GopherCon, Google developer advocate Julia Ferraioli pointed out that, in tech, we often make products accessible but not the processes or the teams we use to build them. True inclusion means having disabled people on your team; the people creating the technology need to be representative of the people using technology, which, increasingly, is everyone.\nIn previous posts, I have shared extensive research on how gender and racial bias manifest in the tech industry, including in retention, promotions, onboarding, and hiring. But in researching bias around disability in the field, I found far less, and this is in part because tech companies aren’t tracking it. When a TechCrunch reporter, who has a severe disability, asked Intel, Apple, Twitter, Facebook, Slack, Google, and Salesforce why none of them included disability in their diversity reports, the companies gave evasive, off-the-record responses. Since the zeroth step to increasing inclusion is to understand the scope and details of the problem, this is an indicator we have a long way to go.\nThe ideas are out there, though. Ted Kennedy Jr., an attorney and state senator who lost his leg as a child due to cancer, recently wrote about common, straightforward themes among companies that are inclusive of people with disabilities:\n\nThey hire people with disabilities.\nThey encourage and advance those employees.\nThey provide accessible tools and technology and have a formal accommodations program.\nThey empower those employees with mentoring and coaching initiatives (Note: Not all mentorship is the same. Research has shown that public endorsement of a mentee’s authority and championing their ideas is far more effective than advice on how the mentee should change and gain self-knowledge.)\n\nI certainly can’t and don’t speak for everyone with chronic illnesses or disabilities, and I encourage you to listen and read the accounts of others. The tech industry problems of exclusion, discrimination, and unhealthy work habits run deep, but there is also a widespread appetite for change. Reconsider the culture at your workplace by hiring and promoting people with disabilities, de-emphasizing hours spent working in favor of quality of work, and allowing a more flexible setup.\nThis post was originally published Feb 12, 2019 on Medium. This is a follow-up to an_ earlier post I wrote on how the tech industry is failing people with disabilities and chronic illnesses.\nThank you to Julia Ferraioli, Jeremy Howard, and Negar Rostamzadeh for giving me valuable feedback on earlier drafts of this post."
  },
  {
    "objectID": "posts/2019-08-07-surveillance/index.html",
    "href": "posts/2019-08-07-surveillance/index.html",
    "title": "8 Things You Need to Know about Surveillance",
    "section": "",
    "text": "Over 225 police departments have partnered with Amazon to have access to Amazon’s video footage obtained as part of the “smart” doorbell product Ring, and in many cases these partnerships are heavily subsidized with taxpayer money. Police departments are allowing Amazon to stream 911 call information directly in real-time, and Amazon requires police departments to read pre-approved scripts when talking about the program. If a homeowner doesn’t want to share data from their video camera doorbell with police, an officer for the Fresno County Sheriff’s Office said they can just go directly to Amazon to obtain it. This creation of an extensive surveillance network, the murky private-public partnership surrounding it, and a lack of any sort of regulations or oversight is frightening. And this is just one of many examples related to surveillance technology that have recently come to light.\nI frequently talk with people who are not that concerned about surveillance, or who feel that the positives outweigh the risks. Here, I want to share some important truths about surveillance:\n\n\nSurveillance can facilitate human rights abuses and even genocide\n\n\nData is often used for different purposes than why it was collected\n\n\nData often contains errors\n\n\nSurveillance typically operates with no accountability\n\n\nSurveillance changes our behavior\n\n\nSurveillance disproportionately impacts the marginalized\n\n\nData privacy is a public good\n\n\nWe don’t have to accept invasive surveillance\n\n\nWhile I was writing this post, a number of investigative articles came out with disturbing new developments related to surveillance. I decided that rather than attempt to include everything in one post (which would make it too long and too dense), I would go ahead and share the above facts about surveillance, as they are just a relevant as ever.\n\n\n\nThe last 24 hours:- NYC police using facial recognition on 11 year old kids- Cops are giving Amazon real-time 911 caller data- California Facial Recognition Interconnect- contd facial rec on protesters in Hong Kong- Palantir founder Peter Thiel gets op-ed in NYTimes\n\n— Rachel Thomas (@math_rachel) August 2, 2019\n\n\n\n\n\nSurveillance can facilitate human rights abuses and even genocide\n\n\nThere is a long history of data about sensitive attributes being misused, including the use of the 1940 USA Census to intern Japanese Americans, a system of identity cards introduced by the Belgian colonial government that were later used during the 1994 Rwandan genocide (in which nearly a million people were murdered), and the role of IBM in helping Nazi Germany use punchcard computers to identify and track the mass killing of millions of Jewish people. More recently, the mass internment of over one million people who are part of an ethnic minority in Western China was facilitated through the use of a surveillance network of cameras, biometric data (including images of people’s faces, audio of their voices, and blood samples), and phone monitoring.\n\n\n\nAdolf Hitler meeting with IBM CEO Tom Watson Sr. in 1937. Source: https://www.computerhistory.org/revolution/punched-cards/2/15/109\n\n\nPictured above is Adolf Hitler (far left) meeting with IBM CEO Tom Watson Sr. (2nd from left), shortly before Hitler awarded Watson a special “Service to the Reich” medal in 1937 (for a timeline of the Holocaust, see here). Watson returned the medal in 1940, although IBM continued to do business with the Nazis. IBM technology helped the Nazis conduct detailed censuses in countries they occupied, to thoroughly identify anyone of Jewish descent. Nazi concentration camps used IBM’s punchcard machines to tabulate prisoners, recording whether they were Jewish, gay, or Gypsies, and whether they died of “natural causes,” execution, suicide, or via “special treatment” in gas chambers. It is not the case that IBM sold the machines and then was done with it. Rather, IBM and its subsidiaries provided regular training and maintenance on-site at the concentration camps: printing off cards, configuring machines, and repairing them as they broke frequently.\n\n\nData is often used for different purposes than why it was collected\n\n\nIn the above examples, the data collection began before genocide was committed. IBM began selling to Nazi Germany well before the Holocaust (although continued for far too long), including helping with Germany’s 1933 census conducted by Adolf Hitler, which was effective at identifying far more Jewish people than had previously been recognized in Germany.\nIt is important to recognize how data and images gathered through surveillance can be weaponized later. Columbia professor Tim Wu wrote that “One [hard truth] is that data and surveillance networks created for one purpose can and will be used for others. You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.”\nPlenty of data collection is not involved with such extreme abuse as genocide; however, in a time of global resurgence of white supremacist, ethno-nationalist, and authoritarian movements, it would be deeply irresponsible to not consider how data & surveillance can and will be weaponized against already vulnerable groups.\n\n\nData often has errors (and no mechanism for correcting them)\n\n\nA database of suspected gang members maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). Even worse, there was no process in place for correcting mistakes or removing people once they’ve been added.\nAn NPR reporter recounts his experience of trying to rent an apartment and discovering that TransUnion, one of the 3 major credit bureaus, incorrectly reported him as having two felony firearms convictions. TransUnion only removed the mistakes after a dozen phone calls and notification that the story would be reported on. This is not an unusual story: the FTC’s large-scale study of credit reports in 2012 found 26% of consumers had at least one mistake in their files and 5% had errors that could be devastating. An even more opaque, unregulated “4th bureau” exists: a collection of companies buying and selling personal information about people on the margins of the banking system (such as immigrants, students, and people with low incomes), with no standards on what types of data are included, no way to opt out, and no system for identifying or correcting mistakes.\n\n\nSurveillance typically operates with no accountability\n\n\nWhat makes the examples in the previous section disturbing is not just that errors occurred, but that there was no way to identify or correct them, and no accountability for those profiting off the error-laden data. Often, even the existence of the systems being used is not publicly known (much less details of how these systems work), unless discovered by journalists or revealed by whistleblowers. The Detroit Police Dept used facial recognition technology for nearly two years without public input and in violation of a requirement that a policy be approved by the city’s Board of Police Commissioners, until a study from Georgetown Law’s Center for Privacy & Technology drew attention to the issue. Palantir, the defense startup founded by billionaire Peter Thiel, ran a program with New Orleans Police Department for 6 years which city council members did not even know about, much less have any oversight.\nAfter two studies found that Amazon’s facial recognition software produced inaccurate and racially biased results, Amazon countered that the researchers should have changed the default parameters. However, it turned out that Amazon was not instructing police departments that use its software to do this either. Surveillance programs are operating with few regulations, no oversight, no accountability around accuracy or mistakes, and in many cases, no public knowledge of what is going on.\n\n\nSurveillance changes our behavior\n\n\nHundreds of thousands of people in Hong Kong are protesting an unpopular new bill which would allow extradition to China. Typically, Hong Kong locals use their rechargeable smart cards to ride the subway. However, during the protests, long lines of people waited to use cash to buy paper tickets (usually something that only tourists do) concerned that they would be tracked for having attended the protests. Would fewer people protest if this was not an option?\nIn the United States, in 2015 the Baltimore Police Department used facial recognition technology to surveil people protesting the death of Freddie Grey, a young Black man who was killed in police custody, and arrested protesters with outstanding warrants. Mass surveillance could have a chilling impact on our rights to move about freely, to express ourselves, and to protest. “We act differently when we know we are ‘on the record.’ Mass privacy is the freedom to act without being watched and thus in a sense, to be who we really are,” Columbia professor Tim Wu wrote in the New York Times.\n\n\n\nFlyer from the company Geofeedia. Source: https://www.aclunc.org/docs/20161011_geofeedia_baltimore_case_study.pdf\n\n\n\n\nSurveillance disproportionately impacts those who are already marginalized\n\n\nSurveillance is applied unevenly, causing the greatest harm to people who are already marginalized, including immigrants, people of color, and people living in poverty. These groups are more heavily policed and surveilled. The Perpetual Line-Up from the Georgetown Law Center on Privacy and Technology studied the unregulated use of facial recognition by police, with half of all Americans appearing in law enforcement databases, and the risks of errors, racial bias, misuses, and threats to civil liberties. The researchers pointed out that African Americans are more likely to appear in these databases (many of which are drawn from mug shots) since they are disproportionately likely to be stopped, interrogated, or arrested. For another example, consider the contrast between how easily people over 65 can apply for Medicare benefits by filling out an online form, with the invasive personal questions asked of a low-income mother on Medicaid about her lovers, hygiene, parental shortcomings, and personal habits.\nIn an article titled Trading privacy for survival is another tax on the poor, Ciara Byrne wrote, “Current public benefits programs ask applicants extremely detailed and personal questions and sometimes mandate home visits, drug tests, fingerprinting, and collection of biometric information… Employers of low-income workers listen to phone calls, conduct drug tests, monitor closed-circuit television, and require psychometric tests as conditions of employment. Prisoners in some states have to consent to be voiceprinted in order to make phone calls.”\n\n\nData privacy is a public good, like air quality or safe drinking water\n\n\nData is more revealing in aggregate. It can be nearly impossible to know what your individual data could reveal when combined with the data of others or with data from other sources, or when machine learning inference is performed on it. For instance, as Zeynep Tufekci wrote in the New York Times, individual Strava users could not have predicted how in aggregate their data could be used to identify the locations of US military bases. “Data privacy is not like a consumer good, where you click ‘I accept’ and all is well. Data privacy is more like air quality or safe drinking water, a public good that cannot be effectively regulated by trusting in the wisdom of millions of individual choices. A more collective response is needed.”\nUnfortunately, this also means that you can’t fully safeguard your privacy on your own. You may choose not to purchase Amazon’s ring doorbell, yet you can still show up in the video footage collected by others. You might strengthen your online privacy practices, yet conclusions will still be inferred about you based on the behavior of others. As Professor Tufekci wrote, we need a collective response.\n\n\nWe don’t have to accept invasive surveillance\n\n\nMany people are uncomfortable with surveillance, but feel like they have no say in the matter. While the threats surveillance poses are large, it is not too late to act. We are seeing success: in response to community organizing and an audit, Los Angeles Police Department scrapped a controversial program to predict who is most likely to commit violent crimes. Citizens, researchers, and activists in Detroit have been effective at drawing attention to the Detroit Police Department’s unregulated use of facial recognition and a bill calling for a 5-year moratorium has been introduced to the state legislature. Local governments in San Francisco, Oakland, and Somerville have banned the use of facial recognition by police.\nFor further resources, please check out: - Georgetown Law Center on Privacy and Technology - Digital Defense Playbook"
  },
  {
    "objectID": "posts/2019-09-24-metrics/index.html",
    "href": "posts/2019-09-24-metrics/index.html",
    "title": "The problem with metrics is a big problem for AI",
    "section": "",
    "text": "Update: This post was expanded into a paper, Reliance on metrics is a fundamental challenge for AI, by Rachel Thomas and David Uminsky, which was accepted to the Ethics of Data Science Conference 2020 and to Cell Patterns. The paper version includes more grounding in previous academic work and a framework towards mitigating these harms.\nGoodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure.” At their heart, what most current AI approaches do is to optimize metrics. The practice of optimizing metrics is not new nor unique to AI, yet AI can be particularly efficient (even too efficient!) at doing so.\nThis is important to understand, because any risks of optimizing metrics are heightened by AI. While metrics can be useful in their proper place, there are harms when they are unthinkingly applied. Some of the scariest instances of algorithms run amok (such as Google’s algorithm contributing to radicalizing people into white supremacy, teachers being fired by an algorithm, or essay grading software that rewards sophisticated garbage) all result from over-emphasizing metrics. We have to understand this dynamic in order to understand the urgent risks we are facing due to misuse of AI.\n\n\n\nHeadlines from HBR, Washington Post, and Vice on some of the outcomes of over-optimizing metrics: rewarding gibberish essays, promoting propaganda, massive fraud at Wells Fargo, and firing good teachers\n\n\nThe following principles will be illustrated through a series of case studies:\n\n\nAny metric is just a proxy for what you really care about\n\n\nMetrics can, and will, be gamed\n\n\nMetrics tend to overemphasize short-term concerns\n\n\nMany online metrics are gathered in highly addictive environments\n\n\nMetrics are most likely to be useful when they are treated as one piece of a bigger picture\n\n\n\nWe can’t measure the things that matter most\n\nMetrics are typically just a proxy for what we really care about. The paper Does Machine Learning Automate Moral Hazard and Error? covers an interesting example: the researchers investigate which factors in someone’s electronic medical record are most predictive of a future stroke. However, the researchers found that several of the most predictive factors (such as accidental injury, a benign breast lump, or colonoscopy) don’t make sense as risk factors for stroke. So, just what is going on? It turned out that the model was just identifying people who utilize health care a lot. They didn’t actually have data of who had a stroke (a physiological event in which regions of the brain are denied new oxygen); they had data about who had access to medical care, chose to go to a doctor, were given the needed tests, and had this billing code added to their chart. But a number of factors influence this process: who has health insurance or can afford their co-pay, who can take time off of work or find childcare, gender and racial biases that impact who gets accurate diagnoses, cultural factors, and more. As a result, the model was largely picking out people who utilized healthcare versus who did not.\nThis an example of the common phenomenon of having to use proxies: You want to know what content users like, so you measure what they click on. You want to know which teachers are most effective, so you measure their students test scores. You want to know about crime, so you measure arrests. These things are not the same. Many things we do care about can not be measured. Metrics can be helpful, but we can’t forget that they are just proxies.\nAs another example, Google used hours spent watching YouTube as a proxy for how happy users were with the content, writing on the Google blog that “If viewers are watching more YouTube, it signals to us that they’re happier with the content they’ve found.” Guillaume Chaslot, an AI engineer who formerly worked at Google/YouTube, shares how this had the side effect of incentivizing conspiracy theories, since convincing users that the rest of the media is lying kept them watching more YouTube.\n\nMetrics can, and will, be gamed\n\nIt is almost inevitable that metrics will be gamed, particularly when they are given too much power. One week this spring, Chaslot collected 84,695 videos from YouTube and analyzed the number of views and the number of channels from which they were recommended. This is what he found (also covered in the Washington Post):\n\n\n\nChart showing Russia Today’s video on the Mueller Report as being an outlier in how many YouTube channels recommended it.\n\n\nThe state-owned media outlet Russia Today was an extreme outlier in how much YouTube’s algorithm had selected it to be recommended by a wide-variety of other YouTube channels. Such algorithmic selections, which begin autoplaying as soon as your current video is done, account for 70% of the time that users spend on YouTube. This chart strongly suggests that Russia Today has in some way gamed YouTube’s algorithm. Platforms are rife with attempts to game their algorithms, to show up higher in search results or recommended content, through fake clicks, fake reviews, fake followers, and more.\nAutomatic essay grading software focuses primarily on metrics like sentence length, vocabulary, spelling, and subject-verb agreement, but is unable to evaluate aspects of writing that are hard to quantify, such as creativity. As a result, gibberish essays randomly generated by computer programs to contain lots of sophisticated words score well. Essays from students in mainland China, which do well on essay length and sophisticated word choice, received higher scores from the algorithms than from expert human graders, suggesting that these students may be using chunks of pre-memorized text.\nAs USA education policy began over-emphasizing student test scores as the primary way to evaluate teachers, there have been widespread scandals of teachers and principals cheating by altering students scores, in Georgia, Indiana, Massachusetts, Nevada, Virginia, Texas, and elsewhere. One consequence of this is that teachers who don’t cheat may be penalized or even fired (when it appears student test scores have dropped to more average levels under their instruction). When metrics are given undue importance, attempts to game those metrics become common.\n\nMetrics tend to overemphasize short-term concerns\n\nIt is much easier to measure short-term quantities: click through rates, month-over-month churn, quarterly earnings. Many long-term trends have a complex mix of factors and are tougher to quantify. What is the long-term impact on user trust of having your brand associated with promoting pedophilia, white supremacy, and flat-earth theories? What is the long-term impact on hiring to be the subject of years worth of privacy scandals, political manipulation, and facilitating genocide?\nSimply measuring what users click on is a short-term concern, and does not take into account factors like the potential long-term impact of a long-form investigative article which may have taken months to research and which could help shape a reader’s understanding of a complex issue and even lead to significant societal changes.\nA recent Harvard Business Review article looked at Wells Fargo as a case study of how letting metrics replace strategy can harm a business. After identifying cross-selling as a measure of long-term customer relationships, Wells Fargo went overboard emphasizing the cross-selling metric: intense pressure on employees combined with an unethical sales culture led to 3.5 million fraudulent deposit and credit card accounts being opened without customers’ consent. The metric of cross-selling is a much more short-term concern compared to the loftier goal of nurturing long-term customer relationships. Overemphasizing metrics removes our focus from long-term concerns such as our values, trust and reputation, and our impact on society and the environment, and myopically focuses on the short-term.\n\nMany metrics gather data of what we do in highly addictive environments\n\nIt matters which metrics we gather and in what environment we do so. Metrics such as what users click on, how much time they spend on sites, and “engagement” are heavily relied on by tech companies as proxies for user preference, and are used to drive important business decisions. Unfortunately, these metrics are gathered in environments engineered to be highly addictive, laden with dark patterns, and where financial and design decisions have already greatly circumscribed the range of options.\n\n\n\nOur online environment is a buffet of junk food\n\n\nZeynep Tufekci, a professor at UNC and regular contributor to the New York Times, compares recommendation algorithms (such as YouTube choosing which videos to auto-play for you and Facebook deciding what to put at the top of your newsfeed) to a cafeteria shoving junk food into children’s faces. “This is a bit like an autopilot cafeteria in a school that has figured out children have sweet teeth, and also like fatty and salty foods. So you make a line offering such food, automatically loading the next plate as soon as the bag of chips or candy in front of the young person has been consumed.” As those selections get normalized, the output becomes ever more extreme: “So the food gets higher and higher in sugar, fat and salt – natural human cravings – while the videos recommended and auto-played by YouTube get more and more bizarre or hateful.” Too many of our online environments are like this, with metrics capturing that we love sugar, fat, and salt, not taking into account that we are in the digital equivalent of a food desert and that companies haven’t been required to put nutrition labels on what they are offering. Such metrics are not indicative of what we would prefer in a healthier or more empowering environment.\n\nWhen Metrics are Useful\n\nAll this is not to say that we should throw metrics out altogether. Data can be valuable in helping us understand the world, test hypotheses, and move beyond gut instincts or hunches. Metrics can be useful when they are in their proper context and place. One way to keep metrics in their place is to consider a slate of many metrics for a fuller picture (and resist the temptation to try to boil these down to a single score). For instance, knowing the rates at which tech companies hire people from under-indexed groups is a very limited data point. For evaluating diversity and inclusion at tech companies, we need to know comparative promotion rates, cap table ownership, retention rates (many tech companies are revolving doors driving people from under-indexed groups away with their toxic cultures), number of harassment victims silenced by NDAs, rates of under-leveling, and more. Even then, all this data should still be combined with listening to first-person experiences of those working at these companies.\nColumbia professor and New York Times Chief Data Scientist Chris Wiggins wrote that quantitative measures should always be combined with qualitative information, “Since we can not know in advance every phenomenon users will experience, we can not know in advance what metrics will quantify these phenomena. To that end, data scientists and machine learning engineers must partner with or learn the skills of user experience research, giving users a voice.”\nAnother key to keeping metrics in their proper place is to keep domain experts and those who will be most impacted closely involved in their development and use. Surely most teachers could have foreseen that evaluating teachers primarily on the standardized test scores of their students would lead to a host of negative consequences.\nI am not opposed to metrics; I am alarmed about the harms caused when metrics are overemphasized, a phenomenon that we see frequently with AI, and which is having a negative, real-world impact. AI running unchecked to optimize metrics has led to Google/YouTube’s heavy promotion of white supremacist material, essay grading software that rewards garbage, and more. By keeping the risks of metrics in mind, we can try to prevent these harms."
  },
  {
    "objectID": "posts/2020-08-06-ergonomics/index.html",
    "href": "posts/2020-08-06-ergonomics/index.html",
    "title": "Essential Work-From-Home Advice: Cheap and Easy Ergonomic Setups",
    "section": "",
    "text": "You weren’t expecting to spend 2020 working from home. You can’t afford a fancy standing desk. You don’t have a home office, or even much spare space, in your apartment. Your neck is getting a permanent crick from hunching over your laptop on the couch. While those of us who are able to work from home are privileged to have this option, we still don’t want to permanently damage our backs, necks, or arms from a bad ergonomic setup.\nThis is not a post for ergonomic aficionados (the setups I share could all be further optimized). This is a post for folks who don’t know where to get started, have a limited budget, and are willing to try simple, scrappy approaches. Key takeway: for 34 dollars (21 for a good mouse, and 13 for a cheap keyboard), as well as some household items, you can create an ergonomic setup like the one below. I will show many other options throughout the post, for both sitting and standing, as well as approaches you can easily assemble/disassemble (if you are using the family dinner table and need to clear it off each evening).\n\n\n\nWhile visiting family, I created an ergonomic setup on a counter\n\n\n\nYou can permanently damage your body with bad ergonomics\n\nYou can permanently damage your back, neck, and wrists from working without an ergonomic setup. Almost two decades ago, my partner Jeremy suffered from repetitive stress injury due to working without an ergonomic setup. At the time, his arms were paralyzed and he had to take months off from work. Even now and after years filled with good ergonomics and yoga, this still impacts his life, severely limiting how much time he can spend in cars or on planes, and creating painful flare-ups. Please take this issue seriously.\n\nKey advice: Have a separate keyboard and mouse\n\nThe most important thing to know is that you want your screen approximately at eye height, and your elbows at approximately right angles to your torso as they type and use the mouse. This is the case whether you are sitting or standing. If you are using a laptop, this will be impossible with the built-in keyboard and trackpad (no matter how nice they are). It is essential to have a separate keyboard and mouse. If you only do one thing to address ergonomics, obtain a separate keyboard and mouse.\nIf you can’t afford an external monitor, no worries, you can just elevate your laptop. Over the years, I have used cardboard boxes, drinking glasses, bottles of soda, board games, and stacks of books to elevate my laptop. I will recommend some keyboards and mice that I like below, but anything is better than using the ones built into your laptop (since that forces you to keep your screen at the wrong height). For example, the picture in the intro is of a set-up I created while visiting a family member’s apartment in 2014, using books and a cardboard box to elevate my keyboard, mouse, and laptop to the appropriate heights.\n\n\n\nFor the deep learning study group, I routinely used a brown cardboard box. Bonus: I could store everything in the box when we had the clear out of that room each night.\n\n\nAbove is a picture from the deep learning study group, which meets 5 days a week, for 7 weeks, every time we run the deep learning course. I use a brown cardboard box to elevate my keyboard. We have to clear out of that conference room each evening, and it is simple for me to put my items in the box. This sort of solution could work if you don’t have a dedicated office space in your home, and need to be able to set up/take down your workstation regularly.\nI rarely worked in coffee shops pre-pandemic (and never do now), but when I had to I would still try to create an ergonomic setup (and go to a coffeeshop where there was enough space!). Here, I’ve stacked my laptop on top of my rolled-up backpack. Ideally, my screen would be higher, but this is still better than having it at table level. Don’t let the perfect be the enemy of the good. Every step you take towards a more ergonomic setup is helpful.\n\n\n\nWhen working at a coffee shop (pre-pandemic), I brought an external keyboard and mouse, and used my rolled-up backpack to raise the height of my laptop screen\n\n\n\nAbout standing desks\n\nIf you have a regular desk (or even just a table) at home and want a standing desk, one option is to convert it using the $22 standing desk approach, which involves an Ikea side table and shelf. I had a previous job in which this was quite popular. Here is a photo of my work desk from that time.\n\n\n\nIn a previous job, many of us set up $22 standing desks using Ikea side tables\n\n\nStanding on a hard floor can be difficult for your back. I have a GelPro mat, which I love. If you can’t afford a GelPro mat, standing on a folded-up yoga mat works great too.\nNote that standing desks are not a cure-all. I’ve often seen people with expensive standing-desk converters (also known as desktop risers) that still have their monitor way too low. Even if you have an external monitor and desktop riser, makes sure your monitor is at an appropriate height. It is likely you will still need to stack it on top of something. If you don’t like the aesthetics of using books or other household items, you can buy a monitor stand, such as this one.\nUsing a standing desk with poor posture is not very ergonomic, so be cognizant of when you start feeling fatigued. I prefer to switch between standing and sitting throughout the day, as my energy fluctuates.\n\nBudget Recommendations\n\nMy “budget recommendation” would be to get an Anker vertical mouse for $21 and literally any keyboard. If you have to choose, I’ve found that having a good mouse is way more important than a good keyboard. It is important that you get some keyboard though, so that you can elevate your laptop screen. In the setup below, I’m using a lightweight travel keyboard that isn’t particularly ergonomic, but it works fine.\n\n\n\nThe barista at this coffee shop kindly let me use 2 plastic tubs to prop up my laptop (pre-pandemic).\n\n\nI realize that at a time when many Americans do not have enough to eat, that you may not have 34 dollars to spare (21 dollars for a mouse and 13 dollars for a cheap keyboard). However, if this is an option for you, it is well worth the cost. If you permanently damage your back, neck, or arms, no amount of money may be enough to heal them later.\n\nOther products I like\n\nMy favorite mouse is the Logitech wireless trackball mouse. I have also used and liked the Anker vertical mouse. For keyboards, I like Goldtouch (I use an older version of this one) or the Microsoft Ergonomic Keyboard. And if you are looking for a compact, lightweight travel keyboard, I like the iClever foldup keyboard.\nAs mentioned above, GelPro mats are great if you are going to be standing, and a folded-up yoga mat is a cheaper alternative.\nI have a Roost portable, lightweight laptop stand, which is great, although I can’t use it since I switched from a Macbook Air to a Microsoft Surface Pro. None of the links in this post are affiliate links; I’m just recommending what I’ve personally used and like.\nFor more about home office set-ups, Jeremy recently posted a twitter thread about his preferred computer set-up (which includes some pricier options). It’s also worth noting that his desk has a small footprint, and fits in the corner of our living room.\n\n\n\nI couldn't be happier with my little standing desk setup. I have tried far to many products over the years, and here's what I highly recommend:1/ pic.twitter.com/lMagQPLys1\n\n— Jeremy Howard (@jeremyphoward) July 22, 2020"
  },
  {
    "objectID": "posts/2021-08-17-eleven-ethics-videos/index.html",
    "href": "posts/2021-08-17-eleven-ethics-videos/index.html",
    "title": "11 Short Videos About AI Ethics",
    "section": "",
    "text": "I made a playlist of 11 short videos (most are 6-13 mins long) on Ethics in Machine Learning. This is from my ethics lecture in Practical Deep Learning for Coders v4. I thought these short videos would be easier to watch, share, or skip around.\n\n\n\n\nWhat are Ethics and Why do they Matter? Machine Learning Edition: Through 3 key case studies, I cover how people can be harmed by machine learning gone wrong, why we as machine learning practitioners should care, and what tech ethics are.\nAll machine learning systems need ways to identify & address mistakes. It is crucial that all machine learning systems are implemented with ways to correctly surface and correct mistakes, and to provide recourse to those harmed.\nThe Problem with Metrics, Feedback Loops, and Hypergrowth: Overreliance on metrics is a core problem both in the field of machine learning and in the tech industry more broadly. As Goodhart’s Law tells us, when a measure becomes the target, it ceases to be a good measure, yet the incentives of venture capital push companies in this direction. We see out-of-control feedback loops, widespread gaming of metrics, and people being harmed as a result.\nNot all types of bias are fixed by diversifying your dataset. The idea of bias is often too general to be useful. There are several different types of bias, and different types require different interventions to try to address them. Through a series of cases studies, we will go deeper into some of the various causes of bias.\n\n\n\nPart of the Ethics Videos Playlist\n\n\nHumans are biased too, so why does machine learning bias matter? A common objection to concerns about bias in machine learning models is to point out that humans are really biased too. This is correct, yet machine learning bias differs from human bias in several key ways that we need to understand and which can heighten the impact.\n7 Questions to Ask About Your Machine Learning Project\nWhat You Need to Know about Disinformation: With a particular focus on how machine learning advances can contribute to disinformation, this covers some of the fundamental things to understand.\nFoundations of Ethics: We consider different lenses through which to evaluate ethics, and what sort of questions to ask.\nTech Ethics Practices to Implement at your Workplace: Practical tech ethics practices you can implement at your workplace.\nHow to Address the Machine Learning Diversity Crisis: Only 12% of machine learning researchers are women. Based on research studies, I outline some evidence-based steps to take towards addressing this diversity crisis.\nAdvanced Technology is not a Substitute for Good Policy: We will look at some examples of what incentives cause companies to change their behavior or not (e.g. being warned for years of your role in an escalating genocide vs. threat of a hefty fine), how many AI ethics concerns are actually about human rights, and case studies of what happened when regulation & safety standards came to other industries.\nYou can find the playlist of 11 short videos here. And here is a longer, full-length free fast.ai course on practical data ethics."
  },
  {
    "objectID": "posts/2021-10-12-medicine-political/index.html",
    "href": "posts/2021-10-12-medicine-political/index.html",
    "title": "Medicine is Political",
    "section": "",
    "text": "Experts warn that we are not prepared for the surge in disability due to long covid, an illness that afflicts between one-fourth and one-third of people who get covid, including mild cases, for months afterwards. Some early covid cases have been sick for 18 months, with no end in sight. The physiological damage that covid causes can include cognitive dysfunction and deficits, brain activity scans similar to those seen in Alzheimer’s patients, GI immune system damage, cornea damage, immune dysfunction, increased risk of kidney outcomes, dysfunction in T cell memory generation, pancreas damage, and ovarian failure. Children are at risk too.\nAs the evidence continues to mount of alarming long term physiological impacts of covid, and tens of millions are unable to return to work, we might expect leaders to take covid more seriously. Yet we are seeing concerted efforts to downplay the long-term health effects of covid using strategies straight out of the climate denial playbook, such as funding contrarian scientists, misleading petitions, social media bots, and disingenuous debate tactics that make the science seem murkier than it is. In many cases, these minimization efforts are being funded by the same billionaires and institutions that fund climate change denialism. Dealing with many millions of newly disabled people will be very expensive for governments, social service programs, private insurance companies, and others. Thus, many have a significant financial interest in distorting the science around long term effects of covid to minimize the perceived impact.\nIn topics ranging from covid-19 to HIV research to the long history of wrongly assuming women’s illnesses are psychosomatic, we have seen again and again that medicine, like all science, is political. This shows up in myriad ways, such as: who provides funding, who receives that funding, which questions get asked, how questions are framed, what data is recorded, what data is left out, what categories included, and whose suffering is counted.\nScientists often like to think of their work as perfectly objective, perfectly rational, free from any bias or influence. Yet by failing to acknowledge the reality that there is no “view from nowhere”, they miss their own blindspots and make themselves vulnerable to bad-faith attacks. As one climate scientist recounted of the last 3 decades, “We spent a long time thinking we were engaged in an argument about data and reason, but now we realize it’s a fight over money and power… They [climate change deniers] focused their lasers on the science and like cats we followed their pointer and their lead.”\nThe American Institute for Economic Research (AIER), a libertarian think tank funded by right wing billionaire Charles Koch which invests in fossil fuels, energy utilities, and tobacco, is best known for its research denying the climate crisis. In October 2020, a document called the Great Barrington Declaration (GBD) was developed at a private AIER retreat, calling for a “herd immunity” approach to covid, arguing against lockdowns, and suggesting that young, healthy people have little to worry about. The three scientists who authored the GBD have prestigious pedigrees and are politically well-connected, speaking to White House Officials and having found favor in the British government. One of them, Sunetra Gupta of Oxford, had released a wildly inaccurate paper in March 2020 claiming that up to 68% of the UK population had been exposed to covid, and that there were already significant levels of herd immunity to coronavirus in both the UK and Italy (again, this was in March 2020). Gupta received funding from billionaire conservative donors, Georg and Emily von Opel. Another one of the authors, Jay Bhattacharya of Stanford, co-authored a widely criticized pre-print in April 2020 that relied on a biased sampling method to “show” that 85 times more people in Santa Clara County California had already had covid compared to other estimates, and thus suggested that the fatality rate for covid was much lower than it truly is.\nHalf of the social media accounts advocating for herd immunity seem to be bots, characterized as engaging in abnormally high levels of retweets & low content diversity. An article in the BMJ recently advised that it is “critical for physicians, scientists, and public health officials to realize that they are not dealing with an orthodox scientific debate, but a well-funded sophisticated science denialist campaign based on ideological and corporate interests.”\nThis myth of perfect scientific objectivity positions modern medicine as completely distinct from a history where women were diagnosed with “hysteria” (roaming uterus) for a variety of symptoms, where Black men were denied syphilis treatment for decades as part of a “scientific study”, and multiple sclerosis was “called hysterical paralysis right up to the day they invented a CAT scan machine” and demyelination could be seen on brain scans.\nHowever, there is not some sort of clean break where bias was eliminated and all unknowns were solved. Black patients, including children, still receive less pain medication than white patients for the same symptoms. Women are still more likely to have their physical symptoms dismissed as psychogenic. Nearly half of women with autoimmune disorders report being labeled as “chronic complainers” by their doctors in the 5 years (on average) they spend seeking a diagnosis. All this impacts what data is recorded in their charts, what symptoms are counted.\nMedical data are not objective truths. Like all data, the context is critical. It can be missing, biased, and incorrect. It is filtered through the opinions of doctors. Even blood tests and imaging scans are filtered through the decisions of what tests to order, what types of scans to take, what accepted guidelines recommend, what technology currently exists. And the technology that exists depends on research and funding decisions stretching back decades, influenced by politics and cultural context.\nOne may hope that in 10 years we will have clearer diagnostic tests for some illnesses which remain contested now, just as the ability to identify multiple sclerosis improved with better imaging. In the meantime, we should listen to patients and trust in their ability to explain their own experiences, even if science can’t fully understand them yet.\nScience does not just progress inevitably, independent of funding and politics and framing and biases. A self-fulfilling prophecy often occurs in which doctors: 1. label a new, poorly understood, multi-system disease as psychogenic, 2. use this as justification to not invest much funding into researching physiological origins, 3. and then point to the lack of evidence as a reason why the illness must be psychogenic.\nThis is largely the experience of ME/CFS patients over the last several decades. Myalgic encephalomyelitis (ME/CFS), involves dysfunction of the immune system, autonomic systems, and energy metabolism (including mitochondrial dysfunction, hypoacetylation, reduced oxygen uptake, and impaired oxygen delivery). ME/CFS is more debilitating than many chronic diseases, including chronic renal failure, lung cancer, stroke, and type-2 diabetes. It is estimated 25–29% of patients are homebound or bedbound. ME/CFS is often triggered by viral infections, so it is not surprising that we are seeing some overlap between ME/CFS and long covid. ME/CFS disproportionately impacts women, and a now discredited 1970 paper identified a major outbreak in 1958 amongst nurses at a British hospital as “epidemic hysteria”. This early narrative of ME/CFS as psychogenic has been difficult to shake. Even as evidence continues to accumulate of immune, metabolic, and autonomous system dysfunction, some doctors persist in believing that ME/CFS must be psychogenic. It has remained woefully underfunded: from 2013-2017, NIH funding was only at 7.3% relative commensurate to its disease burden. Note that the below graph is on a log scale: ME/CFS is at 7%, Depression and asthma are at 100% and diseases like cancer and HIV are closer to 1000%.\n\n\n\nGraph of NIH funding on log scale, from above paper by Mirin, Dimmock, Leonard\n\n\nPortraying patients as unscientific and irrational is the other side of the same coin for the myth that medicine is perfectly rational. Patients that disagree with having symptoms they know are physiological dismissed as psychogenic, that reject treatments from flawed studies, or who distrust medical institutions based on their experiences of racism, sexism, and mis-diagnosis, are labeled as “militant” or “irrational”, and placed in the same category with conspiracy theorists and those peddling disinformation.\nOn an individual level, receiving a psychological misdiagnosis lengthens the time it will take to get the right diagnosis, since many doctors will stop looking for physiological explanations. A study of 12,000 rare disease patients covered by the BBC found that “while being misdiagnosed with the wrong physical disease doubled the time it took to get to the right diagnosis, getting a psychological misdiagnosis extended it even more – by 2.5 up to 14 times, depending on the disease.” This dynamic holds true at the disease level as well: once a disease is mis-labeled as psychogenic, many doctors will stop looking for physiological origins.\nWe are seeing increasing efforts to dismiss long covid as psychogenic in high profile platforms such as the WSJ and New Yorker. The New Yorker’s first feature article on long covid, published last month, neglected to interview any clinicians who treat long covid patients nor to cite the abundant research on how covid causes damage to many organ systems, yet interviewed several doctors in unrelated fields who claim long covid is psychogenic. In response to a patient’s assertion that covid impacts the brain, the author spent an entire paragraph detailing how there is currently no evidence that covid crosses the blood-brain barrier, but didn’t mention the research on covid patients finding cognitive dysfunction and deficits, PET scans similar to those seen in Alzheimer’s patients, neurological damage, and shrinking grey matter. This leaves a general audience with the mistaken impression that it is unproven whether covid impacts the brain, and is a familiar tactic from bad-faith science debates.\nThe New Yorker article set up a strict dichotomy between long covid patients and doctors, suggesting that patients harbor a “disregard for expertise”; are less “concerned about what is and isn’t supported by evidence”; and are overly “impatient.” In contrast, doctors appreciate the “careful study design, methodical data analysis, and the skeptical interpretation of results” that medicine requires. Of course, this is a false dichotomy: many patients are more knowledgeable about the latest research than their doctors, some patients are publishing in peer-reviewed journals, and there are many medical doctors that are also patients. And on the other hand, doctors are just as prone as the rest of us to biases, blind spots, and institutional errors.\n\n\n\nAP Photo/J. Scott Applewhit\n\n\nIn 1987, 40,000 Americans had already died of AIDS, yet the government and pharmaceutical companies were doing little to address this health crisis. AIDS was heavily stigmatized, federal spending was minimal, and pharmaceutical companies lacked urgency. The activists of ACT UP used a two pronged approach: creative and confrontational acts of protest, and informed scientific proposals. When the FDA refused to even discuss giving AIDS patients access to experimental drugs, ACT UP protested at their headquarters, blocking entrances and lying down in front of the building with tombstones saying “Killed by the FDA”. This opened up discussions, and ACT UP offered viable scientific proposals, such as switching from the current approach of conducting drug trials on a small group of people over a long time, and instead testing a large group of people over a short time, radically speeding up the pace at which progress occurred. ACT UP used similar tactics to protest the NIH and pharmaceutical companies, demanding research on how to treat the opportunistic infections that killed AIDS patients, not solely research for a cure. The huge progress that has happened in HIV/AIDS research and treatment would not have happened without the efforts of ACT UP.\nAcross the world, we are at a pivotal time in determining how societies and governments will deal with the masses of newly disabled people due to long covid. Narratives that take hold early often have disproportionate staying power. Will we inaccurately label long covid as psychogenic, primarily invest in psychiatric research that can’t address the well-documented physiological damage caused by covid, and financially abandon the patients who are now unable to work? Or will we take the chance to transform medicine to better recognize the lived experiences and knowledge of patients, to center patient partnerships in biomedical research for complex and multi-system diseases, and strengthen inadequate disability support and services to improve life for all people with disabilities? The decisions we collectively make now on these questions will have reverberations for decades to come."
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html",
    "href": "posts/2022-03-15-math-person/index.html",
    "title": "There’s no such thing as not a math person",
    "section": "",
    "text": "On the surface, I may seem into math: I have a math PhD, taught a graduate computational linear algebra course, co-founded AI research lab fast.ai, and even go by the twitter handle @math_rachel.\nYet many of my experiences of academic math culture have been toxic, sexist, and deeply alienating. At my lowest points, I felt like there was no place for me in math academia or math-heavy tech culture.\nIt is not just mathematicians or math majors who are impacted by this: Western culture is awash in negative feelings and experiences regarding math, which permate from many sources and impact students of all ages. In this post, I will explore the cultural factors, misconceptions, stereotypes, and relevant studies on obstacles that turn people off to math. If you (or your child) doesn’t like math or feels anxious about your own capabilities, you’re not alone, and this isn’t just a personal challenge. The below essay is based on part of a talk I recently gave."
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#myth-of-innate-ability-myth-of-the-lone-genius",
    "href": "posts/2022-03-15-math-person/index.html#myth-of-innate-ability-myth-of-the-lone-genius",
    "title": "There’s no such thing as not a math person",
    "section": "Myth of Innate Ability, Myth of the Lone Genius",
    "text": "Myth of Innate Ability, Myth of the Lone Genius\nOne common myth is the idea that certain people’s brains aren’t “wired” the right way to do math, tech, or AI, that your brain either “works that way” or not. None of the evidence supports this viewpoint, yet when people believe this, it can become a self-fulfilling prophecy. Dr. Omoju Miller, who earned her PhD at UC Berkeley and was a senior machine learning engineer and technical advisor to the CEO at Github, shares some of the research debunking the myth of innate ability in this essay and in her TEDx talk. In reality, there is no such thing as “not a math person.”\nDr. Cathy O’Neil, a Harvard Math PhD and author of Weapons of Math Destruction, wrote about the myth of the lone genius mathematician, “You don’t have to be a genius to become a mathematician. If you find this statement at all surprising, you’re an example of what’s wrong with the way our society identifies, encourages and rewards talent… For each certified genius, there are at least a hundred great people who helped achieve such outstanding results.”\n\n\n\nDr. Miller debunking the myth of innate ability, and Dr. O’Neil debunking the myth of the lone genius mathematician"
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#music-without-singing-or-instruments",
    "href": "posts/2022-03-15-math-person/index.html#music-without-singing-or-instruments",
    "title": "There’s no such thing as not a math person",
    "section": "Music without singing or instruments",
    "text": "Music without singing or instruments\nImagine a world where children are not allowed to sing songs or play instruments until they reach adulthood, after spending a decade or two transcribing sheet music by hand. This scenario is absurd and nightmarish, yet it is analogous to how math is often taught, with the most creative and interesting parts saved until almost everyone has dropped out. Dr. Paul Lockhart eloquently describes this metaphor in his essay, A Mathematician’s Lament, on “how school cheats us out of our most fascinating and imaginative art form.” Dr. Lockhart left his role as a university math professor to teach K-12 math, as he felt that so much reform was needed in how math is taught.\nDr. David Perkins uses the analogy of how children can play baseball wthout knowing all the technical details, without having a full team or playing a full 9 innings, yet still gain a sense of the “whole game.” Math is usually taught with an overemphasis on dry, technical details, without giving students a concept of the “whole game.” It can take years and years before enough technical details are accumulated to build something interesting. There is an overemphasis on techniques rather than meaning.\n\n\n\nWhat if math was taught more like how music or sports are taught?\n\n\nMath curriculums are usually arranged in a vertical manner, with each year building tightly on the previous, such that one bad year can ruin everything that comes after. Many people I talk to can pinpoint the year that math went bad for them: “I used to like math until 6th grade, when I had a bad teacher/was dealing with peer pressure/my undiagnosed ADHD was out of control. After that, I was never able to succeed in future years.” This is less true in other subjects, where one bad history teacher/one bad year doesn’t mean that you can’t succeed at history the following year."
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#gender-race-and-stereotypes",
    "href": "posts/2022-03-15-math-person/index.html#gender-race-and-stereotypes",
    "title": "There’s no such thing as not a math person",
    "section": "Gender, race, and stereotypes",
    "text": "Gender, race, and stereotypes\nFemale teachers’ math anxiety affects girls’ math achievement: In the USA, over 90% of primary school teachers are female, and research has found “the more anxious teachers were about math, the more likely girls (but not boys) were to endorse the commonly held stereotype that ‘boys are good at math, and girls are good at reading’ and the lower these girls’ math achievement… People’s fear and anxiety about doing math—over and above actual math ability—can be an impediment to their math achievement.”\nResearch across a number of universities has found that more women go into engineering when courses focus on problems with positive social impact.\nStructural racism also impacts what messages teachers impart to students. An Atlantic article How Does Race Affect a Student’s Math Education? covered the research paper A Framework for Understanding Whiteness in Mathematics Education, noting that “Constantly reading and hearing about underperforming Black, Latino, and Indigenous students begins to embed itself into how math teachers view these students, attributing achievement differences to their innate ability to succeed in math… teachers start to expect worse performance from certain students, start to teach lower content, and start to use lower-level math instructional practices. By contrast, white and Asian students are given the benefit of the doubt and automatically afforded the opportunity to do more sophisticated and substantive mathematics.”"
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#the-mathematics-community-is-an-absolute-mess-which-actively-pushes-out-the-sort-of-people-who-might-make-it-better",
    "href": "posts/2022-03-15-math-person/index.html#the-mathematics-community-is-an-absolute-mess-which-actively-pushes-out-the-sort-of-people-who-might-make-it-better",
    "title": "There’s no such thing as not a math person",
    "section": "The mathematics community is “an absolute mess which actively pushes out the sort of people who might make it better”",
    "text": "The mathematics community is “an absolute mess which actively pushes out the sort of people who might make it better”\n\n\n\nDr. Harron’s website, and some of the coverage of her number theory thesis, including on the Scientific American blog\n\n\nDr. Piper Harron made waves with her Princeton PhD thesis, utilizing humor, analogies, sarcasm, and genuine efforts to be accessible as she described advanced concepts in a ground-breaking way, very atypical for a mathematics PhD thesis. Dr. Harron wrote openly in the prologue of her thesis on how alienating the culture of mathematics is, “As any good grad student would do, I tried to fit in, mathematically. I absorbed the atmosphere and took attitudes to heart. I was miserable, and on the verge of failure. The problem was not individuals, but a system of self-preservation that, from the outside, feels like a long string of betrayals, some big, some small, perpetrated by your only support system.” At her blog, the Liberated Mathematician, she writes, “My view of mathematics is that it is an absolute mess which actively pushes out the sort of people who might make it better.”\nThese descriptions resonate with my own experiences obtaining a math PhD (as well as the experiences of many friends, at a variety of universities). The toxicity of academic math departments is self-perpetuating, pushing out the people who could make them better."
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#the-full-talk",
    "href": "posts/2022-03-15-math-person/index.html#the-full-talk",
    "title": "There’s no such thing as not a math person",
    "section": "The full talk",
    "text": "The full talk\nThis post is based on the first part of the talk I gave in the below video, which includes more detail and a Q&A. The talk also includes recommendations about math apps and resources, as well as a framework for how to consider screentime. Stay tuned for a future fast.ai blog post covering math apps and screentime."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html",
    "href": "posts/2022-05-17-societal-harms/index.html",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "",
    "text": "When the USA government switched to facial identification service ID.me for unemployment benefits, the software failed to recognize Bill Baine’s face. While the app said that he could have a virtual appointment to be verified instead, he was unable to get through. The screen had a wait time of 2 hours and 47 minutes that never updated, even over the course of weeks. He tried calling various offices, his daughter drove in from out of town to spend a day helping him, and yet he was never able to get a useful human answer on what he was supposed to do, as he went for months without unemployment benefits. In Baine’s case, it was eventually resolved when a journalist hypothesized that the issue was a spotty internet connection, and that Baine would be better off traveling to another town to use a public library computer and internet. Even then, it still took hours for Baine to get his approval.\n\n\n\nJournalist Andrew Kenney of Colorado Public Radio has covered the issues with ID.me\n\n\nBaine was not alone. The number of people receiving unemployment benefits plummeted by 40% in the 3 weeks after ID.me was introduced. Some of these were presumed to be fraudsters, but it is unclear how many genuine people in need of benefits were wrongly harmed by this. These are individual harms, but there are broader, societal harms as well: the cumulative costs of the public having to spend ever more time on hold, trying to navigate user-hostile automated bureaucracies where they can’t get the answers they need. There is the societal cost of greater inequality and greater desperation, as more people are plunged into poverty through erroneous denial of benefits. And there is the undermining of trust in public services, which can be difficult to restore.\nPotential for algorithmic harm takes many forms: loss of opportunity (employment or housing discrimination), economic cost (credit discrimination, narrowed choices), social detriment (stereotype confirmation, dignitary harms), and loss of liberty (increased surveillance, disproportionate incarceration). And each of these four categories manifests in both individual and societal harms.\nIt should come as no surprise that algorithmic systems can give rise to societal harm. These systems are sociotechnical: they are designed by humans and teams that bring their values to the design process, and algorithmic systems continually draw information from, and inevitably bear the marks of, fundamentally unequal, unjust societies. In the context of COVID-19, for example, policy experts warned that historical healthcare inequities risked making their way into the datasets and models being used to predict and respond to the pandemic. And while it’s intuitively appealing to think of large-scale systems as creating the greatest risk of societal harm, algorithmic systems can create societal harm because of the dynamics set off by their interconnection with other systems/ players, like advertisers, or commercially-driven media, and the ways in which they touch on sectors or spaces of public importance.\nStill, in the west, our ideas of harm are often anchored to an individual being harmed by a particular action at a discrete moment in time. As law scholar Natalie Smuha has powerfully argued, legislation (both proposed and passed) in Western countries to address algorithmic risks and harms often focuses on individual rights: regarding how an individual’s data is collected or stored, to not be discriminated against, or to know when AI is being used. Even metrics used to evaluate the fairness of algorithms are often aggregating across individual impacts, but unable to capture longer-term, more complex, or second- and third-order societal impacts."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#case-study-privacy-and-surveillance",
    "href": "posts/2022-05-17-societal-harms/index.html#case-study-privacy-and-surveillance",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Case Study: Privacy and surveillance",
    "text": "Case Study: Privacy and surveillance\nConsider the over-reliance on individual harms in discussing privacy: so often focused on whether individual users have the ability to opt in or out of sharing their data, notions of individual consent, or proposals that individuals be paid for their personal data. Yet widespread surveillance fundamentally changes society: people may begin to self-censor and to be less willing (or able) to advocate for justice or social change. Professor Alvaro Bedoya, director of the Center on Privacy and Technology at the Georgetown University Law Center, traces a history of how surveillance has been used by the state to try to shut down movements for progress– targeting religious minorities, poor people, people of color, immigrants, sex workers and those considered “other”. As Maciej Ceglowski writes, “Ambient privacy is not a property of people, or of their data, but of the world around us… Because our laws frame privacy as an individual right, we don’t have a mechanism for deciding whether we want to live in a surveillance society.”\nDrawing on interviews with African data experts, Birhane et al write that even when data is anonymized and aggregated, it “can reveal information on the community as a whole. While notions of privacy often focus on the individual, there is growing awareness that collective identity is also important within many African communities, and that sharing aggregate information about communities can also be regarded as a privacy violation.” Recent US-based scholarship has also highlighted the importance of thinking about group level privacy (whether that group is made up of individuals who identify as members of that group, or whether it’s a ‘group’ that is algorithmically determined - like individuals with similar shopping habits on Amazon). Because even aggregated anonymised data can reveal important group-level information (e.g., the location of military personnel training via exercise tracking apps) “managing privacy”, these authors argue “is often not intrapersonal but interpersonal.” And yet legal and tech design privacy solutions are often better geared towards assuring individual-level privacy than negotiating group privacy."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#case-study-disinformation-and-erosion-of-trust",
    "href": "posts/2022-05-17-societal-harms/index.html#case-study-disinformation-and-erosion-of-trust",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Case Study: Disinformation and erosion of trust",
    "text": "Case Study: Disinformation and erosion of trust\nAnother example of a collective societal harm comes from how technology platforms such as Facebook have played a significant role in elections ranging from the Philippines to Brazil, yet it can be difficult (and not necessarily possible or useful) to quantify exactly how much: something as complex as a country’s political system and participation involves many interlinking factors. But when ‘deep fakes’ make it “possible to create audio and video of real people saying and doing things they never said or did” or when motivated actors successfully game search engines to amplify disinformation, the (potential) harm that is generated is societal, not just individual. Disinformation and the undermining of trust in institutions and fellow citizens have broad impacts, including on individuals who never use social media.\n\n\n\nReports and Events on Regulatory Approaches to Disinformation\n\n\nEfforts by national governments to deal with the problem through regulation have not gone down well with everyone. ‘Disinformation’ has repeatedly been highlighted as one of the tech-enabled ‘societal harms’ that the UK’s Online Safety Bill or the EU’s Digital Services Act should address, and a range of governments are taking aim at the problem by proposing or passing a slew of (in certain cases ill-advised) ‘anti-misinformation’ laws. But there’s widespread unease around handing power to governments to set standards for what counts as ‘disinformation’. Does reifying ‘disinformation’ as a societal harm become a legitimizing tool for governments looking to silence political dissent or undermine their weaker opponents? It’s a fair and important concern - and yet simply leaving that power in the hands of mostly US-based, unaccountable tech companies is hardly a solution. What are the legitimacy implications if a US company like Twitter were to ban democratically elected Brazilian President Jair Bolsonaro for spreading disinformation, for example? How do we ensure that tech companies are investing sufficiently in governance efforts across the globe, rather than responding in an ad hoc manner to proximal (i.e. mostly US-based) concerns about disinformation? Taking a hands off approach to platform regulation doesn’t make platforms’ efforts to deal with disinformation any less politically fraught."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#individual-harms-individual-solutions",
    "href": "posts/2022-05-17-societal-harms/index.html#individual-harms-individual-solutions",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Individual Harms, Individual Solutions",
    "text": "Individual Harms, Individual Solutions\nIf we consider individual solutions our only option (in terms of policy, law, or behavior), we often limit the scope of the harms we can recognize or the nature of the problems we face. To take an example not related to AI: Oxford professor Trish Greenhalgh et al analyzed the slow reluctance of leaders in the West to accept that covid is airborne (e.g. it can linger and float in the air, similar to cigarette smoke, requiring masks and ventilation to address), rather than droplet dogma (e.g. washing your hands is a key precaution). One reason they highlight is the Western framing of individual responsibility as the solution to most problems. Hand-washing is a solution that fits the idea of individual responsibility, whereas collective responsibility for the quality of shared indoor air does not. The allowable set of solutions helps shape what we identify as a problem. Additionally, the fact that recent research suggests that “the level of interpersonal trust in a society” was a strong predictor of which countries managed COVID-19 most successfully should give us pause. Individualistic framings can limit our imagination about the problems we face and which solutions are likely to be most impactful."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#parallels-with-environmental-harms",
    "href": "posts/2022-05-17-societal-harms/index.html#parallels-with-environmental-harms",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Parallels with Environmental Harms",
    "text": "Parallels with Environmental Harms\nBefore the passage of environmental laws, many existing legal frameworks were not well-suited to address environmental harms. Perhaps a chemical plant releases waste emissions into the air once per week. Many people in surrounding areas may not be aware that they are breathing polluted air, or may not be able to directly link air pollution to a new medical condition, such as asthma, (which could be related to a variety of environmental and genetic factors).\n\n\n\nThere are parallels between air polllution and algorithmic harms\n\n\nThere are many parallels between environmental issues and AI ethics. Environmental harms include individual harms for people who develop discrete health issues from drinking contaminated water or breathing polluted air. Yet, environmental harms are also societal: the societal costs of contaminated water and polluted air can reverberate in subtle, surprising, and far-reaching ways. As law professor Nathalie Smuha writes, environmental harms are often accumulative and build over time. Perhaps each individual release of waste chemicals from a refinery has little impact on its own, but adds up to be significant. In the EU, environmental law allows for mechanisms to show societal harm, as it would be difficult to challenge many environmental harms on the basis of individual rights. Smuha argues that there are many similarities with AI ethics: for opaque AI systems, spanning over time, it can be difficult to prove a direct causal relationship to societal harm."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#directions-forward",
    "href": "posts/2022-05-17-societal-harms/index.html#directions-forward",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Directions Forward",
    "text": "Directions Forward\nTo a large extent our message is to tech companies and policymakers. It’s not enough to focus on the potential individual harms generated by tech and AI: the broader societal costs of tech and AI matter.\nBut those of us outside tech policy circles have a crucial role to play. One way in which we can guard against the risks of the ‘societal harm’ discourse being co-opted by those with political power to legitimise undue interference and further entrench their power is by claiming the language of ‘societal harm’ as the democratic and democratising tool it can be. We all lose when we pretend societal harms don’t exist, or when we acknowledge they exist but throw our hands up. And those with the least power, like Bill Baine, are likely to suffer a disproportionate loss.\nIn his newsletter on Tech and Society, L.M. Sacasas encourages people to ask themselves 41 questions before using a particular technology. They’re all worth reading and thinking about - but we’re listing a few especially relevant ones to get you started. Next time you sit down to log onto social media, order food online, swipe right on a dating app or consider buying a VR headset, ask yourself:\n\nHow does this technology empower me? At whose expense? (Q16)\nWhat feelings does the use of this technology generate in me toward others? (Q17)\nWhat limits does my use of this technology impose upon others? (Q28)\nWhat would the world be like if everyone used this technology exactly as I use it? (Q37)\nDoes my use of this technology make it easier to live as if I had no responsibilities toward my neighbor? (Q40)\nCan I be held responsible for the actions which this technology empowers? Would I feel better if I couldn’t? (Q41)\n\nIt’s on all of us to sensitise ourselves to the societal implications of the tech we use."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html",
    "href": "posts/2022-06-01-qualitative/index.html",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "",
    "text": "“All research is qualitative; some is also quantitative” Harvard Social Scientist and Statistician Gary King\nSuppose you wanted to find out whether a machine learning system being adopted - to recruit candidates, lend money, or predict future criminality - exhibited racial bias. You might calculate model performance across groups with different races. But how was race categorised– through a census record, a police officer’s guess, or by an annotator? Each possible answer raises another set of questions. Following the thread of any seemingly quantitative issue around AI ethics quickly leads to a host of qualitative questions. Throughout AI, qualitative decisions are made about what metrics to optimise for, which categories to use, how to define their bounds, who applies the labels. Similarly, qualitative research is necessary to understand AI systems operating in society: evaluating system performance beyond what can be captured in short term metrics, understanding what is missed by large-scale studies (which can elide details and overlook outliers), and shedding light on the circumstances in which data is produced (often by crowd-sourced or poorly paid workers).\nUnfortunately, there is often a large divide between computer scientists and social scientists, with over-simplified assumptions and fundamental misunderstandings of one another. Even when cross-disciplinary partnerships occur, they often fall into “normal disciplinary divisions of labour: social scientists observe, data scientists make; social scientists do ethics, data scientists do science; social scientists do the incalculable, data scientists do the calculable.” The solution is not for computer scientists to absorb a shallow understanding of the social sciences, but for deeper collaborations. In a paper on exclusionary practices in AI ethics, an interdisciplinary team wrote of the “indifference, devaluation, and lack of mutual support between CS and humanistic social science (HSS), [which elevates] the myth of technologists as ‘ethical unicorns’ that can do it all, though their disciplinary tools are ultimately limited.”\nThis is further reflected in an increasing number of job ads for AI ethicists that list a computer science degree as a requirement, “prioritising technical computer science infrastructure over the social science skills that can evaluate AI’s social impact. In doing so, we are building the field of AI Ethics to replicate the very flaws this field is trying to fix.” Interviews with 26 responsible AI practitioners working in industry highlighted a number of challenges, including that qualitative work was not prioritised. Not only is it impossible to fully understand ethics issues solely through quantitative metrics, inappropriate and misleading quantitative metrics are used to evaluate the responsible AI practitioners themselves. Interviewees reported that their fairness work was evaluated on metrics related to generating revenue, in a stark misalignment of goals."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#qualitative-research-helps-us-evaluate-ai-systems-beyond-short-term-metrics",
    "href": "posts/2022-06-01-qualitative/index.html#qualitative-research-helps-us-evaluate-ai-systems-beyond-short-term-metrics",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "Qualitative research helps us evaluate AI systems beyond short term metrics",
    "text": "Qualitative research helps us evaluate AI systems beyond short term metrics\nWhen companies like Google and YouTube want to test whether the recommendations they are making (in the form of search engine results or YouTube videos, for example) are “good” - they will often focus quite heavily on “engagement” or “dwell time” - the time a user spent looking at or watching the item recommended to them. But it turns out, unsurprisingly, that a focus on engagement and dwell time, narrowly understood, raises all sorts of problems. Demographics can impact dwell time (e.g. older users may spend longer on websites than younger users, just as part of the way they use the internet). A system that ‘learns’ from a user’s behavioural cues (rather than their ‘stated preferences’) might lock them into a limiting feedback loop, appealing to that user’s short term interests rather than those of their ‘Better Selves.’ Scholars have called for more qualitative research to understand user experience and build this into the development of metrics.\nThis is the part where people will point out, rightly, that companies like Google and YouTube rely on a complex range of metrics and signals in their machine learning systems - and that where a website ranks on Google, or how a YouTube video performs in recommendation does not boil down to simple popularity metrics, like engagement. Google employs an extensive process to determine “relevance” and “usefulness” for search results. In its 172-page manual for search result ‘Quality’ evaluation, for example, the company explains how evaluators should assess a website’s ‘Expertise/ Authoritativeness/ Trustworthiness’ or ‘E-A-T’; and what types of content, by virtue of its harmful nature (e.g., to protected groups), should be given a ‘low’ ranking. YouTube has identified specific categories of content (such as news, scientific subjects, and historical information) for which ‘authoritativeness’ should be considered especially important. It has also determined that dubious-but-not-quite-rule-breaking information (what it calls ‘borderline content’) should not be recommended, regardless of the video’s engagement levels.\nIrrespective of how successful we consider the existing approaches of Google Search and YouTube to be (and partly, the issue is that evaluating their implementation from the outside is frustratingly difficult), the point here is that there are constant qualitative judgments being made, about what makes a search result or recommendation “good” and of how to define and quantify expertise, authoritativeness, trustworthiness, borderline content, and other values. This is true of all machine learning evaluation, even when it isn’t explicit. In a paper guiding companies about how to carry out internal audits of their AI systems, Inioluwa Deborah Raji and colleagues emphasise the importance of interviews with management and engineering teams to “capture and pay attention to what falls outside the measurements and metrics, and to render explicit the assumptions and values the metrics apprehend.” (p.40).\nThe importance of thoughtful humanities research is heightened if we are serious about grappling with the potential broader social effects of machine learning systems (both good and bad), which are often delayed, distributed and cumulative."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#small-scale-qualitative-studies-tell-an-important-story",
    "href": "posts/2022-06-01-qualitative/index.html#small-scale-qualitative-studies-tell-an-important-story",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "Small-scale qualitative studies tell an important story",
    "text": "Small-scale qualitative studies tell an important story\nHypothetically, let’s say you wanted to find out whether the use of AI technologies by doctors during a medical appointment would make doctors less attentive to patients - what do you think the best way of doing it would be? You could find some criteria and method for measuring ‘attentiveness’, say tracking the amount of eye contact between the doctor and patient, and analyse this across a representative sample of medical appointments where AI technologies were being used, compared to a control group of medical appointments where AI technologies weren’t being used. Or would you interview doctors about their experiences using the technology during appointments? Or talk to patients about how they felt the technology did, or didn’t, impact their experience?\nIn research circles, we describe these as ‘epistemological’ choices - your judgement of what constitutes the ‘best’ approach is inextricably linked to your judgement about how we can claim to ‘know’ something. These are all valid methods for approaching the question, but you can imagine how they might result in different, even conflicting, insights. For example, you might end up with the following results: - The eye contact tracking experiment suggests that overall, there is no significant difference in doctors’ attentiveness to the patient when the AI tech is introduced. - The interviews with doctors and patients reveal that some doctors and patients feel that the AI technology reduces doctors’ attentiveness to patients, and others feel that it makes no difference or even increases doctors’ attention to the patient.\nEven if people are not negatively impacted by something ‘on average’ (e.g., in our hypothetical eye contact tracking experiment above), there will remain groups of people who will experience negative impacts, perhaps acutely so. “Many of people’s most pressing questions are about effects that vary for different people,” write Matias, Pennington and Chan in a recent paper on the idea of N-of-one trials. To tell people that their experiences aren’t real or valid because they don’t meet some threshold for statistical significance across a large population doesn’t help us account for the breadth and nature of AI’s impacts on the world.\nExamples of this tension between competing claims to knowledge about AI systems’ impacts abound. Influencers who believe they are being systematically downranked (‘shadowbanned’) by Instagram’s algorithmic systems are told by Instagram that this simply isn’t true. Given the inscrutability of these proprietary algorithmic systems, it is impossible for influencers to convincingly dispute Instagram’s claims. Kelley Cotter refers to this as a form of “black box gaslighting”: platforms can “leverage perceptions of their epistemic authority on their algorithms to undermine users’ confidence in what they know about algorithms and destabilise credible criticism.” Her interviews with influencers give voice to stakeholder concerns and perspectives that are elided in Instagram’s official narrative about its systems. The mismatch between different stakeholders’ accounts of ‘reality’ is instructive. For example, a widely-cited paper by Netflix employees claims that Netflix recommendation “influences choice for about 80% of hours streamed at Netflix.” But this claim stands in stark contrast to Mattias Frey’s mixed-methods research (representative survey plus small sample for interviews) run with UK and US adults, in which less than 1 in 5 adults said they primarily relied on Netflix recommendations when deciding what films to watch. Even if this is because users underestimate their reliance on recommender systems, that’s a critically important finding - particularly when we’re trying to regulate recommendation and so many are advocating providing better user-level controls as a check on platform power. Are people really going to go to the trouble of changing their settings if they don’t think they rely on algorithmic suggestions that much anyway?"
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#qualitative-research-sheds-light-on-the-context-of-data-annotation",
    "href": "posts/2022-06-01-qualitative/index.html#qualitative-research-sheds-light-on-the-context-of-data-annotation",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "Qualitative research sheds light on the context of data annotation",
    "text": "Qualitative research sheds light on the context of data annotation\nMachine learning systems rely on vast amounts of data. In many cases, for that data to be useful, it needs to be labelled/ annotated. For example, a hate speech classifier (an AI-enabled tool used to identify and flag potential cases of hate speech on a website) relies on huge datasets of text labelled as ‘hate speech’ or ‘not hate speech’ to ‘learn’ how to spot hate speech. But it turns out that who is doing the annotating and in what context they’re doing it, matters. AI-powered content moderation is often held up as the solution to harmful content online. What has continued to be underplayed is the extent to which those automated systems are and most likely will remain dependent on the manual work of human content moderators sifting through some of the worst and most traumatic online material to power the machine learning datasets on which automated content moderation depends. Emily Denton and her colleagues highlight the significance of annotators’ social identity (e.g., race, gender) and their expertise when it comes to annotation tasks, and they point out the risks associated with overlooking these factors and simply ‘aggregating’ results as ‘ground truth’ rather than properly exploring disagreements between annotators and the important insights that this kind of disagreement might offer.\nHuman commercial content moderators (such as the people that identify and remove violent and traumatic imagery on Facebook) often labour in terrible conditions, lacking psychological support or appropriate financial compensation. The interview-based research of Sarah T. Roberts has been pioneering in highlighting these conditions. Most demand for crowdsourced digital labour comes from the Global North, yet the majority of these workers are based in the Global South and receive low wages. Semi-structured interviews reveal the extent to which workers feel unable to bargain effectively for better pay in the current regulatory environment. As Mark Graham and his colleagues point out, these findings are hugely important in a context where several governments and supranational development organisations like the World Bank are holding up digital work as a promising tool to fight poverty.\nThe decision of how to measure ‘race’ in machine learning systems is highly consequential, especially in the context of existing efforts to evaluate these systems for their “fairness.” Alex Hanna, Emily Denton, Andrew Smart and Jamila Smith-Loud have done crucial work highlighting the limitation of machine learning systems that rely on official records of race or their proxies (e.g. census records), noting that the racial categories provided by such records are “unstable, contingent, and rooted in racial inequality.” The authors emphasise the importance of conducting research in ways that prioritise the perspectives of the marginalised racial communities that fairness metrics are supposed to protect. Qualitative research is ideally placed to contribute to a consideration of “race” in machine learning systems that is grounded in the lived experiences and needs of the racially subjugated."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#what-next",
    "href": "posts/2022-06-01-qualitative/index.html#what-next",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "What next?",
    "text": "What next?\nCollaborations between quantitative and qualitative researchers are valuable in understanding AI ethics from all angles.\nConsider reading more broadly, outside your particular area. Perhaps using the links and researchers listed here as starting points. They’re just a sliver of the wealth that’s out there. You could also check out the Social Media Collective’s Critical Algorithm Studies reading list, the reading list provided by the LSE Digital Ethnography Collective, and Catherine Yeo’s suggestions.\nStrike up conversations with researchers in other fields, and consider the possibility of collaborations. Find a researcher slightly outside your field but whose work you broadly understand and like, and follow them on Twitter. With any luck, they will share more of their work and help you identify other researchers to follow. Collaboration can be an incremental process: Consider inviting the researcher to form part of a discussion panel, reach out to say what you liked and appreciated about their work and why, and share your own work with them if you think it’s aligned with their interests.\nWithin your university or company, is there anything you could do to better reward or facilitate interdisciplinary work? As Humanities Computing Professor Willard McCarty notes, somewhat discouragingly, “professional reward for genuinely interdisciplinary research is rare.” To be sure, individual researchers and practitioners have to be prepared to put themselves out there, compromise and challenge themselves - but carefully tailored institutional incentives and enablers matter."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html",
    "href": "posts/2022-09-06-homeschooling/index.html",
    "title": "My family’s unlikely homeschooling journey",
    "section": "",
    "text": "My husband Jeremy and I never intended to homeschool, and yet we have now, unexpectedly, committed to homeschooling long-term. Prior to the pandemic, we both worked full-time in careers that we loved and found meaningful, and we sent our daughter to a full-day Montessori school. Although I struggled with significant health issues, I felt unbelievably lucky and fulfilled in both my family life and my professional life. The pandemic upended my careful balance. Every family is different, with different needs, circumstances, and constraints, and what works for one may not work for others. My intention here is primarily to share the journey of my own (very privileged) family."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#our-unplanned-introduction-to-homeschooling",
    "href": "posts/2022-09-06-homeschooling/index.html#our-unplanned-introduction-to-homeschooling",
    "title": "My family’s unlikely homeschooling journey",
    "section": "Our unplanned introduction to homeschooling",
    "text": "Our unplanned introduction to homeschooling\nFor the first year of the pandemic, most schools in California, where we lived at the time, were closed. Like countless other families, we were unexpectedly thrust into the world of virtual-school and home-school. We ended up participating in an innovative online program that did NOT try to replicate in-person school. A few key differences: - Each child could work at their own pace, largely through playing educational games and apps that adapted to where they were. There was no particular endpoint that the kids needed to get to at the end of the semester. - Group video calls were limited in size to no more than 6 kids (and often smaller), so kids got lots of personal interaction with their tutors and each other. Even as an adult, I find video calls larger than 6 people overwhelming. - Regular movement breaks, where the kids had jumping jack competitions, did Cosmic Kids yoga videos, held dance parties, and ran around the house for scavenger hunts. - Took advantage of existing materials: the program did not reinvent the wheel, but instead made use of excellent, existing online videos and educational apps.\nFrom August 2020 - March 2021, our daughter was with a small group online, where daily she would spend 1 hour on socio-emotional development (including games, getting to know each other, and discussing feelings), 1 hour on reading, and 1 hour on math. For reading and math, the children each worked at their own pace through engaging games, and could ask the teacher and each other questions whenever they needed help. At the end of these 8 months, our daughter, along with several other kids in her small group, were several years beyond their age levels in both math and reading. It had never been our goal for her to end up accelerated; Jeremy and I were mostly trying to keep her happy and busy for a few hours so we could get some of our own work done. She also had fun and made close friends, who she continues to have video calls and Minecraft virtual playdates with regularly."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#our-unconventional-views",
    "href": "posts/2022-09-06-homeschooling/index.html#our-unconventional-views",
    "title": "My family’s unlikely homeschooling journey",
    "section": "Our unconventional views",
    "text": "Our unconventional views\nAlthough there are plenty of ways to homeschool that don’t involve any screens or technology, Jeremy and I have made use of online tutors, long-distance friendships, educational apps, videos, and web-based games, as key parts of our approach. One thing that helped us going into the pandemic is that we have never treated online/long-distance relationships as inferior to in-person relationships. We both have meaningful friendships that occur primarily, or even entirely, through phone calls, video chats, texts, and online interactions. I have made several big moves since I graduated from high school (moving from Texas to Pennsylvania to North Carolina back to Pennsylvania again and then to California) and I was used to family and close friends being long distance. We live far from our families, and our daughter was already accustomed to chatting with her grandparents on both sides via video calls. My daughter’s best friend is now a child she has never met in person, but has been skyping with almost daily for the last 2 years.\nAnother thing that made this transition easier is that Jeremy and I have never been anti-screen time. In fact, we don’t consider “screen time” a useful category, since a child passively watching a movie alone is different than skyping with their grandparent is different than playing an educational game interactively with their parent beside them. While we almost never let our daughter do things passively and alone with screens, we enjoy relational and educational screen time. Furthermore, we focus on including other positive life elements (e.g. making sure she is getting enough time outside, being physically active, reading, getting enough sleep, etc) rather than emphasising limits.\n{% include image w=“600” url=“mathperson/venndiagram.jpg” caption=“A Venn Diagram showing how I think about screentime. We avoid the outside (white) region and mostly stick to the intersections.” %}"
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#a-return-to-in-person-school",
    "href": "posts/2022-09-06-homeschooling/index.html#a-return-to-in-person-school",
    "title": "My family’s unlikely homeschooling journey",
    "section": "A return to in-person school",
    "text": "A return to in-person school\nIn 2021, our family immigrated from the USA to my husband’s home country of Australia, and we enrolled our daughter at an in-person school, which she attended from late April - early Dec 2021. Our state had closed borders and almost no cases of covid transmission during this time. By all measures, the school she attended is great: friendly families, kind staff, and a fun performing arts program. While our daughter adjusted quickly to the new environment and made friends, she was quite bored. She lost her previous curiosity and enthusiasm, became more passive, and started to spend a lot of time zoning out. The school tried to accommodate her, letting her join an older grade level for math each day. While the material was initially new, she still found the pace too slow. She started to get very upset at home practising piano or playing chess (activities she previously loved, but where mistakes are inevitable), because she had grown accustomed to getting everything right without trying. At one point, all schools in our region closed during an 8-day snap lockdown. Our daughter was disappointed when the lockdown ended and she had to return to school."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#when-homeschooling-works-well-and-when-it-doesnt",
    "href": "posts/2022-09-06-homeschooling/index.html#when-homeschooling-works-well-and-when-it-doesnt",
    "title": "My family’s unlikely homeschooling journey",
    "section": "When homeschooling works well (and when it doesn’t)",
    "text": "When homeschooling works well (and when it doesn’t)\nOver the summer holidays (Dec 2021-Jan 2022), our state pivoted from zero covid to promoting mass infection as “necessary”. We pulled our daughter out of school, initially intending that it would just be a temporary measure until her age group could be fully vaccinated (vaccine rollout was later in Australia than in the USA). However, we immediately saw positive changes, with her regaining her old curiosity, enthusiasm, and proactive nature, all of which she had lost being in school. Her perfectionism disappeared and she began to enjoy challenges again. We supplemented her online classes with in-person playdates, extracurriculars, and sports (due to covid risks, we wear masks and stay outdoors for all of these). We are fortunate to live in a beautiful part of the world, where we can spend most of the year outside. We enjoy visiting the beaches, forests, and parks in our region. Our daughter is happy: playing Minecraft with friends online, learning tennis with other local children, riding bikes as a family, spending hours absorbed in books of her own choosing, enjoying piano and chess again, running around in nature, and learning at her own pace.\nHomeschooled kids typically score 15 to 30 percentile points above public-school students on standardised academic achievement tests, and 87% of studies on social development “showed clearly positive outcomes for the homeschooled compared to those in conventional schools”. However, it is understandable that many children had negative experiences with virtual learning in the past 2 years, given that programs were often hastily thrown together with inadequate resources and inappropriately structured to try to mimic in-person school, against the stressful backdrop of a global pandemic. Many parents faced the impossible task of simultaneously needing to work full-time and help their children full-time (and many other parents did not even have the option to stay home). Every family is different, and virtual learning or homeschooling will not suit everyone. There are children who need in-person services only offered within schools; parents whose work constraints don’t allow for it; and kids who thrive being with tons of other kids.\nDespite the difficulty of the pandemic, there are a variety of families who found that virtual or homeschooling was better for their particular kids. Some parents have shared about children with ADHD who found in-person school too distracting; children who were facing bullying or violence at school; kids who couldn’t get enough sleep on a traditional school schedule; Black and Latino families whose cultural heritages were not being reflected in curriculums. I enjoyed these article featuring a few such families: - What if Some Kids Are Better Off at Home? | The New York Times For parents like me, the pandemic has come with a revelation: For our children, school was torture. - They Rage-Quit the School System—and They’re Not Going Back | WIRED The pandemic created a new, more diverse, more connected crop of homeschoolers. They could help shape what learning looks like for everyone."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#covid-risks",
    "href": "posts/2022-09-06-homeschooling/index.html#covid-risks",
    "title": "My family’s unlikely homeschooling journey",
    "section": "Covid Risks",
    "text": "Covid Risks\nI have had brain surgery twice, was hospitalised in the ICU with a life-threatening brain infection, and have a number of chronic health issues. I am both at higher risk for negative outcomes from covid AND acutely aware of how losing your health can destroy your life. It is lonely and difficult being high-risk in a society that has given up on protecting others. While I am nervous about the long-term impact that homeschooling will have on my career (on top of how my existing health issues already hinder it), acquiring additional disabilities would be far, far worse.\nI have been disturbed to follow the ever-accumulating research on cardiovascular, neurological, and immune system harms that can be caused by covid, even in previously healthy people, even in the vaccinated, and even in children. While vaccines significantly reduce risk of death, unfortunately they provide only a limited reduction in Long Covid risk. Immunity wanes, and people face cumulative risks with each new covid infection (so even if you’ve had covid once or twice, it is best to try to avoid reinfections). I am alarmed that leaders are encouraging mass, repeated infections of a generation of children.\nGiven all this, I am relieved that our decision to continue homeschooling was relatively clear. It much better suits our daughter’s needs AND drastically reduces our family’s covid risk. We can nurture her innate curiosity, protect her intrinsic motivation, and provide in-person social options that are entirely outdoors and safer than being indoors all day at school. Most families are not so fortunate and many face difficult choices, with no good options."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#the-broader-picture",
    "href": "posts/2022-09-06-homeschooling/index.html#the-broader-picture",
    "title": "My family’s unlikely homeschooling journey",
    "section": "The Broader Picture",
    "text": "The Broader Picture\nI believe that high-quality, equitable, and safe public education is important for a healthy democracy, and I worry about the various ongoing ways in which education is being undermined and attacked. Furthermore, due to a lack of covid protections in communities, high-risk children and children with high-risk families are being shut out of in-person school options in the USA, Australia, and many other places. While the workplaces of politicians and a handful of schools in ultra-wealthy areas installed expensive ventilation upgrades, the majority of schools in the USA and Australia have not had any ventilation upgrades, nor received air purifiers. All children deserve access to an education that is safe, fits their needs, and will allow them to thrive. Even when homeschooling does work, it is often still just an individual solution to systemic problems."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#related-posts",
    "href": "posts/2022-09-06-homeschooling/index.html#related-posts",
    "title": "My family’s unlikely homeschooling journey",
    "section": "Related Posts",
    "text": "Related Posts\nA few other posts that you may be interested in, related to my views on education and teaching: - There’s No Such Thing as Not A Math Person: based on a webinar I gave to parents addressing cultural myths about math and how to support your kids in their math education, even if you don’t see yourself as a “math person” - The Qualities of a Good Education: common pitfalls in traditional technical education and ideas towards doing better - What You Need to Know Before Considering a PhD: includes a reflection on how my own success in traditional academic environments was actually a weakness, because I’d learned how to solve problems I was given, but not how to how to find and scope interesting problems on my own"
  }
]