[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am cofounder of fast.ai, which created one of the most popular deep learning courses in the world. I have a PhD in mathematics and was previously the founding director of the Center for Applied Data Ethics at the University of San Francisco.\nI live in Queensland, Australia with my husband and daughter. My interests include mathematical biology, data ethics, and machine learning.\n\nMathematical Biology and AI in Medicine\n\n\nA Mathematical Model of Glutathione Metabolism, Journal of Theoretical Biology and Medical Modeling\nMedicine’s Machine Learning Problem, Boston Review\nEarned a PhD in mathematics from Duke University\nHoward Hughes Medical Institute Fellowship\nkeynote speaker at Stanford’s Artificial Intelligence in Medicine symposium\n\n\nData Ethics\n\n\nProfessor of Practice at Queensland University of Technology Centre for Data Science\nFounding director of Center for Applied Data Ethics (CADE) at University of San Francisco\nReliance on Metrics is a Fundamental Challenge for AI, Patterns. Optimizing metrics is a central aspect of most current AI approaches, yet overemphasizing metrics leads to manipulation, gaming, and a myopic short-term focus.\nCreated and taught data ethics course\nWrote ethics chapter of best-selling book,\nWrote book chapters for:\n\n97 Things About Ethics Everyone in Data Science Should Know\nDeep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD\nRedesigning AI\n\n\n\nMachine Learning and Data Science\n\n\nCo-founder of course.fast.ai\nDesigned and taught graduate level courses on Natural Language Processing and Computational Linear Algebra in the USF Masters of Data Science program\nThe New Era in NLP, Keynote at SciPy (Scientific Python) Conference 2019\nKeynote at ICML AutoML workshop, based on my popular series of AutoML posts\nBeginner friendly workshop on Word Embeddings (such as Word2Vec)\nForbes 20 Incredible Women in AI\nFeatured in book Women Tech Founders on the Rise\nEarly data scientist and software engineer at Uber\n\n\n\n\n\nA few events I’ve spoken at"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "science, data ethics, education",
    "section": "",
    "text": "Qualitative humanities research is crucial to AI\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nFollowing the thread of any seemingly quantitative issue in AI ethics quickly leads to a host of qualitative questions. Unfortunately, there is often a large divide between computer scientists and social scientists, with over-simplified assumptions and fundamental misunderstandings.\n\n\n\n\n\n\nJun 1, 2022\n\n\nLouisa Bartolow and Rachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nAI Harms are Societal, Not Just Individual\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nIn the west, our ideas of harm are largely anchored to an individual being harmed by a particular action at a discrete moment in time. Yet the harms caused by algorithmic systems are often collective and communal.\n\n\n\n\n\n\nMay 17, 2022\n\n\nRachel Thomas and Louisa Bartolo\n\n\n\n\n\n\n  \n\n\n\n\nThere’s no such thing as not a math person\n\n\n\n\n\n\n\nadvice\n\n\neducation\n\n\n\n\nMany cultural factors, misconceptions, stereotypes, and obstacles turn people off to math.\n\n\n\n\n\n\nMar 15, 2022\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\n8 Things You Need to Know about Surveillance\n\n\n\n\n\n\n\nethics\n\n\n\n\nHow surveillance makes us less safe\n\n\n\n\n\n\nAug 7, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nTech’s Long Hours Are Discriminatory and Counterproductive\n\n\n\n\n\n\n\ninclusion\n\n\nwork\n\n\n\n\nWhy working longer doesn’t work.\n\n\n\n\n\n\nFeb 19, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nFive Things That Scare Me About AI\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nThis AI-powered future is already here, and some of the consequences are scarier than you may realize.\n\n\n\n\n\n\nJan 29, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nWhat do machine learning practitioners actually do?\n\n\n\n\n\n\n\nmachine learning\n\n\nadvice\n\n\n\n\nMachine learning is an in-demand field, but there are misconceptions about what the work entails in practice.\n\n\n\n\n\n\nJul 12, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Deep Learning for Tabular Data\n\n\n\n\n\n\n\nmachine learning\n\n\ntechnical\n\n\n\n\nDeep learning is not just for images and text.\n\n\n\n\n\n\nApr 29, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nHow (and why) to create a good validation set\n\n\n\n\n\n\n\nmachine learning\n\n\ntechnical\n\n\n\n\nAvoid failures from poorly chosen validation sets.\n\n\n\n\n\n\nNov 13, 2017\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nDiversity Washing Makes Things Worse\n\n\n\n\n\n\n\ninclusion\n\n\n\n\nShallow, showy diversity efforts aren’t just ineffective, they are actively harmful.\n\n\n\n\n\n\nDec 7, 2015\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nIf you think women in tech is just a pipeline problem, you haven’t been paying attention\n\n\n\n\n\n\n\ninclusion\n\n\n\n\nIt doesn’t matter how many girls you teach to code if you keep driving adult women out of the tech industry.\n\n\n\n\n\n\nJul 27, 2015\n\n\nRachel Thomas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html",
    "href": "posts/2015-07-27-not-pipeline/index.html",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "",
    "text": "This article has been translated into Spanish at Matajuegos and into Russian by Dmitry Si.\nAccording to the Harvard Business Review, 41% of women working in tech eventually end up leaving the field (compared to just 17% of men), and I can understand why…\nI first learned to code at age 16, and am now in my 30s. I have a math PhD from Duke. I still remember my pride in a “knight’s tour” algorithm that I wrote in C++ in high school; the awesome mind warp of an interpreter that can interpret itself (a Scheme course my first semester of college); my fascination with numerous types of matrix factorizations in C in grad school; and my excitement about relational databases and web scrapers in my first real job.\nOver a decade after I first learned to program, I still loved algorithms, but felt alienated and depressed by tech culture. While at a company that was a particularly poor culture fit, I was so unhappy that I hired a career counselor to discuss alternative career paths. Leaving tech would have been devastating, but staying was tough."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#work-hard-play-hard",
    "href": "posts/2015-07-27-not-pipeline/index.html#work-hard-play-hard",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Work hard, play hard",
    "text": "Work hard, play hard\nI’m not the stereotypical male programmer in his early 20s looking to “work hard, play hard”. I do work hard, but I’d rather wake up early than stay up late, and I was already thinking ahead to when my husband and I would need to coordinate our schedules with daycare drop-offs and pick-ups. Kegerators and ping pong tables don’t appeal to me. I’m not aggressive enough to thrive in a combative work environment. Talking to other female friends working in tech, I know that I’m not alone in my frustrations.\nWhen researcher Kieran Snyder interviewed 716 women who left tech after an average tenure of 7 years, almost all of them said they liked the work itself, but most were unhappy with the work environment. In NSF-funded research, Nadya Fouad surveyed 5,300 women who had earned engineering degrees (of all types) over the last 50 years, and 38% of them were no longer working as engineers. Fouad summarized her findings on why they leave with “It’s the climate, stupid!”\nThis is a huge, unnecessary, and expensive loss of talent in a field facing a supposed talent shortage. Given that tech is currently one of the major drivers of the US economy, this impacts everyone. Any tech company struggling to hire and retain as many employees as they need should particularly care about addressing this problem."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#your-company-is-not-a-meritocracy-and-you-are-not-gender-blind",
    "href": "posts/2015-07-27-not-pipeline/index.html#your-company-is-not-a-meritocracy-and-you-are-not-gender-blind",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Your company is NOT a meritocracy and you are NOT “gender-blind”",
    "text": "Your company is NOT a meritocracy and you are NOT “gender-blind”\nYou don’t know if you’re color-blind without testing either\nNobody wants to think of themselves as being sexist. However, a number of studies have shown that identical job applications or resumes are evaluated differently based on whether they are labeled with a male or female name. When men and women read identical scripts containing entrepreneurial pitches or salary negotiations, they are evaluated differently. Both men and women have been shown to have these biases. These biases occur unconsciously and without intention or malice.\nHere is a sampling of just a few of the studies on unconscious gender bias:\n\nInvestors preferred entrepreneurial ventures pitched by a man than an identical pitch from a woman by a rate of 68% to 32% in a study conducted jointly by HBS, Wharton, and MIT Sloan. “Male-narrated pitches were rated as more persuasive, logical and fact-based than were the same pitches narrated by a female voice.”\nIn a randomized, double-blind study by Yale researchers, science faculty at 6 major institutions evaluated applications for a lab manager position. Applications randomly assigned a male name were rated as significantly more competent and hirable and offered a higher starting salary and more career mentoring, compared to identical applications assigned female names.\nWhen men and women negotiated a job offer by reading identical scripts for a Harvard and CMU study, women who asked for a higher salary were rated as being more difficult to work with and less nice, but men were not perceived negatively for negotiating.\nPsychology faculty were sent CVs for an applicant (randomly assigned male or female name), and both men and women were significantly more likely to hire a male applicant than a female applicant with an identical record.\nIn 248 performance reviews of high-performers in tech, negative personality criticism (such as abrasive, strident, or irrational) showed up in 85% of reviews for women and just 2% of reviews for men. It is ridiculous to assume that 85% of women have personality problems and that only 2% of men do.\n\nMost concerningly, a study from Yale researchers shows that perceiving yourself as objective is actually correlated with showing even more bias. The mere desire to not be biased is not enough to overcome decades of cultural conditioning and can even lend more credence to post-hoc justifications. Acknowledging that you have biases that conflict with your values does not make you a bad person. It’s a natural result of our culture. The important thing is to find ways to eliminate them. Blindly believing your company is a meritocracy not only does not make it so, but will actually make it even harder to address implicit bias.\nBias is typically justified post-hoc. Our initial subconscious impression of the female applicant is negative, and then we find logical reasons to justify it. For instance, in the above study by Yale researchers if the male applicant for police chief had more street smarts and the female applicant had more formal education, evaluators decided that street smarts were the most important trait, and if the names were reversed, evaluators decided that formal education was the most important trait."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#good-news-and-bad-news",
    "href": "posts/2015-07-27-not-pipeline/index.html#good-news-and-bad-news",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Good News and Bad News",
    "text": "Good News and Bad News\n\nThe Bad News…\nBecause of the high attrition rate for women working in tech, teaching more girls and women to code is not enough to solve this problem. Because of the above well-documented differences in how men and women are perceived, training women to negotiate better and be more assertive is also not enough to solve this problem. Female voices are perceived as less logical and less persuasive than male voices. Women are perceived negatively for being too assertive. If tech culture is going to change, everyone needs to change, especially men and most especially leaders.\nThe professional and emotional costs to women for speaking out about discrimination can be large (in terms of retaliation, being perceived as less employable or difficult to work with, or companies then seeking to portray them as poor performers). I know a number of female software engineers who will privately share stories of sexism with trusted friends that we are not willing to share publicly because of the risk. This is why it is important to proactively address this issue. There is more than enough published research and personal stories from those who have chosen to publicly share to confirm that this is a widespread issue in the tech industry.\n\n\n…and the Good News\nChange is possible. Although these are schools and not tech companies, Harvey Mudd and Harvard Business School provide inspiring case studies. Strong leaders at both schools enacted sweeping changes to address previously male-centric cultures. Harvey Mudd has raised the percentage of computer science majors that are women to 40% (the national average is 18%). The top 5% of Harvard Business School graduates rose from being approximately 20% women to closer to 40% and the GPA gap between men and women closed, all within one year of making a number of comprehensive, structural changes."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#so-what-can-we-do-about-it",
    "href": "posts/2015-07-27-not-pipeline/index.html#so-what-can-we-do-about-it",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "So What Can We Do About It?",
    "text": "So What Can We Do About It?\nThese recommendations on what companies could do to improve their cultures are based on a mix of research and personal experience. My goal is to have a positive focus, and I would love it if you walked away with at least one concrete goal for making constructive change at your company.\n\nTrain managers\nIt is very common at tech start-ups to promote talented engineers to management without providing them with any management training or oversight, particularly at rapidly growing companies where existing leadership is stretched thin. These new managers are often not aware of any of the research on motivation, human psychology, or bias. Untrained, unsupervised managers cause more harm to women than men, although regardless, all employees would benefit from new managers receiving training, mentorship, and supervision.\n\n\nFormalize hiring and promotion criteria\nIn the Yale study mentioned above regarding applicants for police chief, getting participants to formalize their hiring criteria before they looked at applications (i.e. deciding if formal education or street smarts was more important) reduced bias. I was once on a team where the hiring criteria were amorphous and where the manager frequently overrode majority votes by the team because of “gut feeling”. It seemed like unconscious bias played a large role in decisions, but because of our haphazard approach to hiring, there was no way of truly knowing.\n\n\nLeaders, speak up and act in concrete ways\nLeadership sets the values and culture for a company, so the onus is on them to make it clear that they value diversity. Younger engineers and managers will follow their perceptions of what executives value. In the cases of positive change at Harvey Mudd and Harvard Business School, leadership at the top was spearheading these initiatives. Intel is going to begin tying executives’ compensation to whether they achieve diversity goals on their teams. As Kelly Shuster, director for the Denver chapter of Women Who Code has pointed out, leaders have to get rid of employees who engage in sexist or racist behavior. Otherwise, the company is at risk of losing talented employees, and is sending a message to all employees that discrimination is okay.\n\n\nDon’t rely on self-nominations or self-evaluations\nThere is a well-documented confidence gap between men and women. Don’t rely on people nominating themselves for promotions or to get the most interesting projects, since women are less likely to put themselves forward. Google relies on employees nominating themselves for promotions and data revealed that women were much less likely to do so (and thus much less likely to receive promotions). When senior women began hosting workshops encouraging women to nominate themselves, the number of women at Google receiving promotions increased. Groups are more likely to pick male leaders because of their over-confidence, compared to more qualified women who are less confident. Don’t rely heavily on self-evaluations in performance scoring. Women perceive their abilities as being worse than they are, whereas men have an inflated sense of their abilities.\n\n\nFormally audit employee data\nConfirm that men and women with the same qualifications are earning the same amount and that they are receiving promotions and raises at similar rates (and if not, explore why). Make sure that gendered criticism (such as calling a woman strident or abrasive) is not used in performance reviews. The trend of tech companies releasing their diversity statistics is a good one, but given the high industry attrition rate for women, they should also start releasing their retention rates broken down by gender. I would like to see companies release statistics on the rates at which women are given promotions or raises compared to men, and how performance evaluation scores compare between men and women. By publicly sharing data, companies can hold themselves accountable and can track changes over time.\n\n\nDon’t emphasize face time\nA culture that rewards facetime and encourages people to regularly stay late or eat dinner at the office puts employees with families at a disadvantage (particularly mothers), and research shows that working excess hours does not actually improve productivity in the long-term since workers begin to experience burn out after just a few weeks. Furthermore, when employees burn out and quit, the cost of recruiting and hiring a new employee is typically 20% of the annual salary for that position.\n\n\nCreate a collaborative environment\nStanford research studies document that women are more likely to dislike competitive environments compared to men and are more likely to select out of them, regardless of their ability. Given that women are perceived negatively for being too assertive, it is tougher for women to succeed in a highly aggressive environment as well. Men who speak up more than their peers are rewarded with 10% higher ratings, whereas women who speak up more are punished with 14% lower ratings. Creating a competitive culture where people must fight for their ideas makes it much tougher for women to succeed.\n\n\nOffer maternity leave\nOver 10% of the 716 women who left tech in Kieran Snyder’s research left because of inadequate maternity leave. Several were pressured to return from leave early or to be on call while on leave. These women did not want to be stay-at-home-parents, they just wanted to recover after giving birth. Just as you would not pressure someone to return to work without recovery time after a major surgery, women need time to physically heal after delivering a baby. When Google increased paid maternity leave from 12 weeks to 18 weeks, the number of new moms who quit Google dropped by 50%."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#some-final-thoughts",
    "href": "posts/2015-07-27-not-pipeline/index.html#some-final-thoughts",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Some final thoughts…",
    "text": "Some final thoughts…\n\nA note on racial bias\nThere is a huge amount of research on unconscious racial bias, and tech companies need to address this issue. As Nichole Sanchez, VP of Social Impact at GitHub, describes, calls for diversity are often solely about adding more white women, which is deeply problematic. Racial bias adds another intersectional dimension to the discrimination that women of color experience. In interviews with 60 women of color who work in STEM research, 100% of them had experienced discrimination, and the particular negative stereotypes they faced differed depending on their race. A resume with a traditionally African-American sounding name is less likely to be called for an interview than the same resume with a traditionally white sounding name. I do not have the personal experience to speak about this topic and instead encourage you to read these blog posts and articles by and about tech workers of color on the challenges they’ve faced: Erica Joy (Slack engineer, former Google engineer), Justin Edmund (designer, Pinterest’s 7th employee), Aston Motes (Engineer, Dropbox’s 1st employee), and Angelica Coleman (developer advocate at Zendesk, formerly at Dropbox)."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#now",
    "href": "posts/2015-07-27-not-pipeline/index.html#now",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Now",
    "text": "Now\nI’m currently teaching software development at all-women Hackbright Academy, a job that I love and that suits me perfectly. I want all women to have the opportunity (and I mean truly have the opportunity, without implicit or explicit discrimination) to learn how to program — knowing software development provides so many career and financial possibilities; it’s intellectually rewarding and fun; and being a creator is deeply satisfying. Although I know many women with frustrating experiences of sexism, I also know women who have found companies where they’re happily thriving. I’m glad for the attention tech’s diversity problem has been receiving and I am hopeful about continued change.\nThanks for review, edits, and discussion to: Jeremy Howard and Angie Chang.\nI do more research and further develop the ideas in this post in my later posts: on how showy, shallow diversity strategies make things worse; on bullshit diversity initiatives and some better ideas; and the research on how women are leaving tech because they can’t advance in their careers."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html",
    "href": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html",
    "title": "Diversity Washing Makes Things Worse",
    "section": "",
    "text": "It is painful watching tech companies known to be bad environments for women and people of color make shallow, showy attempts to rebrand themselves as valuing diversity. Perhaps you’re thinking, Any effort towards diversity is a good thing, and there’s no harm in trying, right? Wrong. This is not just a triumph of style over substance; these efforts can harm the people they are purporting to help. For instance, research shows that many diversity programs reduce the number of Black women and Black men in management; diversity structures cause people to be less likely to believe women and people of color; and some forms of unconscious bias training increase bias.\nSince the following points have been covered thoroughly elsewhere, I’m going to take it as a given that:\n\nTech has a diversity problem\nIt’s not just the pipeline (really)\nMeritocracy is a myth\nDiversity is a good thing\n\n(If you’re unfamiliar with these ideas, please read the linked articles; I highly recommend them.) I very much want to see these problems fixed, but they need more than just a coat of PR-friendly paint. Any successful effort towards diversity and inclusion will need to involve comprehensive changes, ongoing self-reflection, and tackling hard problems, not just superficial, high-publicity, quick fixes.\nThe rest of this post will dig into what the research shows about ways that diversity programs can backfire. My next article will suggest some ideas for effective programs."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#the-danger-of-diversity-washing",
    "href": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#the-danger-of-diversity-washing",
    "title": "Diversity Washing Makes Things Worse",
    "section": "The danger of diversity washing",
    "text": "The danger of diversity washing\nResearchers from U of Washington, UCLA, and UCSB showed that the mere presence of diversity policies, diversity training, and diversity awards cause white people to be less likely to believe racial discrimination exists and cause men to be less likely to believe gender discrimination exists, despite other data and evidence. Participants in one study read a New York Times article published as a class action lawsuit for gender discrimination against pharmaceutical giant Novartis went to trial. The twist was that half the participants were shown an article that included a sentence stating that Working Mother magazine had chosen Novartis as one of the 100 best companies in the USA. This sentence was omitted from the article for the other half. Those that read the sentence about the_Working Mother_ accolade were less likely to believe that the female employees had a valid case against Novartis, even though the rest of the article remained the same.\nThe researchers conducted 6 variations of the study. In one version, white people read either a diversity statement, or a mission statement, for a fictional company. They were then shown data on comparative promotion rates by race, as well as an article about a Black employee filing suit for racial discrimination. Participants who had read the diversity statement were less likely to believe that discrimination had occurred and rated the Black employee more negatively (compared to those that read the mission statement, which did not mention diversity), even when the data showed clear racial differences. Other versions of the study provided differing data on hiring rates and salaries. In all versions, the presence of a diversity structure (such as diversity policies, diversity training, or diversity awards) caused white people to be less likely to believe racial discrimination and caused men to be less likely to believe gender discrimination.\nThis shows that the presence of diversity programs can hurt women and people of color by creating what the study’s authors call an illusion of fairness. Because of the “diversity branding”, people are less likely to believe that discrimination exists at that company, regardless of what the data shows. So the next time you see a tech company announce their shiny new diversity initiative with much fanfare, consider one impact: that the negative accounts of women and people of color that work at these companies will be disregarded even more often. This belief that the existence of diversity initiatives equals equality is just one way that such efforts can backfire. Next, let’s look at some real world data from the outcomes of diversity programs (it’s not what you would hope)."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#what-does-data-from-diversity-programs-at-over-700-companies-reveal",
    "href": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#what-does-data-from-diversity-programs-at-over-700-companies-reveal",
    "title": "Diversity Washing Makes Things Worse",
    "section": "What does data from diversity programs at over 700 companies reveal?",
    "text": "What does data from diversity programs at over 700 companies reveal?\nHarvard professor Frank Dobbin led a team of sociologists in reviewing data from 708 companies to evaluate 7 different approaches to trying to increase the share of White women, Black women, and Black men in management, and found that many programs were ineffective and some diversity efforts even made things worse. Programs that targeted stereotypes through education and feedback, such as diversity training, were the least effective, and in some cases reduced diversity. The study found that diversity training was followed by a 7% decline in the proportion of Black women in management. Diversity evaluations of managers were followed by an 8% decline in Black men in management, although a 6% increase for White women. This is a particular issue when tech diversity efforts are often limited only to recruiting White women.\nThe most effective programs were “responsibility structures” such as diversity committees, diversity staff positions, and affirmative action plans. Professor Dobbin stated that “if no one is specifically charged with the task of increasing diversity, then the buck inevitably gets passed ad infinitum. To increase diversity, executives must treat it like any other business goal.” Networking and mentoring produced modest positive effects. Diversity training was one of the least effective approaches. Dobbin says that “even with best practices, you’re not going to get much of an effect. It doesn’t change what happens at work.”\nTogether with Alexandra Kalev of Tel Aviv University, Dobbin later expanded the research to 803 companies, and to include Asian and Hispanic employees (in addition to Black and White employees) in the aptly titled study “Try and Make Me!: Why Corporate Diversity Training Fails.” It is possible that the newer training programs currently in use by tech companies may end up being more effective than those reviewed in Dobbin’s study; however we should wait to see the evidence."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#unconscious-bias-training-can-increase-bias",
    "href": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#unconscious-bias-training-can-increase-bias",
    "title": "Diversity Washing Makes Things Worse",
    "section": "Unconscious bias training can increase bias",
    "text": "Unconscious bias training can increase bias\nUnconscious bias (for example, when people rate identical resumes with a female or traditionally African American name more negatively, as opposed to the same resume with a male or traditionally white name) is a very real problem; however just teaching people that unconscious bias exists does not eliminate it and can even increase bias. In this NYTimes article, Sheryl Sandberg and Wharton professor Adam Grant summarize research that unconscious bias training can increase bias, depending on how it is communicated.\nIn one study from UVA and Washington University, managers read a job interview transcript after either being told either that: stereotypes are rare; or being told that: many people believe stereotypes. When participants were told stereotypes were common and that the candidate was female, they were 28% less likely to hire her and judged her as 27% less likable (compared to the identical transcript with the candidate labeled as male). People may feel more comfortable believing stereotypes when they hear that they are commonly believed. Sandberg and Grant argue that the key is to instead communicate that biases are inaccurate, and that most people don’t want to discriminate."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#the-difficulty-of-self-reflection",
    "href": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#the-difficulty-of-self-reflection",
    "title": "Diversity Washing Makes Things Worse",
    "section": "The Difficulty of Self-Reflection",
    "text": "The Difficulty of Self-Reflection\nPart of the problem with efforts to raise awareness of unconscious bias is that we are great at finding post-hoc justifications for our biases so we tend to see ourselves as immune to bias. Harvard Business School professor Francesca Gino, who has taught courses on biases and decision making to executives and MBA students, states “most of my students easily recognize that their colleagues and friends are biased but generally don’t think they are themselves.” It is both easier to believe that other people discriminate, but not me, because I have good judgement, and easier to believe that other people experience discrimination, but not my coworker, because I see her flaws.\nA study from Yale researchers shows that perceiving yourself as objective is actually correlated with showing even more bias. Researchers from MIT and Indiana University found that company structures that explicitly promote meritocracy (compared to those that don’t) show greater bias against women. The 445 participants in the study all had managerial experience and were asked to evaluate employee profiles given a set of organizational core values (which included meritocracy in some cases but not others). Women were awarded smaller bonuses than men with equivalent performance reviews when the core values emphasized meritocracy.\nGender and racial biases aren’t just problems affecting the education system and other people’s companies; biases are affecting your company. It’s not just that other people need to change; we all need to change, and it’s an ongoing process."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#no-easy-fixes",
    "href": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#no-easy-fixes",
    "title": "Diversity Washing Makes Things Worse",
    "section": "No Easy Fixes",
    "text": "No Easy Fixes\nI am not the first to express similar frustrations, although I have seen little coverage by tech journalists of this aspect of the issue. Erica Joy, an engineer at Slack, wrote that she is “tired of the ‘we hired this many’ and ‘we gave this many dollars to girls coding initiatives.’ None of those numbers accurately portray what the inside of a company looks like.” Darrell Jones III, head of business development at Clef, explained, “when we allow companies to simply ‘educate’ their employees or ‘spread awareness’ by publishing dismal diversity numbers, we let them off the hook.” Cate Huston, a former Google engineer and head of mobile development at Ride, ranked classes of diversity problems from easy to extra hard, and, expressed her disappointment that Grace Hopper Conference (for women in computing) primarily focuses on “easy problems,” asking, “Is it just going to be a celebration of managing the easy things. Of crawling over that exceptionally low bar of sexist marketing materials. Of focusing on the pipeline rather than the woman who are already here.” Freada Kapor Klein, founder of the Level Playing Field Institute, has observed that “there is way too much money going to hackathons teaching privileged girls how to code without any tie-in to anything else… We’re mapping out all of the drop-off points so that as opposed to being the 400th person who funds a girls coding program, we can even out the dollars.” Indie game developer Veve Jaffe wrote of Intel’s diversity efforts, “my experience reflects a growing trend of corporations paying lip service to diversity — and collecting all of its PR benefits — while demanding unpaid work from underrepresented developers.”\nThis is not an argument against donating to girls coding initiatives, hosting hackathons for girls, or creating inclusive marketing materials (all are good things!), just a reminder that these won’t impact the biases the talented women and people of color already working at your company are currently facing. No donation is ever a substitute for the hard work of self-reflection and company-wide change. The “easy” changes are necessary, but they are not sufficient.\nSimilarly, collecting and sharing diversity data is a necessary step in determining if your current approach is working, but not something to celebrate on its own. It is just a means towards the end goal of creating an equitable and inclusive work environment. If the data shows your approach is not working, you need to change what you’re doing.\nOne of the results of the research on data from 708 companies is that “new programs decoupled from everyday practice often have no impact” and that it is more effective to rethink hiring and promotion structures entirely. This is similar to what Dr. Klein has found working on diversity issues for over a decade; Klein says that having “engineering deeply involved in company diversity and inclusion efforts is critically important to getting it right and not having it be a side annoying thing.”"
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#conclusion",
    "href": "posts/2015-12-07-diversity-washing/2015-12-07-diversity-washing.html#conclusion",
    "title": "Diversity Washing Makes Things Worse",
    "section": "Conclusion",
    "text": "Conclusion\nThis post is not an argument against diversity initiatives, but a reminder that diversity and inclusion aren’t automatically achieved when these programs are announced. We should all be on the lookout for concrete progress (not just raw hiring numbers, but also percentage of women and people of color in management and executive positions, salary parity, and retention rates), as well as for negative side effects. Any successful effort towards diversity and inclusion will need to involve ongoing self-reflection, comprehensive changes, and addressing hard problems. In my next post, I will write about examples of more substantial changes.\nMany thanks to Jeremy Howard for feedback on earlier drafts of this post.\nI further develop the ideas here in my next post about bullshit diversity strategies and some better ideas for enacting positive change. I later survey the research on why women are unable to advance in their careers and offer concrete strategies to address this problem. You may also be interested in my post debunking the pipeline myth, which shares my personal story of wanting to leave the tech industry, as well as practical tips."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html",
    "href": "posts/2017-11-13-validation-sets/index.html",
    "title": "How (and why) to create a good validation set",
    "section": "",
    "text": "An all-too-common scenario: a seemingly impressive machine learning model is a complete failure when implemented in production. The fallout includes leaders who are now skeptical of machine learning and reluctant to try it again. How can this happen?\nOne of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a train_test_split method, this method takes a random subset of the data, which is a poor choice for many real-world problems.\nThe definitions of training, validation, and test sets can be fairly nuanced, and the terms are sometimes inconsistently used. In the deep learning community, “test-time inference” is often used to refer to evaluating on data in production, which is not the technical definition of a test set. As mentioned above, sklearn has a train_test_split method, but no train_validation_test_split. Kaggle only provides training and test sets, yet to do well, you will need to split their training set into your own validation and training sets. Also, it turns out that Kaggle’s test set is actually sub-divided into two sets. It’s no suprise that many beginners may be confused! I will address these subtleties below."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#first-what-is-a-validation-set",
    "href": "posts/2017-11-13-validation-sets/index.html#first-what-is-a-validation-set",
    "title": "How (and why) to create a good validation set",
    "section": "First, what is a “validation set”?",
    "text": "First, what is a “validation set”?\nWhen creating a machine learning model, the ultimate goal is for it to be accurate on new data, not just the data you are using to build it. Consider the below example of 3 different models for a set of data:\n\n\n\nunder-fitting and over-fitting (Source: Andrew Ng’s Machine Learning Coursera class)\n\n\nThe error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it’s not the best choice. Why is that? If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\nThe underlying idea is that:\n\nthe training set is used to train a given model\nthe validation set is used to choose between models (for instance, does a random forest or a neural net work better for your problem? do you want a random forest with 40 trees or 50 trees?)\nthe test set tells you how you’ve done. If you’ve tried out a lot of different models, you may get one that does well on your validation set just by chance, and having a test set helps make sure that is not the case.\n\nA key property of the validation and test sets is that they must be representative of the new data you will see in the future. This may sound like an impossible order! By definition, you haven’t seen this data yet. But there are still a few things you know about it."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#when-is-a-random-subset-not-good-enough",
    "href": "posts/2017-11-13-validation-sets/index.html#when-is-a-random-subset-not-good-enough",
    "title": "How (and why) to create a good validation set",
    "section": "When is a random subset not good enough?",
    "text": "When is a random subset not good enough?\nIt’s instructive to look at a few examples. Although many of these examples come from Kaggle competitions, they are representative of problems you would see in the workplace.\n\nTime series\nIf your data is a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates your are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future). If your data includes the date and you are building a model to use in the future, you will want to choose a continuous section with the latest dates as your validation set (for instance, the last two weeks or last month of the available data).\nSuppose you want to split the time series data below into training and validation sets:\n\n\n\nTime series data\n\n\nA random subset is a poor choice (too easy to fill in the gaps, and not indicative of what you’ll need in production):\n\n\n\na poor choice for your training set\n\n\nUse the earlier data as your training set (and the later data for the validation set):\n\n\n\na better choice for your training set\n\n\nKaggle currently has a competition to predict the sales in a chain of Ecuadorian grocery stores. Kaggle’s “training data” runs from Jan 1 2013 to Aug 15 2017 and the test data spans Aug 16 2017 to Aug 31 2017. A good approach would be to use Aug 1 to Aug 15 2017 as your validation set, and all the earlier data as your training set.\n\n\nNew people, new boats, new…\nYou also need to think about what ways the data you will be making predictions for in production may be qualitatively different from the data you have to train your model with.\nIn the Kaggle distracted driver competition, the independent data are pictures of drivers at the wheel of a car, and the dependent variable is a category such as texting, eating, or safely looking ahead. If you were the insurance company building a model from this data, note that you would be most interested in how the model performs on drivers you haven’t seen before (since you would likely have training data only for a small group of people). This is true of the Kaggle competition as well: the test data consists of people that weren’t used in the training set.\n\n\n\nTwo images of the same person drinking a soda while driving\n\n\nIf you put one of the above images in your training set and one in the validation set, your model will seem to be performing better than it would on new people. Another perspective is that if you used all the people in training your model, your model may be overfitting to particularities of those specific people, and not just learning the states (texting, eating, etc).\nA similar dynamic was at work in the Kaggle fisheries competition to identify the species of fish caught by fishing boats in order to reduce illegal fishing of endangered populations. The test set consisted of boats that didn’t appear in the training data. This means that you’d want your validation set to include boats that are not in the training set.\nSometimes it may not be clear how your test data will differ. For instance, for a problem using satellite imagery, you’d need to gather more information on whether the training set just contained certain geographic locations, or if it came from geographically scattered data."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#the-dangers-of-cross-validation",
    "href": "posts/2017-11-13-validation-sets/index.html#the-dangers-of-cross-validation",
    "title": "How (and why) to create a good validation set",
    "section": "The dangers of cross-validation",
    "text": "The dangers of cross-validation\nThe reason that sklearn doesn’t have a train_validation_test split is that it is assumed you will often be using cross-validation, in which different subsets of the training set serve as the validation set. For example, for a 3-fold cross validation, the data is divided into 3 sets: A, B, and C. A model is first trained on A and B combined as the training set, and evaluated on the validation set C. Next, a model is trained on A and C combined as the training set, and evaluated on validation set B. And so on, with the model performance from the 3 folds being averaged in the end.\nHowever, the problem with cross-validation is that it is rarely applicable to real world problems, for all the reasons describedin the above sections. Cross-validation only works in the same cases where you can randomly shuffle your data to choose a validation set."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#kaggles-training-set-your-training-validation-sets",
    "href": "posts/2017-11-13-validation-sets/index.html#kaggles-training-set-your-training-validation-sets",
    "title": "How (and why) to create a good validation set",
    "section": "Kaggle’s “training set” = your training + validation sets",
    "text": "Kaggle’s “training set” = your training + validation sets\nOne great thing about Kaggle competitions is that they force you to think about validation sets more rigorously (in order to do well). For those who are new to Kaggle, it is a platform that hosts machine learning competitions. Kaggle typically breaks the data into two sets you can download:\n\na training set, which includes the independent variables, as well as the dependent variable (what you are trying to predict). For the example of an Ecuadorian grocery store trying to predict sales, the independent variables include the store id, item id, and date; the dependent variable is the number sold. For the example of trying to determine whether a driver is engaging in dangerous behaviors behind the wheel, the independent variable could be a picture of the driver, and the dependent variable is a category (such as texting, eating, or safely looking forward).\na test set, which just has the independent variables. You will make predictions for the test set, which you can submit to Kaggle and get back a score of how well you did.\n\nThis is the basic idea needed to get started with machine learning, but to do well, there is a bit more complexity to understand. You will want to create your own training and validation sets (by splitting the Kaggle “training” data). You will just use your smaller training set (a subset of Kaggle’s training data) for building your model, and you can evaluate it on your validation set (also a subset of Kaggle’s training data) before you submit to Kaggle.\nThe most important reason for this is that Kaggle has split the test data into two sets: for the public and private leaderboards. The score you see on the public leaderboard is just for a subset of your predictions (and you don’t know which subset!). How your predictions fare on the private leaderboard won’t be revealed until the end of the competition. The reason this is important is that you could end up overfitting to the public leaderboard and you wouldn’t realize it until the very end when you did poorly on the private leaderboard. Using a good validation set can prevent this. You can check if your validation set is any good by seeing if your model has similar scores on it to compared with on the Kaggle test set.\nAnother reason it’s important to create your own validation set is that Kaggle limits you to two submissions per day, and you will likely want to experiment more than that. Thirdly, it can be instructive to see exactly what you’re getting wrong on the validation set, and Kaggle doesn’t tell you the right answers for the test set or even which data points you’re getting wrong, just your overall score.\nUnderstanding these distinctions is not just useful for Kaggle. In any predictive machine learning project, you want your model to be able to perform well on new data."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html",
    "href": "posts/2018-04-29-categorical-embeddings/index.html",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "",
    "text": "There is a powerful technique that is winning Kaggle competitions and is widely used at Google (according to Jeff Dean), Pinterest, and Instacart, yet that many people don’t even realize is possible: the use of deep learning for tabular data, and in particular, the creation of embeddings for categorical variables.\nDespite what you may have heard, you can use deep learning for the type of data you might keep in a SQL database, a Pandas DataFrame, or an Excel spreadsheet (including time-series data). I will refer to this as tabular data, although it can also be known as relational data, structured data, or other terms (see my twitter poll and comments for more discussion).\nTabular data is the most commonly used type of data in industry, but deep learning on tabular data receives far less attention than deep learning for computer vision and natural language processing. This post covers some key concepts from applying neural networks to tabular data, in particular the idea of creating embeddings for categorical variables, and highlights 2 relevant modules of the fastai library:\nThe material from this post is covered in much more detail starting around 1:59:45 in the Lesson 3 video and continuing in Lesson 4 of our free, online Practical Deep Learning for Coders course. To see example code of how this approach can be used in practice, check out our Lesson 3 jupyter notebook."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#embeddings-for-categorical-variables",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#embeddings-for-categorical-variables",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Embeddings for Categorical Variables",
    "text": "Embeddings for Categorical Variables\nA key technique to making the most of deep learning for tabular data is to use embeddings for your categorical variables. This approach allows for relationships between categories to be captured. Perhaps Saturday and Sunday have similar behavior, and maybe Friday behaves like an average of a weekend and a weekday. Similarly, for zip codes, there may be patterns for zip codes that are geographically near each other, and for zip codes that are of similar socio-economic status.\n\nTaking Inspiration from Word Embeddings\nA way to capture these multi-dimensional relationships between categories is to use embeddings. This is the same idea as is used with word embeddings, such as Word2Vec. For instance, a 3-dimensional version of a word embedding might look like:\n\n\n\n\n\n\n\npuppy\n[0.9, 1.0, 0.0]\n\n\ndog\n[1.0, 0.2, 0.0]\n\n\nkitten\n[0.0, 1.0, 0.9]\n\n\ncat\n[0.0, 0.2, 1.0]\n\n\n\nNotice that the first dimension is capturing something related to being a dog, and the second dimension captures youthfulness. This example was made up by hand, but in practice you would use machine learning to find the best representations (while semantic values such as dogginess and youth would be captured, they might not line up with a single dimension so cleanly). You can check out my workshop on word embeddings for more details about how word embeddings work.\n\n\n\nillustration from my word embeddings workshop: vectors for baby animal words are closer together, and an unrelated word like ‘avalanche’ is further away\n\n\n\n\nApplying Embeddings for Categorical Variables\nSimilarly, when working with categorical variables, we will represent each category by a vector of floating point numbers (the values of this representation are learned as the network is trained).\nFor instance, a 4-dimensional version of an embedding for day of week could look like:\n\n\n\n\n\n\n\nSunday\n[.8, .2, .1, .1]\n\n\nMonday\n[.1, .2, .9, .9]\n\n\nTuesday\n[.2, .1, .9, .8]\n\n\n\nHere, Monday and Tuesday are fairly similar, yet they are both quite different from Sunday. Again, this is a toy example. In practice, our neural network would learn the best representations for each category while it is training, and each dimension (or direction, which doesn’t necessarily line up with ordinal dimensions) could have multiple meanings. Rich relationships can be captured in these distributed representations.\n\n\nReusing Pretrained Categorical Embeddings\nEmbeddings capture richer relationships and complexities than the raw categories. Once you have learned embeddings for a category which you commonly use in your business (e.g. product, store id, or zip code), you can use these pre-trained embeddings for other models. For instance, Pinterest has created 128-dimensional embeddings for its pins in a library called Pin2Vec, and Instacart has embeddings for its grocery items, stores, and customers.\n\n\n\nFrom the Instacart blog post ‘Deep Learning with Emojis (not Math)’\n\n\nThe fastai library contains an implementation for categorical variables, which work with Pytorch’s nn.Embedding module, so this is not something you need to code from hand each time you want to use it."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#treating-some-continuous-variables-as-categorical",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#treating-some-continuous-variables-as-categorical",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Treating some Continuous Variables as Categorical",
    "text": "Treating some Continuous Variables as Categorical\nWe generally recommend treating month, year, day of week, and some other variables as categorical, even though they could be treated as continuous. Often for variables with a relatively small number of categories, this results in better performance. This is a modeling decision that the data scientist makes. Generally, we want to keep continuous variables represented by floating point numbers as continuous.\nAlthough we can choose to treat continuous variables as categorical, the reverse is not true: any variables that are categorical must be treated as categorical."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#time-series-data",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#time-series-data",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Time Series Data",
    "text": "Time Series Data\nThe approach of using neural networks together with categorical embeddings can be applied to time series data as well. In fact, this was the model used by students of Yoshua Bengio to win 1st place in the Kaggle Taxi competition(paper here), using a trajectory of GPS points and timestamps to predict the length of a taxi ride. It was also used by the 3rd place winners in the Kaggle Rossmann Competition, which involved using time series data from a chain of stores to predict future sales. The 1st and 2nd place winners of this competition used complicated ensembles that relied on specialist knowledge, while the 3rd place entry was a single model with no domain-specific feature engineering.\n\n\n\nThe winning architecture from the Kaggle Taxi Trajectory Competition\n\n\nIn our Lesson 3 jupyter notebook we walk through a solution for the Kaggle Rossmann Competition. This data set (like many data sets) includes both categorical data (such as the state the store is located in, or being one of 3 different store types) and continuous data (such as the distance to the nearest competitor or the temperature of the local weather). The fastai library lets you enter both categorical and continuous variables as input to a neural network.\nWhen applying machine learning to time-series data, you nearly always want to choose a validation set that is a continuous selection with the latest available dates that you have data for. As I wrote in a previous post, “If your data is a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates your are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future).”\nOne key to successfully using deep learning with time series data is to split the date into multiple categorical variables (year, month, week, day of week, day of month, and Booleans for whether it’s the start/end of a month/quarter/year). The fastai library has implemented a method to handle this for you, as described below."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#modules-to-know-in-the-fastai-library",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#modules-to-know-in-the-fastai-library",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Modules to Know in the Fastai Library",
    "text": "Modules to Know in the Fastai Library\nWe will be releasing more documentation for the fastai library in coming months, but it is already available on pip and on github, and it is used in the Practical Deep Learning for Coders course. The fastai library is built on top of Pytorch and encodes best practices and helpful high-level abstractions for using neural networks. The fastai library achieves state-of-the-art results and was recently used to win the Stanford DAWNBench competition (fastest CIFAR10 training).\n\nfastai.column_data\nfastai.column_data.ColumnarModelData takes a Pandas DataFrame as input and creates a type of ModelData object (an object which contains data loaders for the training, validation, and test sets, and which is the fundamental way of keeping track of your data while training models).\n\n\nfastai.structured\nThe fastai.structured module of the fastai library is built on top of Pandas, and includes methods to transform DataFrames in a number of ways, improving the performance of machine learning models by pre-processing the data appropriately and creating the right types of variables.\nFor instance, fastai.structured.add_datepart converts dates (e.g. 2000-03-11) into a number of variables (year, month, week, day of week, day of month, and booleans for whether it’s the start/end of a month/quarter/year.)\nOther useful methods in the module allow you to:\n\nFill in missing values with the median whilst adding a boolean indicator variable (fix_missing)\nChange any columns of strings in a Pandas DataFrame to a column of categorical values (train_cats)"
  },
  {
    "objectID": "posts/2018-07-12-automl1/index.html",
    "href": "posts/2018-07-12-automl1/index.html",
    "title": "What do machine learning practitioners actually do?",
    "section": "",
    "text": "This post is part 1 of a series. Part 2 is an opinionated introduction to AutoML and neural architecture search, and Part 3 looks at Google’s AutoML in particular.\nThere are frequent media headlines about both the scarcity of machine learning talent (see here, here, and here) and about the promises of companies claiming their products automate machine learning and eliminate the need for ML expertise altogether (see here, here, and here). In his keynote at the TensorFlow DevSummit, Google’s head of AI Jeff Dean estimated that there are tens of millions of organizations that have electronic data that could be used for machine learning but lack the necessary expertise and skills. I follow these issues closely since my work at fast.ai focuses on enabling more people to use machine learning and on making it easier to use.\nIn thinking about how we can automate some of the work of machine learning, as well as how to make it more accessible to people with a wider variety of backgrounds, it’s first necessary to ask, what is it that machine learning practitioners do? Any solution to the shortage of machine learning expertise requires answering this question: whether it’s so we know what skills to teach, what tools to build, or what processes to automate.\n\n\n\nWhat do machine learning practitioners do? (Image Source: #WOCinTech Chat)\n\n\nThis post is the first in a 3-part series. It will address what it is that machine learning practitioners do, with Part 2 explaining AutoML and neural architecture search (which several high profile figures have suggested will be key to decreasing the need for data scientists) and Part 3 will cover Google’s heavily hyped AutoML product in particular.\n\nBuilding Data Products is Complex Work\n\nWhile many academic machine learning sources focus almost exclusively on predictive modeling, that is just one piece of what machine learning practitioners do in the wild. The processes of appropriately framing a business problem, collecting and cleaning the data, building the model, implementing the result, and then monitoring for changes are interconnected in many ways that often make it hard to silo off just a single piece (without at least being aware of what the other pieces entail). As Jeremy Howard et al. wrote in Designing great data products, Great predictive modeling is an important part of the solution, but it no longer stands on its own; as products become more sophisticated, it disappears into the plumbing.\n\n\n\nBuilding Data Products is Complex Work (Source: Wikimedia Commons)\n\n\nA team from Google, D. Sculley et al., wrote the classic Machine Learning: The High-Interest Credit Card of Technical Debt about the code complexity and technical debt often created when using machine learning in practice. The authors identify a number of system-level interactions, risks, and anti-patterns, including:\n\nglue code: massive amount of supporting code written to get data into and out of general-purpose packages\npipeline jungles: the system for preparing data in an ML-friendly format may become a jungle of scrapes, joins, and sampling steps, often with intermediate files output\nre-use input signals in ways that create unintended tight coupling of otherwise disjoint systems\nrisk that changes in the external world may make models or input signals change behavior in unintended ways, and these can be difficult to monitor\n\nThe authors write, A remarkable portion of real-world “machine learning” work is devoted to tackling issues of this form… It’s worth noting that glue code and pipeline jungles are symptomatic of integration issues that may have a root cause in overly separated “research” and “engineering” roles… It may be surprising to the academic community to know that only a tiny fraction of the code in many machine learning systems is actually doing “machine learning”. (emphasis mine)\n\nWhen machine learning projects fail\n\nIn a previous post, I identified some failure modes in which machine learning projects are not effective in the workplace:\n\nThe data science team builds really cool stuff that never gets used. There’s no buy-in from the rest of the organization for what they’re working on, and some of the data scientists don’t have a good sense of what can realistically be put into production.\nThere is a backlog with data scientists producing models much faster than there is engineering support to put them in production.\nThe data infrastructure engineers are separate from the data scientists. The pipelines don’t have the data the data scientists are asking for now, and the data scientists are under-utilizing the data sources the infrastructure engineers have collected.\nThe company has definitely decided on feature/product X. They need a data scientist to gather some data that supports this decision. The data scientist feels like the PM is ignoring data that contradicts the decision; the PM feels that the data scientist is ignoring other business logic.\nThe data science team interviews a candidate with impressive math modeling and engineering skills. Once hired, the candidate is embedded in a vertical product team that needs simple business analytics. The data scientist is bored and not utilizing their skills.\n\nI framed these as organizational failures in my original post, but they can also be described as various participants being overly focused on just one slice of the complex system that makes up a full data product. These are failures of communication and goal alignment between different parts of the data product pipeline.\n\nSo, what do machine learning practitioners do?\n\nAs suggested above, building a machine learning product is a multi-faceted and complex task. Here are some of the things that machine learning practitioners may need to do during the process:\nUnderstanding the context:\n\nidentify areas of the business that could benefit from machine learning\ncommunicate with other stakeholders about what machine learning is and is not capable of (there are often many misconceptions)\ndevelop understanding of business strategy, risks, and goals to make sure everyone is on the same page\nidentify what kind of data the organization has\nappropriately frame and scope the task\nunderstand operational constraints (e.g. what data is actually available at inference time)\nproactively identify ethical risks, including how your work could be mis-used by harassers, trolls, authoritarian governments, or for propaganda/disinformation campaigns (and plan how to reduce these risks)\nidentify potential biases and potential negative feedback loops\n\nData: - make plans to collect more of different data (if needed and if possible) - stitch together data from many different sources: often this data has been collected in different formats or with inconsistent conventions - deal with missing or corrupted data - visualize the data - create appropriate training, validation, and test sets\nModeling: - choose which model to use - fit model resource needs into constraints (e.g. will the completed model need to run on an edge device, in a low memory or high latency environment, etc) - choose hyperparameters (e.g. in the case of deep learning, this includes choosing an architecture, loss function, and optimizer) - train the model (and debug why it’s not training). This can involve: - adjusting hyperparmeters (e.g. such as the learning rate) - outputing intermediate results to see how the loss, training error, and validation error are changing with time - inspecting the data the model is wrong on to look for patterns - identifying underlying errors or issues with the data - realizing you need to change how you clean and pre-process the data - realizing you need more or different data augmentation - realizing you need more or different data - trying out different models - identifying if you are under- or over-fitting\nProductionize: - creating an API or web app with your model as an endpoint in order to productionize - exporting your model into the needed format - plan for how often your model will need to be retrained with updated data (e.g. perhaps you will retrain nightly or weekly)\nMonitor: - track model performance over time - monitor the input data, to identify if it changes with time in a way that would invalidate your model - communicate your results to the rest of the organization - have a plan in place for how you will monitor and respond to mistakes or unexpected consequences\nCertainly, not every machine learning practitioner needs to do all of the above steps, but components of this process will be a part of many machine learning applications. Even if you are working on just a subset of these steps, a familiarity with the rest of the process will help ensure that you are not overlooking considerations that would keep your project from being successful!\n\nTwo of the hardest parts of Machine Learning\n\nFor myself and many others I know, I would highlight two of the most time-consuming and frustrating aspects of machine learning (in particular, deep learning) as:\n\nDealing with data formatting, inconsistencies, and errors is often a messy and tedious process.\nTraining deep learning models is a notoriously brittle process right now.\n\n\nIs cleaning data really part of ML? Yes.\n\nDealing with data formatting, inconsistencies, and errors is often a messy and tedious process. People will sometimes describe machine learning as separate from data science, as though for machine learning, you can just begin with your nicely cleaned, formatted data set. However, in my experience, the process of cleaning a data set and training a model are usually interwoven: I frequently find issues in the model training that cause me to go back and change the pre-processing for the input data.\n\n\n\nDealing with messy and inconsistent data is necessary\n\n\n\nTraining Deep Learning Models is Brittle and Finicky (for now)\n\nThe difficulty of getting models to train deters many beginners, who often wind up feeling discouraged. Even experts frequently complain of how frustrating and fickle the training process can be. One AI researcher at Stanford told me, I taught a course on deep learning and had all the students do their own projects. It was so hard. The students couldn’t get their models to train, and we were like “well, that’s deep learning”. Ali Rahimi, an AI researcher with over a decade of experience and winner of the NIPS 2017 Test of Time Award, complained about the brittleness of training in his NeurIPS award speech. How many of you have designed a deep net from scratch, built it from the ground up, architecture and all, and when it didn’t work, you felt bad about yourself? Rahimi asked the audience of AI researchers, and many raised their hands. Rahimi continued, This happens to me about every 3 months.\nThe fact that even AI experts sometimes have trouble training new models implies that the process has yet to be automated in a way where it could be incorporated into a general-purpose product. Some of the biggest advances in deep learning will come through discovering more robust training methods. We have already seen this some with advances like dropout, super convergence, and transfer learning, all of which make training easier. Through the power of transfer learning (to be discussed in Part 3) training can be a robust process when defined for a narrow enough problem domain; however, we still have a ways to go in making training more robust in general.\n\nFor Academic Researchers\n\nEven if you are working on theoretical machine learning research, it is useful to understand the process that machine learning practitioners working on practical problems go through, as that might provide insights on what the most relevant or high-impact areas of research are.\nAs Googler engineers D. Sculley et al. wrote, Technical debt is an issue that both engineers and researchers need to be aware of. Research solutions that provide a tiny accuracy benefit at the cost of massive increases in system complexity are rarely wise practice… Paying down technical debt is not always as exciting as proving a new theorem, but it is a critical part of consistently strong innovation. And developing holistic, elegant solutions for complex machine learning systems is deeply rewarding work. (emphasis mine)\n\nAutoML\n\nNow that we have an overview of some of the tasks that machine learning practitioners do as part of their work, we are ready to evaluate attempts to automate this work. As it’s name suggests, AutoML is one field in particular that has focused on automating machine learning, and a subfield of AutoML called neural architecture search is currently receiving a ton of attention. In part 2, I will explain what AutoML and neural architecture search are, and in part 3, look at Google’s AutoML in particular.\nBe sure to check out Part 2: An Opinionated Introduction to AutoML Neural Architecture Search, and Part 3: Google’s AutoML: Cutting Through the Hype"
  },
  {
    "objectID": "posts/2019-01-29-five-scary-things/index.html",
    "href": "posts/2019-01-29-five-scary-things/index.html",
    "title": "Five Things That Scare Me About AI",
    "section": "",
    "text": "AI is being increasingly used to make important decisions. Many AI experts (including Jeff Dean, head of AI at Google, and Andrew Ng, founder of Coursera and deeplearning.ai) say that warnings about sentient robots are overblown, but other harms are not getting enough attention. I agree. I am an AI researcher, and I’m worried about some of the societal impacts that we’re already seeing. In particular, these 5 things scare me about AI:\n\n\nAlgorithms are often implemented without ways to address mistakes.\n\n\nAI makes it easier to not feel responsible.\n\n\nAI encodes & magnifies bias.\n\n\nOptimizing metrics above all else leads to negative outcomes.\n\n\nThere is no accountability for big tech companies.\n\n\nAt the end, I’ll briefly share some positive ways that we can try to address these.\nBefore we dive in, I need to clarify one point that is important to understand: algorithms (and the complex systems they are a part of) can make mistakes. These mistakes come from a variety of sources: bugs in the code, inaccurate or biased data, approximations we have to make (e.g. you want to measure health and you use hospital readmissions as a proxy, or you are interested in crime and use arrests as a proxy. These things are related, but not the same), misunderstandings between different stakeholders (policy makers, those collecting the data, those coding the algorithm, those deploying it), how computer systems interact with human systems, and more.\nThis article discusses a variety of algorithmic systems. I don’t find debates about definitions particularly interesting, including what counts as “AI” or if a particular algorithm qualifies as “intelligent” or not. Please note that the dynamics described in this post hold true both for simpler algorithms, as well as more complex ones.\n\n\nAlgorithms are often implemented without ways to address mistakes.\n\n\nAfter the state of Arkansas implemented software to determine people’s healthcare benefits, many people saw a drastic reduction in the amount of care they received, but were given no explanation and no way to appeal. Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week, transforming her life for the worse. Eventually, a lengthy court case uncovered errors in the software implementation, and Tammy’s hours were restored (along with those of many others who were impacted by the errors).\nObservations of 5th grade teacher Sarah Wysocki’s classroom yielded positive reviews. Her assistant principal wrote, “It is a pleasure to visit a classroom in which the elements of sound teaching, motivated students and a positive learning environment are so effectively combined.” Two months later, she was fired by an opaque algorithm, along with over 200 other teachers. The head of the PTA and a parent of one of Wyscoki’s students described her as “One of the best teachers I’ve ever come in contact with. Every time I saw her, she was attentive to the children, went over their schoolwork, she took time with them and made sure.” That people are losing needed healthcare without an explanation or being fired without explanation is truly dystopian!\n\n\n\nHeadlines from the Verge and the Washington Post\n\n\nAs I covered in a previous post, people use outputs from algorithms differently than they use decisions made by humans: - Algorithms are more likely to be implemented with no appeals process in place. - Algorithms are often used at scale. - Algorithmic systems are cheap. - People are more likely to assume algorithms are objective or error-free. As Peter Haas said, “In AI, we have Milgram’s ultimate authority figure,” referring to Stanley Milgram’s famous experiments showing that most people will obey orders from authority figures, even to the point of harming or killing other humans. How much more likely will people be to trust algorithms perceived as objective and correct?\nThere is a lot of overlap between these factors. If the main motivation for implementing an algorithm is cost-cutting, adding an appeals process (or even diligently checking for errors) may be considered an “unnecessary” expense. Cathy O’Neill, who earned her math PhD at Harvard, wrote a book Weapons of Math Destruction, in which she covers how algorithms are disproportionately impacting poor people, whereas the privileged are more likely to still have access to human attention (in hiring, education, and more).\n\n\nAI makes it easier to not feel responsible.\n\n\nLet’s return to the case of the buggy software used to determine health benefits in Arkansas. How could this have been prevented? In order to prevent severely disabled people from mistakenly losing access to needed healthcare, we need to talk about responsibility. Unfortunately, complex systems lend themselves to a dynamic in which nobody feels responsible for the outcome.\nThe creator of the algorithm for healthcare benefits, Brant Fries (who has been earning royalties off this algorithm, which is in use in over half the 50 states), blamed state policy makers. I’m sure the state policy makers could blame the implementers of the software. When asked if there should be a way to communicate how the algorithm works to the disabled people losing their healthcare, Fries callously said, “It’s probably something we should do. Yeah, I also should probably dust under my bed,” and then later clarified that he thought it was someone else’s responsibility.\nThis passing of the buck and failure to take responsibility is common in many bureaucracies. As danah boyd observed, “Bureaucracy has often been used to shift or evade responsibility. Who do you hold responsible in a complex system?” Boyd gives the examples of high-ranking bureaucrats in Nazi Germany, who did not see themselves as responsible for the Holocaust. boyd continues, “Today’s algorithmic systems are extending bureaucracy.”\nAnother example of nobody feeling responsible comes from the case of research to classify gang crime. A database of gang members assembled by the Los Angeles Police Department (and 3 other California law enforcement agencies) was found to have 42 babies who were under the age of 1 when added to the gang database (28 were said to have admitted to being gang members). Keep in mind these are just some of the most obvious errors- we don’t know how many other people were falsely included.\nI don’t bring this up for the primary purpose of pointing fingers or casting blame. However, a world of complex systems in which nobody feels responsible for the outcomes (which can include severely disabled people losing access to the healthcare they need, or innocent people being labeled as gang members) is not a pleasant place. Our work is almost always a small piece of a larger whole, yet a sense of responsibility is necessary to try to address and prevent negative outcomes.\n\n\nAI encodes & magnifies bias.\n\n\nBut isn’t algorithmic bias just a reflection of how the world is? I get asked a variation of this question every time I give a talk about bias. To which my answer is: No, our algorithms and products impact the world and are part of feedback loops. Consider an algorithm to predict crime and determine where to send police officers: sending more police to a particular neighhorhood is not just an effect, but also a cause. More police officers can lead to more arrests in a given neighborhood, which could cause the algorithm to send even more police to that neighborhood (a mechanism described in this paper on runaway feedback loops).\nBias is being encoded and even magnified in a variety of applications: - software used to decide prison sentences that has twice as high a false positive rate for Black defendents as for white defendents - computer vision software from Amazon, Microsoft, and IBM performs significantly worse on people of color\n[Research by Joy Buolamwini and Timnit Gebru found that commercial computer vision software performed significantly worse on women with dark skin. Gendershades.org]{gendershades3.png){width=60%}\n\nWord embeddings, which are a building block for language tools like Gmail’s SmartReply and Google Translate, generate useful analogies such as Rome:Italy :: Madrid:Spain, as well as biased analogies such as man:computer programmer :: woman: homemaker.\nMachine learning used in recruiting software developed at Amazon penalized applicants who attended all-women’s colleges, as well as any resumes that contained the word “women’s.”\nOver 2/3 of the images in ImageNet, the most studied image data set in the world, are from the Western world (USA, England, Spain, Italy, Australia).\n\n\n\n\nChart from ‘No Classification without Representation’ by Shankar, et. al, shows the origin of ImageNet photos: 45% US, 8% UK, 6% Italy, 3% Canada, 3% Australia, 3% Spain,…\n\n\nSince a Cambrian explosion of machine learning products is occuring, the biases that are calcified now and in the next few years may have a disproportionately huge impact for ages to come (and will be much harder to undo decades from now).\n\n\nOptimizing metrics above all else leads to negative outcomes.\n\n\nWorldwide, people watch 1 billion hours of YouTube per day (yes, that says PER DAY). A large part of YouTube’s successs has been due to its recommendation system, in which a video selected by an algorithm automatically begin playing once the previous video is over. Unfortunately, these recommendations are disproportionately for conspiracy theories promoting white supremacy, climate change denial, and denial of the mass shootings that plague the USA. What is going on? YouTube’s algorithm is trying to maximize how much time people spend watching YouTube, and conspiracy theorists watch significantly more YouTube than people who trust a variety of media sources. Unfortunately, a recommendation system trying only to maximize time spent on its own platform will incentivize content that tells you the rest of the media is lying.\n“YouTube may be one of the most powerful radicalizing instruments of the 21st century,” Professor Zeynep Tufekci wrote in the New York Times. Guillaume Chaslot is a former YouTube engineer turned whistleblower. He has been outspoken about the harms caused by YouTube, and he partnered with the Guardian and the Wall Street Journal to study the extremism and bias in YouTube’s recommendations.\n[Photo of Guillaume Chaslot from the Guardian article]{chaslot.png){width=60%}\nYouTube is owned by Google, which is earning billions of dollars by aggressively introducing vulnerable people to conspiracy theories, while the rest of society bears the externalized costs of rising authoritarian governments, a resurgence in white supremacist movements, failure to act on climate change (even as extreme weather is creating increasing numbers of refugees), growing distrust of mainstream news sources, and a failure to pass sensible gun laws.\nThis problem is an example of the tyranny of metrics: metrics are just a proxy for what you really care about, and unthinkingly optimizing a metric can lead to unexpected, negative results. One analog example is that when the UK began publishing the success rates of surgeons, heart surgeons began turning down risky (but necessary) surgeries to try to keep their scores as high as possible.\nReturning to the account of the popular 5th grade teacher who was fired by an algorithm, she suspects that the underlying reason she was fired was that her incoming students had unusually high test scores the previous year (making it seem like their scores had dropped to a more average level after her teaching), and that their former teachers may have cheated. As USA education policy began over-emphasizing student test scores as the primary way to evaluate teachers, there have been widespread scandals of teachers and principals cheating by altering students scores, in Georgia, Indiana, Massachusetts, Nevada, Virginia, Texas, and elsewhere. When metrics are given undue importance, attempts to game those metrics become common.\n\n\nThere is no accountability for big tech companies.\n\n\nMajor tech companies are the primary ones driving AI advances, and their algorithms impact billions of people. Unfortunately, these companies have zero accountability. YouTube (owned by Google) is helping to radicalize people into white supremacy. Google allowed advertisers to target people who search racist phrases like “black people ruin neighborhoods” and Facebook allowed advertisers to target groups like “jew haters”. Amazon’s facial recognition technology misidentified 28 members of congress as criminals, yet it is already in use by police departments. Palantir’s predictive policing technology was used for 6 years in New Orleans, with city council members not even knowing about the program, much less having any oversight. The newsfeed/timeline/recommendation algorithms of all the major platforms tend to reward incendiary content, prioritizing it for users.\nIn early 2018, the UN ruled that Facebook had played a “determining role” in the ongoing genocide in Myanmar. “I’m afraid that Facebook has now turned into a beast,” said the UN investigator. This result was not a surprise to anyone who had been following the situation in Myanmar. People warned Facebook executives about how the platform was being used to spread dehumanizing hate speech and incite violence against an ethnic minority as early as 2013, and again in 2014 and 2015. As early as 2014, news outlets such as Al Jazeera were covering Facebook’s role in inciting ethnic violence in Myanmar.\nOne person close to the case said, “That’s not 20/20 hindsight. The scale of this problem was significant and it was already apparent.” Facebook execs were warned in 2015 that Facebook could play the same role in Myanmar that radio broadcasts had played during the 1994 Rwandan genocide. As of 2015, Facebook only employed 4 contractors who spoke Burmese (the primary language in Myanmar).\nContrast Facebook’s inaction in Myanmar with their swift action in Germany after the passage of a new law, which could have resulted in penalties of up to 50 million euros. Facebook hired 1,200 German contractors in under a year. In 2018, five years after Facebook was first warned about how they were being used to incite violence in Myanmar, they hired “dozens” of Burmese contractors, a fraction of their response in Germany. The credible threat of a large financial penalty may be the only thing Facebook responds to.\nWhile it can be easy to focus on regulations that are misguided or ineffective, we often take for granted safety standards and regulations that have largely worked well. One major success story comes from automobile safety. Early cars had sharp metal knobs on dashboard that lodged in people’s skulls during crashes, plate glass windows that shattered dangerously, and non-collapsible steering columns that would frequently impale drivers. Beyond that, there was a widespread belief that the only issue with cars was the people driving them, and car manufactures did not want data on car safety to be collected. It took consumer safety advocates decades to push the conversation to how cars could be designed with greater safety, and to pass laws regarding seat belts, driver licenses, crash tests, and the collection of car crash data. For more on this topic, Datasheets for Datasets covers cases studies of how standardization came to the electronics, pharmaceutical, and automobile industries, and 99% Invisible has a deep dive on the history of car safety (with parallels and contrasts to the gun industry).\n\nHow We Can Do Better\n\nThe good news: none of the problems listed here are inherent to algorithms! There are ways we can do better:\n\nMake sure there is a meaningful, human appeals process. Plan for how to catch and address mistakes in advance.\nTake responsibility, even when our work is just one part of the system.\nBe on the lookout for bias. Create datasheets for data sets.\nChoose not to just optimize metrics.\nPush for thoughtful regulations and standards for the tech industry.\n\nThe problems we are facing can feel scary and complex. However, it is still very early on in this age of AI and increasing algorithmic automation. Now is a great time to take action: we can change our culture, cultivate a greater sense of responsibility for our work, seek out thoughtful accountability to counterbalance the inordinate power that major tech companies have, and choose to create more humane products and systems. Technology is just a tool, and it can be used for good or bad. Let’s work to use it for good, to improve the lives of many, rather than just generate wealth for a small number of people.\n\nRelated Posts\n\nYou may be interested in these related posts on tech and ethics:\n\nAI Ethics Resources\nWhat HBR Gets Wrong About Algorithms and Bias\nTech’s long hours are discriminatory & counter-productive\nArtificial Intelligence Needs All of Us (TEDx talk)"
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html",
    "href": "posts/2019-02-12-long-hours/index.html",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "",
    "text": "Whether you realize it or not, you are likely interacting with ill or disabled people regularly. According to recent survey data, a high portion of the U.S. workforce reports having a disability (30 percent), even though a much smaller percentage says they’ve self-identified as disabled to their employer (only 3.2 percent). Often, these illnesses and disabilities are impossible for others to observe, so many people choose to keep their conditions a secret from managers and co-workers to avoid discrimination.\nHealth is not binary; it can fluctuate and is subjective. I have experienced a number of health challenges, including having brain surgery twice (once while pregnant) and one life-threatening brain infection (which can take years to recover from). Trust me when I say that you can’t assess someone’s health based on their appearance or mood. And yet, over one-third of people with disabilities say they have experienced negative bias in their current job.\nI work in the tech industry, where there is an overt glorification — and in many cases, a requirement — of working unhealthily long hours. This is in spite of research showing that putting in longer hours doesn’t lead to greater productivity and instead is harmful. And when you’re ill or disabled and working in this field, the long hours can be not just counterproductive but discriminatory.\n\n\n\nA banner from 1856 reads, “8 hours labour, 8 hours recreation, 8 hours rest.” Source: Wikimedia"
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#fewer-hours-in-the-day",
    "href": "posts/2019-02-12-long-hours/index.html#fewer-hours-in-the-day",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "Fewer hours in the day",
    "text": "Fewer hours in the day\nMany people with chronic illnesses or disabilities simply have fewer hours in the day. We may need more sleep than comparatively healthy people — and yet still wake up feeling awful — as well as have to carefully budget limited energy. Conditions often require frequent doctor visits, blood tests, MRIs, physical therapy, and other appointments, plus there’s dealing with the administrative burden of managing scheduling, billing, and insurance claims, all of which frequently involve errors.\nIn an episode of the podcast No End in Sight, which is focused on chronic illness, a front-end software engineer named John pinpointed his experience feeling time-crunched. John has bipolar disorder and Fabry disease, a rare genetic disorder that causes reduced kidney function and chronic pain and requires him to get regular IV infusion treatments. He described being told during a job interview at Microsoft that he needed to spend more of his free time coding:\n\nI really felt looked down on as being lazy. And really, I’m not lazy. I have chronic illness, and I’m trying to do the best — like, I’m not trying to push myself too hard because I don’t want to throw myself into a bipolar tailspin. And I also don’t want to hurt my hands and have it be even worse to type… I was told by this abled person how to go about living assuming that I was abled, and it was just really frustrating. I’ve contributed at least a thousand hours to open source, and I’m supposed to just keep doing more. When does it end?\n\nNatasha Walton, who founded the Tech Disability Project, has fibromyalgia and post-traumatic stress disorder. She noted on Twitter that certain aspects of the day, like sleep and fitness routines, are not optional for her. “They account for the time I spend meeting my body’s basic needs each and every day so that I can participate in the wider world,” she explained.\nThe tech work environment is hostile even to healthy people. The “ideal worker” in tech is in perfect health, child-free, and has no other commitments. I’ve had several jobs in tech that I could do for a time, and even do quite well, but that I knew would be unsustainable for me long-term. The question was not if I would burn out, but when. Numerous co-workers have also seemed on the brink of burnout regardless of whether they had a chronic illness. I even have tech-industry friends who developed permanent chronic illnesses while in toxic work environments.\nThere are companies where people like me would not be welcome based on unreasonable employee demands. Last year, Andrew Ng’s deeplearning.ai posted a controversial job ad that not only specified that employees typically spend 70–90 hours per week working and studying (later changed to 70+ hours), but that doing so is the natural consequence of believing you can change the world. Many companies operate on this assumption, even if most are not quite so frank about it.\nElon Musk posted a declaration that to change the world, people need to work 80 hours per week, peaking above 100 at times. Uber formerly had an explicit company value to “work harder, longer, and smarter” and served dinner at 8:15 p.m. “Working seven days a week, sometimes until 1 or 2 a.m., was considered normal,” said one former employee. A New York Times article about Amazon described “marathon conference calls on Easter Sunday and Thanksgiving, criticism from bosses for spotty Internet access on vacation, and hours spent working at home most nights or weekends,” as well as employees being given low-performance ratings directly after cancer treatment, major surgeries, or giving birth to a stillborn child."
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#the-research-on-productivity",
    "href": "posts/2019-02-12-long-hours/index.html#the-research-on-productivity",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "The research on productivity",
    "text": "The research on productivity\nAs much as possible, we need to get away from the shallow idea that the quantity of time worked is what matters. The tech industry’s obsession with ridiculously long hours is not only inaccessible to many disabled people and harmful to everyone’s health and relationships, but as Olivia Goldhill pointed out for Quartz at Work, research on productivity suggests it’s just inefficient:\n\nAs countless studies have shown, this simply isn’t true. Productivity dramatically decreases with longer work hours, and completely drops off once people reach 55 hours of work a week, to the point that, on average, someone working 70 hours in a week achieves no more than a colleague working 15 fewer hours.\n\nAlex Soojung-Kim Pang’s book Rest covers the crucial role that leisure time and downtime play in our creativity, health, and productivity. Prolific, talented figures including Charles Darwin, Henri Poincaré, G.H. Hardy, mathematician Paul Halmos, Charles Dickens, and many others were known to engage in only four or five hours of highly concentrated work per day. Pang also highlights an overlooked aspect of the “rule” popularized by Malcolm Gladwell that to become an expert takes 10,000 hours of practice. Gladwell based it on psychologist K. Anders Ericsson’s study of top musical performers, but Pang observes that the top performers also slept more and took afternoon naps:\n\nWe’ve come to believe that world-class performance comes after 10,000 hours of practice. But that’s wrong. It comes after 10,000 hours of deliberate practice, 12,500 hours of deliberate rest, and 30,000 hours of sleep.\n\nMore support for rest-boosted productivity is detailed in a Harvard Business Review roundup titled “The Research Is Clear: Long Hours Backfire for People and for Companies.” It highlights a variety of other study results:\n\nManagers could not tell the difference between employees who worked 80-hour weeks and those who pretended to — though they still penalized employees who were open about working less.\nOverwork is linked to impaired sleep, and sleep deprivation has long been known to lengthen reaction time, interfere with problem-solving, and even induce an impairment equivalent to being drunk.\nDepression, heavy drinking, diabetes, memory problems, heart disease, and poorer judgment calls are all repercussions tied to being overworked.\nPredictable, required time off (like nights and weekends) make teams more productive.\n\n\n\n\nRoman timekeeping: Four clocks show the amount of night and day at times of the year. Image from Wikimedia"
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#the-importance-of-flexible-work-environments",
    "href": "posts/2019-02-12-long-hours/index.html#the-importance-of-flexible-work-environments",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "The importance of flexible work environments",
    "text": "The importance of flexible work environments\nAccommodations, even simple ones, can mean a world of difference to employees with illnesses or disabilities. Brianne Benness, founder of the No End in Sight podcast mentioned above, has written about how a flexible job with remote work helped her stay employed during her at-the-time undiagnosed illness: “When I woke up in a lot of pain, I could tell my boss I was working from home… When the pain in my neck made it too distracting to sit at my desk, I could move to a couch and lie down with my head supported.”\nBut when Brianne switched to a job with a more rigid in-office schedule, both her health and work level plummeted. With nowhere to lie down with her laptop, she would not only get distracted by the pain but put her focus on trying to seem productive. Meanwhile, she lost energy fast during the day, struggled with brain fog, and would go to bed as soon as she got home. She nailed the conundrum in explaining, “I know that when my brain is firing on all cylinders, I can get more done in five hours than I can get done in a full week when my brain is plodding. But I don’t know how to share that value with an employer.”\n\nThe tech industry problems of exclusion, discrimination, and unhealthy work habits run deep, but there is also a widespread appetite for change.\n\nSome business leaders and employers are recognizing the value on their own. A Harvard Business Review article details an experiment by Stanford professor Nicholas Bloom and Ctrip travel website cofounder James Liang in which they let half of Ctrip’s employees work from home for nine months. They found that the group working from home was both more productive and only half as likely to quit as other employees. Bloom said he was blown away by the results, and the benefits of flexible work were much greater than he expected.\nMassive employers like PricewaterhouseCoopers are experimenting, too. PwC is the second largest professional services firm in the world, and last year, it announced a new flexible work program in which potential employees can choose how many hours per week or how many months per year they are available to work. It seems to have created a valuable appeal in recruiting. When one of the company’s leaders, Anne Donovan, shared her advice on switching to a more flexible culture, she asserted that everyone deserves the same degree of flexibility and that culture comes from the top."
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#we-have-a-long-way-to-go",
    "href": "posts/2019-02-12-long-hours/index.html#we-have-a-long-way-to-go",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "We have a long way to go",
    "text": "We have a long way to go\nIn her keynote at GopherCon, Google developer advocate Julia Ferraioli pointed out that, in tech, we often make products accessible but not the processes or the teams we use to build them. True inclusion means having disabled people on your team; the people creating the technology need to be representative of the people using technology, which, increasingly, is everyone.\nIn previous posts, I have shared extensive research on how gender and racial bias manifest in the tech industry, including in retention, promotions, onboarding, and hiring. But in researching bias around disability in the field, I found far less, and this is in part because tech companies aren’t tracking it. When a TechCrunch reporter, who has a severe disability, asked Intel, Apple, Twitter, Facebook, Slack, Google, and Salesforce why none of them included disability in their diversity reports, the companies gave evasive, off-the-record responses. Since the zeroth step to increasing inclusion is to understand the scope and details of the problem, this is an indicator we have a long way to go.\nThe ideas are out there, though. Ted Kennedy Jr., an attorney and state senator who lost his leg as a child due to cancer, recently wrote about common, straightforward themes among companies that are inclusive of people with disabilities:\n\nThey hire people with disabilities.\nThey encourage and advance those employees.\nThey provide accessible tools and technology and have a formal accommodations program.\nThey empower those employees with mentoring and coaching initiatives (Note: Not all mentorship is the same. Research has shown that public endorsement of a mentee’s authority and championing their ideas is far more effective than advice on how the mentee should change and gain self-knowledge.)\n\nI certainly can’t and don’t speak for everyone with chronic illnesses or disabilities, and I encourage you to listen and read the accounts of others. The tech industry problems of exclusion, discrimination, and unhealthy work habits run deep, but there is also a widespread appetite for change. Reconsider the culture at your workplace by hiring and promoting people with disabilities, de-emphasizing hours spent working in favor of quality of work, and allowing a more flexible setup.\nThis post was originally published Feb 12, 2019 on Medium. This is a follow-up to an_ earlier post I wrote on how the tech industry is failing people with disabilities and chronic illnesses.\nThank you to Julia Ferraioli, Jeremy Howard, and Negar Rostamzadeh for giving me valuable feedback on earlier drafts of this post."
  },
  {
    "objectID": "posts/2019-08-07-surveillance/index.html",
    "href": "posts/2019-08-07-surveillance/index.html",
    "title": "8 Things You Need to Know about Surveillance",
    "section": "",
    "text": "Over 225 police departments have partnered with Amazon to have access to Amazon’s video footage obtained as part of the “smart” doorbell product Ring, and in many cases these partnerships are heavily subsidized with taxpayer money. Police departments are allowing Amazon to stream 911 call information directly in real-time, and Amazon requires police departments to read pre-approved scripts when talking about the program. If a homeowner doesn’t want to share data from their video camera doorbell with police, an officer for the Fresno County Sheriff’s Office said they can just go directly to Amazon to obtain it. This creation of an extensive surveillance network, the murky private-public partnership surrounding it, and a lack of any sort of regulations or oversight is frightening. And this is just one of many examples related to surveillance technology that have recently come to light.\nI frequently talk with people who are not that concerned about surveillance, or who feel that the positives outweigh the risks. Here, I want to share some important truths about surveillance:\n\n\nSurveillance can facilitate human rights abuses and even genocide\n\n\nData is often used for different purposes than why it was collected\n\n\nData often contains errors\n\n\nSurveillance typically operates with no accountability\n\n\nSurveillance changes our behavior\n\n\nSurveillance disproportionately impacts the marginalized\n\n\nData privacy is a public good\n\n\nWe don’t have to accept invasive surveillance\n\n\nWhile I was writing this post, a number of investigative articles came out with disturbing new developments related to surveillance. I decided that rather than attempt to include everything in one post (which would make it too long and too dense), I would go ahead and share the above facts about surveillance, as they are just a relevant as ever.\n\n\n\nThe last 24 hours:- NYC police using facial recognition on 11 year old kids- Cops are giving Amazon real-time 911 caller data- California Facial Recognition Interconnect- contd facial rec on protesters in Hong Kong- Palantir founder Peter Thiel gets op-ed in NYTimes\n\n— Rachel Thomas (@math_rachel) August 2, 2019\n\n\n\n\n\nSurveillance can facilitate human rights abuses and even genocide\n\n\nThere is a long history of data about sensitive attributes being misused, including the use of the 1940 USA Census to intern Japanese Americans, a system of identity cards introduced by the Belgian colonial government that were later used during the 1994 Rwandan genocide (in which nearly a million people were murdered), and the role of IBM in helping Nazi Germany use punchcard computers to identify and track the mass killing of millions of Jewish people. More recently, the mass internment of over one million people who are part of an ethnic minority in Western China was facilitated through the use of a surveillance network of cameras, biometric data (including images of people’s faces, audio of their voices, and blood samples), and phone monitoring.\n\n\n\nAdolf Hitler meeting with IBM CEO Tom Watson Sr. in 1937. Source: https://www.computerhistory.org/revolution/punched-cards/2/15/109\n\n\nPictured above is Adolf Hitler (far left) meeting with IBM CEO Tom Watson Sr. (2nd from left), shortly before Hitler awarded Watson a special “Service to the Reich” medal in 1937 (for a timeline of the Holocaust, see here). Watson returned the medal in 1940, although IBM continued to do business with the Nazis. IBM technology helped the Nazis conduct detailed censuses in countries they occupied, to thoroughly identify anyone of Jewish descent. Nazi concentration camps used IBM’s punchcard machines to tabulate prisoners, recording whether they were Jewish, gay, or Gypsies, and whether they died of “natural causes,” execution, suicide, or via “special treatment” in gas chambers. It is not the case that IBM sold the machines and then was done with it. Rather, IBM and its subsidiaries provided regular training and maintenance on-site at the concentration camps: printing off cards, configuring machines, and repairing them as they broke frequently.\n\n\nData is often used for different purposes than why it was collected\n\n\nIn the above examples, the data collection began before genocide was committed. IBM began selling to Nazi Germany well before the Holocaust (although continued for far too long), including helping with Germany’s 1933 census conducted by Adolf Hitler, which was effective at identifying far more Jewish people than had previously been recognized in Germany.\nIt is important to recognize how data and images gathered through surveillance can be weaponized later. Columbia professor Tim Wu wrote that “One [hard truth] is that data and surveillance networks created for one purpose can and will be used for others. You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.”\nPlenty of data collection is not involved with such extreme abuse as genocide; however, in a time of global resurgence of white supremacist, ethno-nationalist, and authoritarian movements, it would be deeply irresponsible to not consider how data & surveillance can and will be weaponized against already vulnerable groups.\n\n\nData often has errors (and no mechanism for correcting them)\n\n\nA database of suspected gang members maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). Even worse, there was no process in place for correcting mistakes or removing people once they’ve been added.\nAn NPR reporter recounts his experience of trying to rent an apartment and discovering that TransUnion, one of the 3 major credit bureaus, incorrectly reported him as having two felony firearms convictions. TransUnion only removed the mistakes after a dozen phone calls and notification that the story would be reported on. This is not an unusual story: the FTC’s large-scale study of credit reports in 2012 found 26% of consumers had at least one mistake in their files and 5% had errors that could be devastating. An even more opaque, unregulated “4th bureau” exists: a collection of companies buying and selling personal information about people on the margins of the banking system (such as immigrants, students, and people with low incomes), with no standards on what types of data are included, no way to opt out, and no system for identifying or correcting mistakes.\n\n\nSurveillance typically operates with no accountability\n\n\nWhat makes the examples in the previous section disturbing is not just that errors occurred, but that there was no way to identify or correct them, and no accountability for those profiting off the error-laden data. Often, even the existence of the systems being used is not publicly known (much less details of how these systems work), unless discovered by journalists or revealed by whistleblowers. The Detroit Police Dept used facial recognition technology for nearly two years without public input and in violation of a requirement that a policy be approved by the city’s Board of Police Commissioners, until a study from Georgetown Law’s Center for Privacy & Technology drew attention to the issue. Palantir, the defense startup founded by billionaire Peter Thiel, ran a program with New Orleans Police Department for 6 years which city council members did not even know about, much less have any oversight.\nAfter two studies found that Amazon’s facial recognition software produced inaccurate and racially biased results, Amazon countered that the researchers should have changed the default parameters. However, it turned out that Amazon was not instructing police departments that use its software to do this either. Surveillance programs are operating with few regulations, no oversight, no accountability around accuracy or mistakes, and in many cases, no public knowledge of what is going on.\n\n\nSurveillance changes our behavior\n\n\nHundreds of thousands of people in Hong Kong are protesting an unpopular new bill which would allow extradition to China. Typically, Hong Kong locals use their rechargeable smart cards to ride the subway. However, during the protests, long lines of people waited to use cash to buy paper tickets (usually something that only tourists do) concerned that they would be tracked for having attended the protests. Would fewer people protest if this was not an option?\nIn the United States, in 2015 the Baltimore Police Department used facial recognition technology to surveil people protesting the death of Freddie Grey, a young Black man who was killed in police custody, and arrested protesters with outstanding warrants. Mass surveillance could have a chilling impact on our rights to move about freely, to express ourselves, and to protest. “We act differently when we know we are ‘on the record.’ Mass privacy is the freedom to act without being watched and thus in a sense, to be who we really are,” Columbia professor Tim Wu wrote in the New York Times.\n\n\n\nFlyer from the company Geofeedia. Source: https://www.aclunc.org/docs/20161011_geofeedia_baltimore_case_study.pdf\n\n\n\n\nSurveillance disproportionately impacts those who are already marginalized\n\n\nSurveillance is applied unevenly, causing the greatest harm to people who are already marginalized, including immigrants, people of color, and people living in poverty. These groups are more heavily policed and surveilled. The Perpetual Line-Up from the Georgetown Law Center on Privacy and Technology studied the unregulated use of facial recognition by police, with half of all Americans appearing in law enforcement databases, and the risks of errors, racial bias, misuses, and threats to civil liberties. The researchers pointed out that African Americans are more likely to appear in these databases (many of which are drawn from mug shots) since they are disproportionately likely to be stopped, interrogated, or arrested. For another example, consider the contrast between how easily people over 65 can apply for Medicare benefits by filling out an online form, with the invasive personal questions asked of a low-income mother on Medicaid about her lovers, hygiene, parental shortcomings, and personal habits.\nIn an article titled Trading privacy for survival is another tax on the poor, Ciara Byrne wrote, “Current public benefits programs ask applicants extremely detailed and personal questions and sometimes mandate home visits, drug tests, fingerprinting, and collection of biometric information… Employers of low-income workers listen to phone calls, conduct drug tests, monitor closed-circuit television, and require psychometric tests as conditions of employment. Prisoners in some states have to consent to be voiceprinted in order to make phone calls.”\n\n\nData privacy is a public good, like air quality or safe drinking water\n\n\nData is more revealing in aggregate. It can be nearly impossible to know what your individual data could reveal when combined with the data of others or with data from other sources, or when machine learning inference is performed on it. For instance, as Zeynep Tufekci wrote in the New York Times, individual Strava users could not have predicted how in aggregate their data could be used to identify the locations of US military bases. “Data privacy is not like a consumer good, where you click ‘I accept’ and all is well. Data privacy is more like air quality or safe drinking water, a public good that cannot be effectively regulated by trusting in the wisdom of millions of individual choices. A more collective response is needed.”\nUnfortunately, this also means that you can’t fully safeguard your privacy on your own. You may choose not to purchase Amazon’s ring doorbell, yet you can still show up in the video footage collected by others. You might strengthen your online privacy practices, yet conclusions will still be inferred about you based on the behavior of others. As Professor Tufekci wrote, we need a collective response.\n\n\nWe don’t have to accept invasive surveillance\n\n\nMany people are uncomfortable with surveillance, but feel like they have no say in the matter. While the threats surveillance poses are large, it is not too late to act. We are seeing success: in response to community organizing and an audit, Los Angeles Police Department scrapped a controversial program to predict who is most likely to commit violent crimes. Citizens, researchers, and activists in Detroit have been effective at drawing attention to the Detroit Police Department’s unregulated use of facial recognition and a bill calling for a 5-year moratorium has been introduced to the state legislature. Local governments in San Francisco, Oakland, and Somerville have banned the use of facial recognition by police.\nFor further resources, please check out: - Georgetown Law Center on Privacy and Technology - Digital Defense Playbook"
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html",
    "href": "posts/2022-03-15-math-person/index.html",
    "title": "There’s no such thing as not a math person",
    "section": "",
    "text": "On the surface, I may seem into math: I have a math PhD, taught a graduate computational linear algebra course, co-founded AI research lab fast.ai, and even go by the twitter handle @math_rachel.\nYet many of my experiences of academic math culture have been toxic, sexist, and deeply alienating. At my lowest points, I felt like there was no place for me in math academia or math-heavy tech culture.\nIt is not just mathematicians or math majors who are impacted by this: Western culture is awash in negative feelings and experiences regarding math, which permate from many sources and impact students of all ages. In this post, I will explore the cultural factors, misconceptions, stereotypes, and relevant studies on obstacles that turn people off to math. If you (or your child) doesn’t like math or feels anxious about your own capabilities, you’re not alone, and this isn’t just a personal challenge. The below essay is based on part of a talk I recently gave."
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#myth-of-innate-ability-myth-of-the-lone-genius",
    "href": "posts/2022-03-15-math-person/index.html#myth-of-innate-ability-myth-of-the-lone-genius",
    "title": "There’s no such thing as not a math person",
    "section": "Myth of Innate Ability, Myth of the Lone Genius",
    "text": "Myth of Innate Ability, Myth of the Lone Genius\nOne common myth is the idea that certain people’s brains aren’t “wired” the right way to do math, tech, or AI, that your brain either “works that way” or not. None of the evidence supports this viewpoint, yet when people believe this, it can become a self-fulfilling prophecy. Dr. Omoju Miller, who earned her PhD at UC Berkeley and was a senior machine learning engineer and technical advisor to the CEO at Github, shares some of the research debunking the myth of innate ability in this essay and in her TEDx talk. In reality, there is no such thing as “not a math person.”\nDr. Cathy O’Neil, a Harvard Math PhD and author of Weapons of Math Destruction, wrote about the myth of the lone genius mathematician, “You don’t have to be a genius to become a mathematician. If you find this statement at all surprising, you’re an example of what’s wrong with the way our society identifies, encourages and rewards talent… For each certified genius, there are at least a hundred great people who helped achieve such outstanding results.”\n\n\n\nDr. Miller debunking the myth of innate ability, and Dr. O’Neil debunking the myth of the lone genius mathematician"
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#music-without-singing-or-instruments",
    "href": "posts/2022-03-15-math-person/index.html#music-without-singing-or-instruments",
    "title": "There’s no such thing as not a math person",
    "section": "Music without singing or instruments",
    "text": "Music without singing or instruments\nImagine a world where children are not allowed to sing songs or play instruments until they reach adulthood, after spending a decade or two transcribing sheet music by hand. This scenario is absurd and nightmarish, yet it is analogous to how math is often taught, with the most creative and interesting parts saved until almost everyone has dropped out. Dr. Paul Lockhart eloquently describes this metaphor in his essay, A Mathematician’s Lament, on “how school cheats us out of our most fascinating and imaginative art form.” Dr. Lockhart left his role as a university math professor to teach K-12 math, as he felt that so much reform was needed in how math is taught.\nDr. David Perkins uses the analogy of how children can play baseball wthout knowing all the technical details, without having a full team or playing a full 9 innings, yet still gain a sense of the “whole game.” Math is usually taught with an overemphasis on dry, technical details, without giving students a concept of the “whole game.” It can take years and years before enough technical details are accumulated to build something interesting. There is an overemphasis on techniques rather than meaning.\n\n\n\nWhat if math was taught more like how music or sports are taught?\n\n\nMath curriculums are usually arranged in a vertical manner, with each year building tightly on the previous, such that one bad year can ruin everything that comes after. Many people I talk to can pinpoint the year that math went bad for them: “I used to like math until 6th grade, when I had a bad teacher/was dealing with peer pressure/my undiagnosed ADHD was out of control. After that, I was never able to succeed in future years.” This is less true in other subjects, where one bad history teacher/one bad year doesn’t mean that you can’t succeed at history the following year."
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#gender-race-and-stereotypes",
    "href": "posts/2022-03-15-math-person/index.html#gender-race-and-stereotypes",
    "title": "There’s no such thing as not a math person",
    "section": "Gender, race, and stereotypes",
    "text": "Gender, race, and stereotypes\nFemale teachers’ math anxiety affects girls’ math achievement: In the USA, over 90% of primary school teachers are female, and research has found “the more anxious teachers were about math, the more likely girls (but not boys) were to endorse the commonly held stereotype that ‘boys are good at math, and girls are good at reading’ and the lower these girls’ math achievement… People’s fear and anxiety about doing math—over and above actual math ability—can be an impediment to their math achievement.”\nResearch across a number of universities has found that more women go into engineering when courses focus on problems with positive social impact.\nStructural racism also impacts what messages teachers impart to students. An Atlantic article How Does Race Affect a Student’s Math Education? covered the research paper A Framework for Understanding Whiteness in Mathematics Education, noting that “Constantly reading and hearing about underperforming Black, Latino, and Indigenous students begins to embed itself into how math teachers view these students, attributing achievement differences to their innate ability to succeed in math… teachers start to expect worse performance from certain students, start to teach lower content, and start to use lower-level math instructional practices. By contrast, white and Asian students are given the benefit of the doubt and automatically afforded the opportunity to do more sophisticated and substantive mathematics.”"
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#the-mathematics-community-is-an-absolute-mess-which-actively-pushes-out-the-sort-of-people-who-might-make-it-better",
    "href": "posts/2022-03-15-math-person/index.html#the-mathematics-community-is-an-absolute-mess-which-actively-pushes-out-the-sort-of-people-who-might-make-it-better",
    "title": "There’s no such thing as not a math person",
    "section": "The mathematics community is “an absolute mess which actively pushes out the sort of people who might make it better”",
    "text": "The mathematics community is “an absolute mess which actively pushes out the sort of people who might make it better”\n\n\n\nDr. Harron’s website, and some of the coverage of her number theory thesis, including on the Scientific American blog\n\n\nDr. Piper Harron made waves with her Princeton PhD thesis, utilizing humor, analogies, sarcasm, and genuine efforts to be accessible as she described advanced concepts in a ground-breaking way, very atypical for a mathematics PhD thesis. Dr. Harron wrote openly in the prologue of her thesis on how alienating the culture of mathematics is, “As any good grad student would do, I tried to fit in, mathematically. I absorbed the atmosphere and took attitudes to heart. I was miserable, and on the verge of failure. The problem was not individuals, but a system of self-preservation that, from the outside, feels like a long string of betrayals, some big, some small, perpetrated by your only support system.” At her blog, the Liberated Mathematician, she writes, “My view of mathematics is that it is an absolute mess which actively pushes out the sort of people who might make it better.”\nThese descriptions resonate with my own experiences obtaining a math PhD (as well as the experiences of many friends, at a variety of universities). The toxicity of academic math departments is self-perpetuating, pushing out the people who could make them better."
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#the-full-talk",
    "href": "posts/2022-03-15-math-person/index.html#the-full-talk",
    "title": "There’s no such thing as not a math person",
    "section": "The full talk",
    "text": "The full talk\nThis post is based on the first part of the talk I gave in the below video, which includes more detail and a Q&A. The talk also includes recommendations about math apps and resources, as well as a framework for how to consider screentime. Stay tuned for a future fast.ai blog post covering math apps and screentime."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html",
    "href": "posts/2022-05-17-societal-harms/index.html",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "",
    "text": "When the USA government switched to facial identification service ID.me for unemployment benefits, the software failed to recognize Bill Baine’s face. While the app said that he could have a virtual appointment to be verified instead, he was unable to get through. The screen had a wait time of 2 hours and 47 minutes that never updated, even over the course of weeks. He tried calling various offices, his daughter drove in from out of town to spend a day helping him, and yet he was never able to get a useful human answer on what he was supposed to do, as he went for months without unemployment benefits. In Baine’s case, it was eventually resolved when a journalist hypothesized that the issue was a spotty internet connection, and that Baine would be better off traveling to another town to use a public library computer and internet. Even then, it still took hours for Baine to get his approval.\n\n\n\nJournalist Andrew Kenney of Colorado Public Radio has covered the issues with ID.me\n\n\nBaine was not alone. The number of people receiving unemployment benefits plummeted by 40% in the 3 weeks after ID.me was introduced. Some of these were presumed to be fraudsters, but it is unclear how many genuine people in need of benefits were wrongly harmed by this. These are individual harms, but there are broader, societal harms as well: the cumulative costs of the public having to spend ever more time on hold, trying to navigate user-hostile automated bureaucracies where they can’t get the answers they need. There is the societal cost of greater inequality and greater desperation, as more people are plunged into poverty through erroneous denial of benefits. And there is the undermining of trust in public services, which can be difficult to restore.\nPotential for algorithmic harm takes many forms: loss of opportunity (employment or housing discrimination), economic cost (credit discrimination, narrowed choices), social detriment (stereotype confirmation, dignitary harms), and loss of liberty (increased surveillance, disproportionate incarceration). And each of these four categories manifests in both individual and societal harms.\nIt should come as no surprise that algorithmic systems can give rise to societal harm. These systems are sociotechnical: they are designed by humans and teams that bring their values to the design process, and algorithmic systems continually draw information from, and inevitably bear the marks of, fundamentally unequal, unjust societies. In the context of COVID-19, for example, policy experts warned that historical healthcare inequities risked making their way into the datasets and models being used to predict and respond to the pandemic. And while it’s intuitively appealing to think of large-scale systems as creating the greatest risk of societal harm, algorithmic systems can create societal harm because of the dynamics set off by their interconnection with other systems/ players, like advertisers, or commercially-driven media, and the ways in which they touch on sectors or spaces of public importance.\nStill, in the west, our ideas of harm are often anchored to an individual being harmed by a particular action at a discrete moment in time. As law scholar Natalie Smuha has powerfully argued, legislation (both proposed and passed) in Western countries to address algorithmic risks and harms often focuses on individual rights: regarding how an individual’s data is collected or stored, to not be discriminated against, or to know when AI is being used. Even metrics used to evaluate the fairness of algorithms are often aggregating across individual impacts, but unable to capture longer-term, more complex, or second- and third-order societal impacts."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#case-study-privacy-and-surveillance",
    "href": "posts/2022-05-17-societal-harms/index.html#case-study-privacy-and-surveillance",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Case Study: Privacy and surveillance",
    "text": "Case Study: Privacy and surveillance\nConsider the over-reliance on individual harms in discussing privacy: so often focused on whether individual users have the ability to opt in or out of sharing their data, notions of individual consent, or proposals that individuals be paid for their personal data. Yet widespread surveillance fundamentally changes society: people may begin to self-censor and to be less willing (or able) to advocate for justice or social change. Professor Alvaro Bedoya, director of the Center on Privacy and Technology at the Georgetown University Law Center, traces a history of how surveillance has been used by the state to try to shut down movements for progress– targeting religious minorities, poor people, people of color, immigrants, sex workers and those considered “other”. As Maciej Ceglowski writes, “Ambient privacy is not a property of people, or of their data, but of the world around us… Because our laws frame privacy as an individual right, we don’t have a mechanism for deciding whether we want to live in a surveillance society.”\nDrawing on interviews with African data experts, Birhane et al write that even when data is anonymized and aggregated, it “can reveal information on the community as a whole. While notions of privacy often focus on the individual, there is growing awareness that collective identity is also important within many African communities, and that sharing aggregate information about communities can also be regarded as a privacy violation.” Recent US-based scholarship has also highlighted the importance of thinking about group level privacy (whether that group is made up of individuals who identify as members of that group, or whether it’s a ‘group’ that is algorithmically determined - like individuals with similar shopping habits on Amazon). Because even aggregated anonymised data can reveal important group-level information (e.g., the location of military personnel training via exercise tracking apps) “managing privacy”, these authors argue “is often not intrapersonal but interpersonal.” And yet legal and tech design privacy solutions are often better geared towards assuring individual-level privacy than negotiating group privacy."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#case-study-disinformation-and-erosion-of-trust",
    "href": "posts/2022-05-17-societal-harms/index.html#case-study-disinformation-and-erosion-of-trust",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Case Study: Disinformation and erosion of trust",
    "text": "Case Study: Disinformation and erosion of trust\nAnother example of a collective societal harm comes from how technology platforms such as Facebook have played a significant role in elections ranging from the Philippines to Brazil, yet it can be difficult (and not necessarily possible or useful) to quantify exactly how much: something as complex as a country’s political system and participation involves many interlinking factors. But when ‘deep fakes’ make it “possible to create audio and video of real people saying and doing things they never said or did” or when motivated actors successfully game search engines to amplify disinformation, the (potential) harm that is generated is societal, not just individual. Disinformation and the undermining of trust in institutions and fellow citizens have broad impacts, including on individuals who never use social media.\n\n\n\nReports and Events on Regulatory Approaches to Disinformation\n\n\nEfforts by national governments to deal with the problem through regulation have not gone down well with everyone. ‘Disinformation’ has repeatedly been highlighted as one of the tech-enabled ‘societal harms’ that the UK’s Online Safety Bill or the EU’s Digital Services Act should address, and a range of governments are taking aim at the problem by proposing or passing a slew of (in certain cases ill-advised) ‘anti-misinformation’ laws. But there’s widespread unease around handing power to governments to set standards for what counts as ‘disinformation’. Does reifying ‘disinformation’ as a societal harm become a legitimizing tool for governments looking to silence political dissent or undermine their weaker opponents? It’s a fair and important concern - and yet simply leaving that power in the hands of mostly US-based, unaccountable tech companies is hardly a solution. What are the legitimacy implications if a US company like Twitter were to ban democratically elected Brazilian President Jair Bolsonaro for spreading disinformation, for example? How do we ensure that tech companies are investing sufficiently in governance efforts across the globe, rather than responding in an ad hoc manner to proximal (i.e. mostly US-based) concerns about disinformation? Taking a hands off approach to platform regulation doesn’t make platforms’ efforts to deal with disinformation any less politically fraught."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#individual-harms-individual-solutions",
    "href": "posts/2022-05-17-societal-harms/index.html#individual-harms-individual-solutions",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Individual Harms, Individual Solutions",
    "text": "Individual Harms, Individual Solutions\nIf we consider individual solutions our only option (in terms of policy, law, or behavior), we often limit the scope of the harms we can recognize or the nature of the problems we face. To take an example not related to AI: Oxford professor Trish Greenhalgh et al analyzed the slow reluctance of leaders in the West to accept that covid is airborne (e.g. it can linger and float in the air, similar to cigarette smoke, requiring masks and ventilation to address), rather than droplet dogma (e.g. washing your hands is a key precaution). One reason they highlight is the Western framing of individual responsibility as the solution to most problems. Hand-washing is a solution that fits the idea of individual responsibility, whereas collective responsibility for the quality of shared indoor air does not. The allowable set of solutions helps shape what we identify as a problem. Additionally, the fact that recent research suggests that “the level of interpersonal trust in a society” was a strong predictor of which countries managed COVID-19 most successfully should give us pause. Individualistic framings can limit our imagination about the problems we face and which solutions are likely to be most impactful."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#parallels-with-environmental-harms",
    "href": "posts/2022-05-17-societal-harms/index.html#parallels-with-environmental-harms",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Parallels with Environmental Harms",
    "text": "Parallels with Environmental Harms\nBefore the passage of environmental laws, many existing legal frameworks were not well-suited to address environmental harms. Perhaps a chemical plant releases waste emissions into the air once per week. Many people in surrounding areas may not be aware that they are breathing polluted air, or may not be able to directly link air pollution to a new medical condition, such as asthma, (which could be related to a variety of environmental and genetic factors).\n\n\n\nThere are parallels between air polllution and algorithmic harms\n\n\nThere are many parallels between environmental issues and AI ethics. Environmental harms include individual harms for people who develop discrete health issues from drinking contaminated water or breathing polluted air. Yet, environmental harms are also societal: the societal costs of contaminated water and polluted air can reverberate in subtle, surprising, and far-reaching ways. As law professor Nathalie Smuha writes, environmental harms are often accumulative and build over time. Perhaps each individual release of waste chemicals from a refinery has little impact on its own, but adds up to be significant. In the EU, environmental law allows for mechanisms to show societal harm, as it would be difficult to challenge many environmental harms on the basis of individual rights. Smuha argues that there are many similarities with AI ethics: for opaque AI systems, spanning over time, it can be difficult to prove a direct causal relationship to societal harm."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#directions-forward",
    "href": "posts/2022-05-17-societal-harms/index.html#directions-forward",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Directions Forward",
    "text": "Directions Forward\nTo a large extent our message is to tech companies and policymakers. It’s not enough to focus on the potential individual harms generated by tech and AI: the broader societal costs of tech and AI matter.\nBut those of us outside tech policy circles have a crucial role to play. One way in which we can guard against the risks of the ‘societal harm’ discourse being co-opted by those with political power to legitimise undue interference and further entrench their power is by claiming the language of ‘societal harm’ as the democratic and democratising tool it can be. We all lose when we pretend societal harms don’t exist, or when we acknowledge they exist but throw our hands up. And those with the least power, like Bill Baine, are likely to suffer a disproportionate loss.\nIn his newsletter on Tech and Society, L.M. Sacasas encourages people to ask themselves 41 questions before using a particular technology. They’re all worth reading and thinking about - but we’re listing a few especially relevant ones to get you started. Next time you sit down to log onto social media, order food online, swipe right on a dating app or consider buying a VR headset, ask yourself:\n\nHow does this technology empower me? At whose expense? (Q16)\nWhat feelings does the use of this technology generate in me toward others? (Q17)\nWhat limits does my use of this technology impose upon others? (Q28)\nWhat would the world be like if everyone used this technology exactly as I use it? (Q37)\nDoes my use of this technology make it easier to live as if I had no responsibilities toward my neighbor? (Q40)\nCan I be held responsible for the actions which this technology empowers? Would I feel better if I couldn’t? (Q41)\n\nIt’s on all of us to sensitise ourselves to the societal implications of the tech we use."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html",
    "href": "posts/2022-06-01-qualitative/index.html",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "",
    "text": "“All research is qualitative; some is also quantitative” Harvard Social Scientist and Statistician Gary King\nSuppose you wanted to find out whether a machine learning system being adopted - to recruit candidates, lend money, or predict future criminality - exhibited racial bias. You might calculate model performance across groups with different races. But how was race categorised– through a census record, a police officer’s guess, or by an annotator? Each possible answer raises another set of questions. Following the thread of any seemingly quantitative issue around AI ethics quickly leads to a host of qualitative questions. Throughout AI, qualitative decisions are made about what metrics to optimise for, which categories to use, how to define their bounds, who applies the labels. Similarly, qualitative research is necessary to understand AI systems operating in society: evaluating system performance beyond what can be captured in short term metrics, understanding what is missed by large-scale studies (which can elide details and overlook outliers), and shedding light on the circumstances in which data is produced (often by crowd-sourced or poorly paid workers).\nUnfortunately, there is often a large divide between computer scientists and social scientists, with over-simplified assumptions and fundamental misunderstandings of one another. Even when cross-disciplinary partnerships occur, they often fall into “normal disciplinary divisions of labour: social scientists observe, data scientists make; social scientists do ethics, data scientists do science; social scientists do the incalculable, data scientists do the calculable.” The solution is not for computer scientists to absorb a shallow understanding of the social sciences, but for deeper collaborations. In a paper on exclusionary practices in AI ethics, an interdisciplinary team wrote of the “indifference, devaluation, and lack of mutual support between CS and humanistic social science (HSS), [which elevates] the myth of technologists as ‘ethical unicorns’ that can do it all, though their disciplinary tools are ultimately limited.”\nThis is further reflected in an increasing number of job ads for AI ethicists that list a computer science degree as a requirement, “prioritising technical computer science infrastructure over the social science skills that can evaluate AI’s social impact. In doing so, we are building the field of AI Ethics to replicate the very flaws this field is trying to fix.” Interviews with 26 responsible AI practitioners working in industry highlighted a number of challenges, including that qualitative work was not prioritised. Not only is it impossible to fully understand ethics issues solely through quantitative metrics, inappropriate and misleading quantitative metrics are used to evaluate the responsible AI practitioners themselves. Interviewees reported that their fairness work was evaluated on metrics related to generating revenue, in a stark misalignment of goals."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#qualitative-research-helps-us-evaluate-ai-systems-beyond-short-term-metrics",
    "href": "posts/2022-06-01-qualitative/index.html#qualitative-research-helps-us-evaluate-ai-systems-beyond-short-term-metrics",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "Qualitative research helps us evaluate AI systems beyond short term metrics",
    "text": "Qualitative research helps us evaluate AI systems beyond short term metrics\nWhen companies like Google and YouTube want to test whether the recommendations they are making (in the form of search engine results or YouTube videos, for example) are “good” - they will often focus quite heavily on “engagement” or “dwell time” - the time a user spent looking at or watching the item recommended to them. But it turns out, unsurprisingly, that a focus on engagement and dwell time, narrowly understood, raises all sorts of problems. Demographics can impact dwell time (e.g. older users may spend longer on websites than younger users, just as part of the way they use the internet). A system that ‘learns’ from a user’s behavioural cues (rather than their ‘stated preferences’) might lock them into a limiting feedback loop, appealing to that user’s short term interests rather than those of their ‘Better Selves.’ Scholars have called for more qualitative research to understand user experience and build this into the development of metrics.\nThis is the part where people will point out, rightly, that companies like Google and YouTube rely on a complex range of metrics and signals in their machine learning systems - and that where a website ranks on Google, or how a YouTube video performs in recommendation does not boil down to simple popularity metrics, like engagement. Google employs an extensive process to determine “relevance” and “usefulness” for search results. In its 172-page manual for search result ‘Quality’ evaluation, for example, the company explains how evaluators should assess a website’s ‘Expertise/ Authoritativeness/ Trustworthiness’ or ‘E-A-T’; and what types of content, by virtue of its harmful nature (e.g., to protected groups), should be given a ‘low’ ranking. YouTube has identified specific categories of content (such as news, scientific subjects, and historical information) for which ‘authoritativeness’ should be considered especially important. It has also determined that dubious-but-not-quite-rule-breaking information (what it calls ‘borderline content’) should not be recommended, regardless of the video’s engagement levels.\nIrrespective of how successful we consider the existing approaches of Google Search and YouTube to be (and partly, the issue is that evaluating their implementation from the outside is frustratingly difficult), the point here is that there are constant qualitative judgments being made, about what makes a search result or recommendation “good” and of how to define and quantify expertise, authoritativeness, trustworthiness, borderline content, and other values. This is true of all machine learning evaluation, even when it isn’t explicit. In a paper guiding companies about how to carry out internal audits of their AI systems, Inioluwa Deborah Raji and colleagues emphasise the importance of interviews with management and engineering teams to “capture and pay attention to what falls outside the measurements and metrics, and to render explicit the assumptions and values the metrics apprehend.” (p.40).\nThe importance of thoughtful humanities research is heightened if we are serious about grappling with the potential broader social effects of machine learning systems (both good and bad), which are often delayed, distributed and cumulative."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#small-scale-qualitative-studies-tell-an-important-story-even-and-perhaps-especially-when-they-seem-to-contradict-large-scale-objective-studies",
    "href": "posts/2022-06-01-qualitative/index.html#small-scale-qualitative-studies-tell-an-important-story-even-and-perhaps-especially-when-they-seem-to-contradict-large-scale-objective-studies",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "Small-scale qualitative studies tell an important story even (and perhaps especially) when they seem to contradict large-scale ‘objective’ studies",
    "text": "Small-scale qualitative studies tell an important story even (and perhaps especially) when they seem to contradict large-scale ‘objective’ studies\nHypothetically, let’s say you wanted to find out whether the use of AI technologies by doctors during a medical appointment would make doctors less attentive to patients - what do you think the best way of doing it would be? You could find some criteria and method for measuring ‘attentiveness’, say tracking the amount of eye contact between the doctor and patient, and analyse this across a representative sample of medical appointments where AI technologies were being used, compared to a control group of medical appointments where AI technologies weren’t being used. Or would you interview doctors about their experiences using the technology during appointments? Or talk to patients about how they felt the technology did, or didn’t, impact their experience?\nIn research circles, we describe these as ‘epistemological’ choices - your judgement of what constitutes the ‘best’ approach is inextricably linked to your judgement about how we can claim to ‘know’ something. These are all valid methods for approaching the question, but you can imagine how they might result in different, even conflicting, insights. For example, you might end up with the following results: - The eye contact tracking experiment suggests that overall, there is no significant difference in doctors’ attentiveness to the patient when the AI tech is introduced. - The interviews with doctors and patients reveal that some doctors and patients feel that the AI technology reduces doctors’ attentiveness to patients, and others feel that it makes no difference or even increases doctors’ attention to the patient.\nEven if people are not negatively impacted by something ‘on average’ (e.g., in our hypothetical eye contact tracking experiment above), there will remain groups of people who will experience negative impacts, perhaps acutely so. “Many of people’s most pressing questions are about effects that vary for different people,” write Matias, Pennington and Chan in a recent paper on the idea of N-of-one trials. To tell people that their experiences aren’t real or valid because they don’t meet some threshold for statistical significance across a large population doesn’t help us account for the breadth and nature of AI’s impacts on the world.\nExamples of this tension between competing claims to knowledge about AI systems’ impacts abound. Influencers who believe they are being systematically downranked (‘shadowbanned’) by Instagram’s algorithmic systems are told by Instagram that this simply isn’t true. Given the inscrutability of these proprietary algorithmic systems, it is impossible for influencers to convincingly dispute Instagram’s claims. Kelley Cotter refers to this as a form of “black box gaslighting”: platforms can “leverage perceptions of their epistemic authority on their algorithms to undermine users’ confidence in what they know about algorithms and destabilise credible criticism.” Her interviews with influencers give voice to stakeholder concerns and perspectives that are elided in Instagram’s official narrative about its systems. The mismatch between different stakeholders’ accounts of ‘reality’ is instructive. For example, a widely-cited paper by Netflix employees claims that Netflix recommendation “influences choice for about 80% of hours streamed at Netflix.” But this claim stands in stark contrast to Mattias Frey’s mixed-methods research (representative survey plus small sample for interviews) run with UK and US adults, in which less than 1 in 5 adults said they primarily relied on Netflix recommendations when deciding what films to watch. Even if this is because users underestimate their reliance on recommender systems, that’s a critically important finding - particularly when we’re trying to regulate recommendation and so many are advocating providing better user-level controls as a check on platform power. Are people really going to go to the trouble of changing their settings if they don’t think they rely on algorithmic suggestions that much anyway?"
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#qualitative-research-sheds-light-on-the-context-of-data-annotation",
    "href": "posts/2022-06-01-qualitative/index.html#qualitative-research-sheds-light-on-the-context-of-data-annotation",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "Qualitative research sheds light on the context of data annotation",
    "text": "Qualitative research sheds light on the context of data annotation\nMachine learning systems rely on vast amounts of data. In many cases, for that data to be useful, it needs to be labelled/ annotated. For example, a hate speech classifier (an AI-enabled tool used to identify and flag potential cases of hate speech on a website) relies on huge datasets of text labelled as ‘hate speech’ or ‘not hate speech’ to ‘learn’ how to spot hate speech. But it turns out that who is doing the annotating and in what context they’re doing it, matters. AI-powered content moderation is often held up as the solution to harmful content online. What has continued to be underplayed is the extent to which those automated systems are and most likely will remain dependent on the manual work of human content moderators sifting through some of the worst and most traumatic online material to power the machine learning datasets on which automated content moderation depends. Emily Denton and her colleagues highlight the significance of annotators’ social identity (e.g., race, gender) and their expertise when it comes to annotation tasks, and they point out the risks associated with overlooking these factors and simply ‘aggregating’ results as ‘ground truth’ rather than properly exploring disagreements between annotators and the important insights that this kind of disagreement might offer.\nHuman commercial content moderators (such as the people that identify and remove violent and traumatic imagery on Facebook) often labour in terrible conditions, lacking psychological support or appropriate financial compensation. The interview-based research of Sarah T. Roberts has been pioneering in highlighting these conditions. Most demand for crowdsourced digital labour comes from the Global North, yet the majority of these workers are based in the Global South and receive low wages. Semi-structured interviews reveal the extent to which workers feel unable to bargain effectively for better pay in the current regulatory environment. As Mark Graham and his colleagues point out, these findings are hugely important in a context where several governments and supranational development organisations like the World Bank are holding up digital work as a promising tool to fight poverty.\nThe decision of how to measure ‘race’ in machine learning systems is highly consequential, especially in the context of existing efforts to evaluate these systems for their “fairness.” Alex Hanna, Emily Denton, Andrew Smart and Jamila Smith-Loud have done crucial work highlighting the limitation of machine learning systems that rely on official records of race or their proxies (e.g. census records), noting that the racial categories provided by such records are “unstable, contingent, and rooted in racial inequality.” The authors emphasise the importance of conducting research in ways that prioritise the perspectives of the marginalised racial communities that fairness metrics are supposed to protect. Qualitative research is ideally placed to contribute to a consideration of “race” in machine learning systems that is grounded in the lived experiences and needs of the racially subjugated."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#what-next",
    "href": "posts/2022-06-01-qualitative/index.html#what-next",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "What next?",
    "text": "What next?\nCollaborations between quantitative and qualitative researchers are valuable in understanding AI ethics from all angles.\nConsider reading more broadly, outside your particular area. Perhaps using the links and researchers listed here as starting points. They’re just a sliver of the wealth that’s out there. You could also check out the Social Media Collective’s Critical Algorithm Studies reading list, the reading list provided by the LSE Digital Ethnography Collective, and Catherine Yeo’s suggestions.\nStrike up conversations with researchers in other fields, and consider the possibility of collaborations. Find a researcher slightly outside your field but whose work you broadly understand and like, and follow them on Twitter. With any luck, they will share more of their work and help you identify other researchers to follow. Collaboration can be an incremental process: Consider inviting the researcher to form part of a discussion panel, reach out to say what you liked and appreciated about their work and why, and share your own work with them if you think it’s aligned with their interests.\nWithin your university or company, is there anything you could do to better reward or facilitate interdisciplinary work? As Humanities Computing Professor Willard McCarty notes, somewhat discouragingly, “professional reward for genuinely interdisciplinary research is rare.” To be sure, individual researchers and practitioners have to be prepared to put themselves out there, compromise and challenge themselves - but carefully tailored institutional incentives and enablers matter."
  }
]