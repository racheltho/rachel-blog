[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "MSc Immunology student and cofounder of fast.ai\nPast: professor & director of USF Center Applied Data Ethics, data scientist, wrote several book chapters on data ethics, one of Forbes 20 Incredible Women in AI, PhD in mathematics\nInterests: mathematical biology, data ethics, and machine learning\nLocation: Queensland, Australia\n\nMathematical Biology and AI in Medicine\n\n\nA Mathematical Model of Glutathione Metabolism, Journal of Theoretical Biology and Medical Modeling\nMedicine’s Machine Learning Problem, Boston Review\nEarned a PhD in mathematics from Duke University\nHoward Hughes Medical Institute Fellowship\nkeynote speaker at Stanford’s Artificial Intelligence in Medicine symposium\n\n\nData Ethics\n\n\nProfessor of Practice at Queensland University of Technology Centre for Data Science\nFounding director of Center for Applied Data Ethics (CADE) at University of San Francisco\nReliance on Metrics is a Fundamental Challenge for AI, Patterns. Optimizing metrics is a central aspect of most current AI approaches, yet overemphasizing metrics leads to manipulation, gaming, and a myopic short-term focus.\nCreated and taught data ethics course\nWrote ethics chapter of best-selling book,\nWrote book chapters for:\n\n97 Things About Ethics Everyone in Data Science Should Know\nDeep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD\nRedesigning AI\n\n\n\nMachine Learning and Data Science\n\n\nCo-founder of course.fast.ai\nDesigned and taught graduate level courses on Natural Language Processing and Computational Linear Algebra in the USF Masters of Data Science program\nThe New Era in NLP, Keynote at SciPy (Scientific Python) Conference 2019\nKeynote at ICML AutoML workshop, based on my popular series of AutoML posts\nBeginner friendly workshop on Word Embeddings (such as Word2Vec)\nForbes 20 Incredible Women in AI\nFeatured in book Women Tech Founders on the Rise\nEarly data scientist and software engineer at Uber\n\n\n\n\n\nA few events I’ve spoken at"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "a blog about data, ethics, & immunology",
    "section": "",
    "text": "Viruses: The Silent Triggers of Autoimmune and Neurodegenerative Diseases\n\n\n\n\n\n\n\nscience\n\n\n\n\nHow a simple cold can lead to life-changing disease\n\n\n\n\n\n\nMar 22, 2023\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nViruses are weirder, worse, & more preventable than you realise\n\n\n\n\n\n\n\nscience\n\n\n\n\nThe suprising, devastating, long-term consequences of viruses\n\n\n\n\n\n\nMar 7, 2023\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nHow to Remember Anything\n\n\n\n\n\n\n\neducation\n\n\nadvice\n\n\n\n\nA study technique for all ages and all subjects\n\n\n\n\n\n\nFeb 21, 2023\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nI was an AI researcher. Now, I am an immunology student.\n\n\n\n\n\n\n\neducation\n\n\nscience\n\n\n\n\nLast year, I became captivated by a new topic in a way that I hadn’t felt since I first discovered machine learning\n\n\n\n\n\n\nFeb 7, 2023\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nMy family’s unlikely homeschooling journey\n\n\n\n\n\n\n\neducation\n\n\n\n\nPrior to 2020, we never expected to homeschool, and now we have committed to it long-term.\n\n\n\n\n\n\nSep 6, 2022\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nQualitative humanities research is crucial to AI\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nFollowing the thread of any quantitative issue leads to qualitative questions.\n\n\n\n\n\n\nJun 1, 2022\n\n\nLouisa Bartolo and Rachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nAI Harms are Societal, Not Just Individual\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nMuch like pollution, the harms caused by algorithmic systems are often collective and communal.\n\n\n\n\n\n\nMay 17, 2022\n\n\nRachel Thomas and Louisa Bartolo\n\n\n\n\n\n\n  \n\n\n\n\nThere’s no such thing as not a math person\n\n\n\n\n\n\n\nadvice\n\n\neducation\n\n\n\n\nMany cultural factors, misconceptions, stereotypes, and obstacles turn people off to math.\n\n\n\n\n\n\nMar 15, 2022\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nAvoiding Data Disasters\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nWhy you can’t ignore the crucial role of humans in the data science pipeline\n\n\n\n\n\n\nNov 4, 2021\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nMedicine is Political\n\n\n\n\n\n\n\nscience\n\n\nethics\n\n\n\n\nFrom covid-19 to HIV research to the long history of wrongly assuming women’s illnesses are psychosomatic, we have seen again and again that medicine, like all science, is political.\n\n\n\n\n\n\nOct 12, 2021\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\n11 Short Videos About AI Ethics\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nShort videos about machine learning ethics to watch and share.\n\n\n\n\n\n\nAug 16, 2021\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nEssential Work-From-Home Advice: Cheap and Easy Ergonomic Setups\n\n\n\n\n\n\n\nadvice\n\n\nwork\n\n\n\n\nYou can permanently damage your back, neck, and wrists from working without an ergonomic setup. Learn how to create one for less.\n\n\n\n\n\n\nAug 6, 2020\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\n4 Principles for Responsible Government Use of Technology\n\n\n\n\n\n\n\nethics\n\n\n\n\nGovernment use of new technology raises issues around surveillance of vulnerable populations, unintended consequences, and potential misuse.\n\n\n\n\n\n\nJan 21, 2020\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nThe problem with metrics is a big problem for AI\n\n\n\n\n\n\n\nmachine learning\n\n\nethics\n\n\n\n\nUnthinkingly optimizing metrics can lead to a variety of grave harms, and what most current AI approaches do is to optimize metrics.\n\n\n\n\n\n\nSep 24, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\n8 Things You Need to Know about Surveillance\n\n\n\n\n\n\n\nethics\n\n\n\n\nHow surveillance makes us less safe\n\n\n\n\n\n\nAug 7, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nAdvice for Better Blog Posts\n\n\n\n\n\n\n\nadvice\n\n\n\n\nA blog is like a resume, only better.\n\n\n\n\n\n\nMay 13, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nTech’s Long Hours Are Discriminatory and Counterproductive\n\n\n\n\n\n\n\ninclusion\n\n\nwork\n\n\n\n\nWhy working longer doesn’t work.\n\n\n\n\n\n\nFeb 19, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nFive Things That Scare Me About AI\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\n\n\nAn AI-powered future is already here, and some of the consequences are scarier than you may realize.\n\n\n\n\n\n\nJan 29, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nThings everyone should know about chronic illness & disability\n\n\n\n\n\n\n\ninclusion\n\n\n\n\nMisconceptions about chronic illness are harmful\n\n\n\n\n\n\nJan 21, 2019\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nWhat Harvard Business Review Gets Wrong About Algorithms and Bias\n\n\n\n\n\n\n\nethics\n\n\n\n\nComparing human and computer-based decision-making is not an apples-to-apples comparison.\n\n\n\n\n\n\nAug 7, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nGoogle’s AutoML: Cutting Through the Hype\n\n\n\n\n\n\n\ntechnical\n\n\nmachine learning\n\n\n\n\nAnalyzing Google’s claims that we all need custom neural networks\n\n\n\n\n\n\nJul 23, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nAn Opinionated Introduction to AutoML and Neural Architecture Search\n\n\n\n\n\n\n\ntechnical\n\n\nmachine learning\n\n\n\n\nAutoML is being heavily hyped– but would AugmentedML be a better approach?\n\n\n\n\n\n\nJul 16, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nWhat do machine learning practitioners actually do?\n\n\n\n\n\n\n\nmachine learning\n\n\nadvice\n\n\nwork\n\n\n\n\nMachine learning is an in-demand field, but there are misconceptions about what the work entails in practice.\n\n\n\n\n\n\nJul 12, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Deep Learning for Tabular Data\n\n\n\n\n\n\n\nmachine learning\n\n\ntechnical\n\n\n\n\nDeep learning is not just for images and text.\n\n\n\n\n\n\nApr 29, 2018\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nMaking Peace with Personal Branding\n\n\n\n\n\n\n\nadvice\n\n\n\n\nEven though the term ‘personal branding’ makes me cringe, it is a useful concept\n\n\n\n\n\n\nDec 18, 2017\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nWhat you need to do deep learning\n\n\n\n\n\n\n\nadvice\n\n\ntechnical\n\n\nmachine learning\n\n\n\n\nThe hardware, software, background, and data you need to do deep learning (it may be less than you expect).\n\n\n\n\n\n\nNov 16, 2017\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nHow (and why) to create a good validation set\n\n\n\n\n\n\n\nmachine learning\n\n\ntechnical\n\n\n\n\nAvoid failures from poorly chosen validation sets.\n\n\n\n\n\n\nNov 13, 2017\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nWhy you (yes, you) should blog\n\n\n\n\n\n\n\nadvice\n\n\n\n\nThe top advice I would give my younger self would be to start blogging sooner.\n\n\n\n\n\n\nJul 28, 2017\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nHow to Make Tech Interviews a Little Less Awful\n\n\n\n\n\n\n\nadvice\n\n\nwork\n\n\ninclusion\n\n\n\n\nThe tech interview process is very, very broken.\n\n\n\n\n\n\nMar 5, 2017\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nHow should you structure your Data Science and Engineering teams?\n\n\n\n\n\n\n\nadvice\n\n\nwork\n\n\n\n\nThere are a lot of potential pitfalls related to data science and org structure (no matter what you choose)\n\n\n\n\n\n\nDec 8, 2016\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nPlease don’t say ‘It used to be called big data and now it’s called deep learning’\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nOn the difference between big data, machine learning, and deep learning.\n\n\n\n\n\n\nNov 17, 2016\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nProviding a Good Education in Deep Learning\n\n\n\n\n\n\n\neducation\n\n\n\n\nMachine learning education without the tedium and gatekeeping\n\n\n\n\n\n\nOct 8, 2016\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nThe Real Reason Women Quit Tech (and How to Address It)\n\n\n\n\n\n\n\ninclusion\n\n\n\n\nWhy recruiting and mentoring women doesn’t solve diversity\n\n\n\n\n\n\nOct 4, 2016\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nDiversity Washing Makes Things Worse\n\n\n\n\n\n\n\ninclusion\n\n\n\n\nShallow, showy diversity efforts aren’t just ineffective, they are actively harmful.\n\n\n\n\n\n\nDec 7, 2015\n\n\nRachel Thomas\n\n\n\n\n\n\n  \n\n\n\n\nIf you think women in tech is just a pipeline problem, you haven’t been paying attention\n\n\n\n\n\n\n\ninclusion\n\n\n\n\nIt doesn’t matter how many girls you teach to code if you keep driving adult women out of the tech industry.\n\n\n\n\n\n\nJul 27, 2015\n\n\nRachel Thomas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html",
    "href": "posts/2015-07-27-not-pipeline/index.html",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "",
    "text": "This article has been translated into Spanish at Matajuegos and into Russian by Dmitry Si.\nAccording to the Harvard Business Review, 41% of women working in tech eventually end up leaving the field (compared to just 17% of men), and I can understand why…\nI first learned to code at age 16, and am now in my 30s. I have a math PhD from Duke. I still remember my pride in a “knight’s tour” algorithm that I wrote in C++ in high school; the awesome mind warp of an interpreter that can interpret itself (a Scheme course my first semester of college); my fascination with numerous types of matrix factorizations in C in grad school; and my excitement about relational databases and web scrapers in my first real job.\nOver a decade after I first learned to program, I still loved algorithms, but felt alienated and depressed by tech culture. While at a company that was a particularly poor culture fit, I was so unhappy that I hired a career counselor to discuss alternative career paths. Leaving tech would have been devastating, but staying was tough."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#work-hard-play-hard",
    "href": "posts/2015-07-27-not-pipeline/index.html#work-hard-play-hard",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Work hard, play hard",
    "text": "Work hard, play hard\nI’m not the stereotypical male programmer in his early 20s looking to “work hard, play hard”. I do work hard, but I’d rather wake up early than stay up late, and I was already thinking ahead to when my husband and I would need to coordinate our schedules with daycare drop-offs and pick-ups. Kegerators and ping pong tables don’t appeal to me. I’m not aggressive enough to thrive in a combative work environment. Talking to other female friends working in tech, I know that I’m not alone in my frustrations.\nWhen researcher Kieran Snyder interviewed 716 women who left tech after an average tenure of 7 years, almost all of them said they liked the work itself, but most were unhappy with the work environment. In NSF-funded research, Nadya Fouad surveyed 5,300 women who had earned engineering degrees (of all types) over the last 50 years, and 38% of them were no longer working as engineers. Fouad summarized her findings on why they leave with “It’s the climate, stupid!”\nThis is a huge, unnecessary, and expensive loss of talent in a field facing a supposed talent shortage. Given that tech is currently one of the major drivers of the US economy, this impacts everyone. Any tech company struggling to hire and retain as many employees as they need should particularly care about addressing this problem."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#your-company-is-not-a-meritocracy-and-you-are-not-gender-blind",
    "href": "posts/2015-07-27-not-pipeline/index.html#your-company-is-not-a-meritocracy-and-you-are-not-gender-blind",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Your company is NOT a meritocracy and you are NOT “gender-blind”",
    "text": "Your company is NOT a meritocracy and you are NOT “gender-blind”\nYou don’t know if you’re color-blind without testing either\nNobody wants to think of themselves as being sexist. However, a number of studies have shown that identical job applications or resumes are evaluated differently based on whether they are labeled with a male or female name. When men and women read identical scripts containing entrepreneurial pitches or salary negotiations, they are evaluated differently. Both men and women have been shown to have these biases. These biases occur unconsciously and without intention or malice.\nHere is a sampling of just a few of the studies on unconscious gender bias:\n\nInvestors preferred entrepreneurial ventures pitched by a man than an identical pitch from a woman by a rate of 68% to 32% in a study conducted jointly by HBS, Wharton, and MIT Sloan. “Male-narrated pitches were rated as more persuasive, logical and fact-based than were the same pitches narrated by a female voice.”\nIn a randomized, double-blind study by Yale researchers, science faculty at 6 major institutions evaluated applications for a lab manager position. Applications randomly assigned a male name were rated as significantly more competent and hirable and offered a higher starting salary and more career mentoring, compared to identical applications assigned female names.\nWhen men and women negotiated a job offer by reading identical scripts for a Harvard and CMU study, women who asked for a higher salary were rated as being more difficult to work with and less nice, but men were not perceived negatively for negotiating.\nPsychology faculty were sent CVs for an applicant (randomly assigned male or female name), and both men and women were significantly more likely to hire a male applicant than a female applicant with an identical record.\nIn 248 performance reviews of high-performers in tech, negative personality criticism (such as abrasive, strident, or irrational) showed up in 85% of reviews for women and just 2% of reviews for men. It is ridiculous to assume that 85% of women have personality problems and that only 2% of men do.\n\nMost concerningly, a study from Yale researchers shows that perceiving yourself as objective is actually correlated with showing even more bias. The mere desire to not be biased is not enough to overcome decades of cultural conditioning and can even lend more credence to post-hoc justifications. Acknowledging that you have biases that conflict with your values does not make you a bad person. It’s a natural result of our culture. The important thing is to find ways to eliminate them. Blindly believing your company is a meritocracy not only does not make it so, but will actually make it even harder to address implicit bias.\nBias is typically justified post-hoc. Our initial subconscious impression of the female applicant is negative, and then we find logical reasons to justify it. For instance, in the above study by Yale researchers if the male applicant for police chief had more street smarts and the female applicant had more formal education, evaluators decided that street smarts were the most important trait, and if the names were reversed, evaluators decided that formal education was the most important trait."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#good-news-and-bad-news",
    "href": "posts/2015-07-27-not-pipeline/index.html#good-news-and-bad-news",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Good News and Bad News",
    "text": "Good News and Bad News\n\nThe Bad News…\nBecause of the high attrition rate for women working in tech, teaching more girls and women to code is not enough to solve this problem. Because of the above well-documented differences in how men and women are perceived, training women to negotiate better and be more assertive is also not enough to solve this problem. Female voices are perceived as less logical and less persuasive than male voices. Women are perceived negatively for being too assertive. If tech culture is going to change, everyone needs to change, especially men and most especially leaders.\nThe professional and emotional costs to women for speaking out about discrimination can be large (in terms of retaliation, being perceived as less employable or difficult to work with, or companies then seeking to portray them as poor performers). I know a number of female software engineers who will privately share stories of sexism with trusted friends that we are not willing to share publicly because of the risk. This is why it is important to proactively address this issue. There is more than enough published research and personal stories from those who have chosen to publicly share to confirm that this is a widespread issue in the tech industry.\n\n\n…and the Good News\nChange is possible. Although these are schools and not tech companies, Harvey Mudd and Harvard Business School provide inspiring case studies. Strong leaders at both schools enacted sweeping changes to address previously male-centric cultures. Harvey Mudd has raised the percentage of computer science majors that are women to 40% (the national average is 18%). The top 5% of Harvard Business School graduates rose from being approximately 20% women to closer to 40% and the GPA gap between men and women closed, all within one year of making a number of comprehensive, structural changes."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#so-what-can-we-do-about-it",
    "href": "posts/2015-07-27-not-pipeline/index.html#so-what-can-we-do-about-it",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "So What Can We Do About It?",
    "text": "So What Can We Do About It?\nThese recommendations on what companies could do to improve their cultures are based on a mix of research and personal experience. My goal is to have a positive focus, and I would love it if you walked away with at least one concrete goal for making constructive change at your company.\n\nTrain managers\nIt is very common at tech start-ups to promote talented engineers to management without providing them with any management training or oversight, particularly at rapidly growing companies where existing leadership is stretched thin. These new managers are often not aware of any of the research on motivation, human psychology, or bias. Untrained, unsupervised managers cause more harm to women than men, although regardless, all employees would benefit from new managers receiving training, mentorship, and supervision.\n\n\nFormalize hiring and promotion criteria\nIn the Yale study mentioned above regarding applicants for police chief, getting participants to formalize their hiring criteria before they looked at applications (i.e. deciding if formal education or street smarts was more important) reduced bias. I was once on a team where the hiring criteria were amorphous and where the manager frequently overrode majority votes by the team because of “gut feeling”. It seemed like unconscious bias played a large role in decisions, but because of our haphazard approach to hiring, there was no way of truly knowing.\n\n\nLeaders, speak up and act in concrete ways\nLeadership sets the values and culture for a company, so the onus is on them to make it clear that they value diversity. Younger engineers and managers will follow their perceptions of what executives value. In the cases of positive change at Harvey Mudd and Harvard Business School, leadership at the top was spearheading these initiatives. Intel is going to begin tying executives’ compensation to whether they achieve diversity goals on their teams. As Kelly Shuster, director for the Denver chapter of Women Who Code has pointed out, leaders have to get rid of employees who engage in sexist or racist behavior. Otherwise, the company is at risk of losing talented employees, and is sending a message to all employees that discrimination is okay.\n\n\nDon’t rely on self-nominations or self-evaluations\nThere is a well-documented confidence gap between men and women. Don’t rely on people nominating themselves for promotions or to get the most interesting projects, since women are less likely to put themselves forward. Google relies on employees nominating themselves for promotions and data revealed that women were much less likely to do so (and thus much less likely to receive promotions). When senior women began hosting workshops encouraging women to nominate themselves, the number of women at Google receiving promotions increased. Groups are more likely to pick male leaders because of their over-confidence, compared to more qualified women who are less confident. Don’t rely heavily on self-evaluations in performance scoring. Women perceive their abilities as being worse than they are, whereas men have an inflated sense of their abilities.\n\n\nFormally audit employee data\nConfirm that men and women with the same qualifications are earning the same amount and that they are receiving promotions and raises at similar rates (and if not, explore why). Make sure that gendered criticism (such as calling a woman strident or abrasive) is not used in performance reviews. The trend of tech companies releasing their diversity statistics is a good one, but given the high industry attrition rate for women, they should also start releasing their retention rates broken down by gender. I would like to see companies release statistics on the rates at which women are given promotions or raises compared to men, and how performance evaluation scores compare between men and women. By publicly sharing data, companies can hold themselves accountable and can track changes over time.\n\n\nDon’t emphasize face time\nA culture that rewards facetime and encourages people to regularly stay late or eat dinner at the office puts employees with families at a disadvantage (particularly mothers), and research shows that working excess hours does not actually improve productivity in the long-term since workers begin to experience burn out after just a few weeks. Furthermore, when employees burn out and quit, the cost of recruiting and hiring a new employee is typically 20% of the annual salary for that position.\n\n\nCreate a collaborative environment\nStanford research studies document that women are more likely to dislike competitive environments compared to men and are more likely to select out of them, regardless of their ability. Given that women are perceived negatively for being too assertive, it is tougher for women to succeed in a highly aggressive environment as well. Men who speak up more than their peers are rewarded with 10% higher ratings, whereas women who speak up more are punished with 14% lower ratings. Creating a competitive culture where people must fight for their ideas makes it much tougher for women to succeed.\n\n\nOffer maternity leave\nOver 10% of the 716 women who left tech in Kieran Snyder’s research left because of inadequate maternity leave. Several were pressured to return from leave early or to be on call while on leave. These women did not want to be stay-at-home-parents, they just wanted to recover after giving birth. Just as you would not pressure someone to return to work without recovery time after a major surgery, women need time to physically heal after delivering a baby. When Google increased paid maternity leave from 12 weeks to 18 weeks, the number of new moms who quit Google dropped by 50%."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#some-final-thoughts",
    "href": "posts/2015-07-27-not-pipeline/index.html#some-final-thoughts",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Some final thoughts…",
    "text": "Some final thoughts…\n\nA note on racial bias\nThere is a huge amount of research on unconscious racial bias, and tech companies need to address this issue. As Nichole Sanchez, VP of Social Impact at GitHub, describes, calls for diversity are often solely about adding more white women, which is deeply problematic. Racial bias adds another intersectional dimension to the discrimination that women of color experience. In interviews with 60 women of color who work in STEM research, 100% of them had experienced discrimination, and the particular negative stereotypes they faced differed depending on their race. A resume with a traditionally African-American sounding name is less likely to be called for an interview than the same resume with a traditionally white sounding name. I do not have the personal experience to speak about this topic and instead encourage you to read these blog posts and articles by and about tech workers of color on the challenges they’ve faced: Erica Joy (Slack engineer, former Google engineer), Justin Edmund (designer, Pinterest’s 7th employee), Aston Motes (Engineer, Dropbox’s 1st employee), and Angelica Coleman (developer advocate at Zendesk, formerly at Dropbox)."
  },
  {
    "objectID": "posts/2015-07-27-not-pipeline/index.html#now",
    "href": "posts/2015-07-27-not-pipeline/index.html#now",
    "title": "If you think women in tech is just a pipeline problem, you haven’t been paying attention",
    "section": "Now",
    "text": "Now\nI’m currently teaching software development at all-women Hackbright Academy, a job that I love and that suits me perfectly. I want all women to have the opportunity (and I mean truly have the opportunity, without implicit or explicit discrimination) to learn how to program — knowing software development provides so many career and financial possibilities; it’s intellectually rewarding and fun; and being a creator is deeply satisfying. Although I know many women with frustrating experiences of sexism, I also know women who have found companies where they’re happily thriving. I’m glad for the attention tech’s diversity problem has been receiving and I am hopeful about continued change.\nThanks for review, edits, and discussion to: Jeremy Howard and Angie Chang.\nI do more research and further develop the ideas in this post in my later posts: on how showy, shallow diversity strategies make things worse; on bullshit diversity initiatives and some better ideas; and the research on how women are leaving tech because they can’t advance in their careers."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html",
    "href": "posts/2015-12-07-diversity-washing/index.html",
    "title": "Diversity Washing Makes Things Worse",
    "section": "",
    "text": "It is painful watching tech companies known to be bad environments for women and people of color make shallow, showy attempts to rebrand themselves as valuing diversity. Perhaps you’re thinking, Any effort towards diversity is a good thing, and there’s no harm in trying, right? Wrong. This is not just a triumph of style over substance; these efforts can harm the people they are purporting to help. For instance, research shows that many diversity programs reduce the number of Black women and Black men in management; diversity structures cause people to be less likely to believe women and people of color; and some forms of unconscious bias training increase bias.\nSince the following points have been covered thoroughly elsewhere, I’m going to take it as a given that:\n\nTech has a diversity problem\nIt’s not just the pipeline (really)\nMeritocracy is a myth\nDiversity is a good thing\n\n(If you’re unfamiliar with these ideas, please read the linked articles; I highly recommend them.) I very much want to see these problems fixed, but they need more than just a coat of PR-friendly paint. Any successful effort towards diversity and inclusion will need to involve comprehensive changes, ongoing self-reflection, and tackling hard problems, not just superficial, high-publicity, quick fixes.\nThe rest of this post will dig into what the research shows about ways that diversity programs can backfire. My next article will suggest some ideas for effective programs."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#the-danger-of-diversity-washing",
    "href": "posts/2015-12-07-diversity-washing/index.html#the-danger-of-diversity-washing",
    "title": "Diversity Washing Makes Things Worse",
    "section": "The danger of diversity washing",
    "text": "The danger of diversity washing\nResearchers from U of Washington, UCLA, and UCSB showed that the mere presence of diversity policies, diversity training, and diversity awards cause white people to be less likely to believe racial discrimination exists and cause men to be less likely to believe gender discrimination exists, despite other data and evidence. Participants in one study read a New York Times article published as a class action lawsuit for gender discrimination against pharmaceutical giant Novartis went to trial. The twist was that half the participants were shown an article that included a sentence stating that Working Mother magazine had chosen Novartis as one of the 100 best companies in the USA. This sentence was omitted from the article for the other half. Those that read the sentence about the_Working Mother_ accolade were less likely to believe that the female employees had a valid case against Novartis, even though the rest of the article remained the same.\nThe researchers conducted 6 variations of the study. In one version, white people read either a diversity statement, or a mission statement, for a fictional company. They were then shown data on comparative promotion rates by race, as well as an article about a Black employee filing suit for racial discrimination. Participants who had read the diversity statement were less likely to believe that discrimination had occurred and rated the Black employee more negatively (compared to those that read the mission statement, which did not mention diversity), even when the data showed clear racial differences. Other versions of the study provided differing data on hiring rates and salaries. In all versions, the presence of a diversity structure (such as diversity policies, diversity training, or diversity awards) caused white people to be less likely to believe racial discrimination and caused men to be less likely to believe gender discrimination.\nThis shows that the presence of diversity programs can hurt women and people of color by creating what the study’s authors call an illusion of fairness. Because of the “diversity branding”, people are less likely to believe that discrimination exists at that company, regardless of what the data shows. So the next time you see a tech company announce their shiny new diversity initiative with much fanfare, consider one impact: that the negative accounts of women and people of color that work at these companies will be disregarded even more often. This belief that the existence of diversity initiatives equals equality is just one way that such efforts can backfire. Next, let’s look at some real world data from the outcomes of diversity programs (it’s not what you would hope)."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#what-does-data-from-diversity-programs-at-over-700-companies-reveal",
    "href": "posts/2015-12-07-diversity-washing/index.html#what-does-data-from-diversity-programs-at-over-700-companies-reveal",
    "title": "Diversity Washing Makes Things Worse",
    "section": "What does data from diversity programs at over 700 companies reveal?",
    "text": "What does data from diversity programs at over 700 companies reveal?\nHarvard professor Frank Dobbin led a team of sociologists in reviewing data from 708 companies to evaluate 7 different approaches to trying to increase the share of White women, Black women, and Black men in management, and found that many programs were ineffective and some diversity efforts even made things worse. Programs that targeted stereotypes through education and feedback, such as diversity training, were the least effective, and in some cases reduced diversity. The study found that diversity training was followed by a 7% decline in the proportion of Black women in management. Diversity evaluations of managers were followed by an 8% decline in Black men in management, although a 6% increase for White women. This is a particular issue when tech diversity efforts are often limited only to recruiting White women.\nThe most effective programs were “responsibility structures” such as diversity committees, diversity staff positions, and affirmative action plans. Professor Dobbin stated that “if no one is specifically charged with the task of increasing diversity, then the buck inevitably gets passed ad infinitum. To increase diversity, executives must treat it like any other business goal.” Networking and mentoring produced modest positive effects. Diversity training was one of the least effective approaches. Dobbin says that “even with best practices, you’re not going to get much of an effect. It doesn’t change what happens at work.”\nTogether with Alexandra Kalev of Tel Aviv University, Dobbin later expanded the research to 803 companies, and to include Asian and Hispanic employees (in addition to Black and White employees) in the aptly titled study “Try and Make Me!: Why Corporate Diversity Training Fails.” It is possible that the newer training programs currently in use by tech companies may end up being more effective than those reviewed in Dobbin’s study; however we should wait to see the evidence."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#unconscious-bias-training-can-increase-bias",
    "href": "posts/2015-12-07-diversity-washing/index.html#unconscious-bias-training-can-increase-bias",
    "title": "Diversity Washing Makes Things Worse",
    "section": "Unconscious bias training can increase bias",
    "text": "Unconscious bias training can increase bias\nUnconscious bias (for example, when people rate identical resumes with a female or traditionally African American name more negatively, as opposed to the same resume with a male or traditionally white name) is a very real problem; however just teaching people that unconscious bias exists does not eliminate it and can even increase bias. In this NYTimes article, Sheryl Sandberg and Wharton professor Adam Grant summarize research that unconscious bias training can increase bias, depending on how it is communicated.\nIn one study from UVA and Washington University, managers read a job interview transcript after either being told either that: stereotypes are rare; or being told that: many people believe stereotypes. When participants were told stereotypes were common and that the candidate was female, they were 28% less likely to hire her and judged her as 27% less likable (compared to the identical transcript with the candidate labeled as male). People may feel more comfortable believing stereotypes when they hear that they are commonly believed. Sandberg and Grant argue that the key is to instead communicate that biases are inaccurate, and that most people don’t want to discriminate."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#the-difficulty-of-self-reflection",
    "href": "posts/2015-12-07-diversity-washing/index.html#the-difficulty-of-self-reflection",
    "title": "Diversity Washing Makes Things Worse",
    "section": "The Difficulty of Self-Reflection",
    "text": "The Difficulty of Self-Reflection\nPart of the problem with efforts to raise awareness of unconscious bias is that we are great at finding post-hoc justifications for our biases so we tend to see ourselves as immune to bias. Harvard Business School professor Francesca Gino, who has taught courses on biases and decision making to executives and MBA students, states “most of my students easily recognize that their colleagues and friends are biased but generally don’t think they are themselves.” It is both easier to believe that other people discriminate, but not me, because I have good judgement, and easier to believe that other people experience discrimination, but not my coworker, because I see her flaws.\nA study from Yale researchers shows that perceiving yourself as objective is actually correlated with showing even more bias. Researchers from MIT and Indiana University found that company structures that explicitly promote meritocracy (compared to those that don’t) show greater bias against women. The 445 participants in the study all had managerial experience and were asked to evaluate employee profiles given a set of organizational core values (which included meritocracy in some cases but not others). Women were awarded smaller bonuses than men with equivalent performance reviews when the core values emphasized meritocracy.\nGender and racial biases aren’t just problems affecting the education system and other people’s companies; biases are affecting your company. It’s not just that other people need to change; we all need to change, and it’s an ongoing process."
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#no-easy-fixes",
    "href": "posts/2015-12-07-diversity-washing/index.html#no-easy-fixes",
    "title": "Diversity Washing Makes Things Worse",
    "section": "No Easy Fixes",
    "text": "No Easy Fixes\nI am not the first to express similar frustrations, although I have seen little coverage by tech journalists of this aspect of the issue. Erica Joy, an engineer at Slack, wrote that she is “tired of the ‘we hired this many’ and ‘we gave this many dollars to girls coding initiatives.’ None of those numbers accurately portray what the inside of a company looks like.” Darrell Jones III, head of business development at Clef, explained, “when we allow companies to simply ‘educate’ their employees or ‘spread awareness’ by publishing dismal diversity numbers, we let them off the hook.” Cate Huston, a former Google engineer and head of mobile development at Ride, ranked classes of diversity problems from easy to extra hard, and, expressed her disappointment that Grace Hopper Conference (for women in computing) primarily focuses on “easy problems,” asking, “Is it just going to be a celebration of managing the easy things. Of crawling over that exceptionally low bar of sexist marketing materials. Of focusing on the pipeline rather than the woman who are already here.” Freada Kapor Klein, founder of the Level Playing Field Institute, has observed that “there is way too much money going to hackathons teaching privileged girls how to code without any tie-in to anything else… We’re mapping out all of the drop-off points so that as opposed to being the 400th person who funds a girls coding program, we can even out the dollars.” Indie game developer Veve Jaffe wrote of Intel’s diversity efforts, “my experience reflects a growing trend of corporations paying lip service to diversity — and collecting all of its PR benefits — while demanding unpaid work from underrepresented developers.”\nThis is not an argument against donating to girls coding initiatives, hosting hackathons for girls, or creating inclusive marketing materials (all are good things!), just a reminder that these won’t impact the biases the talented women and people of color already working at your company are currently facing. No donation is ever a substitute for the hard work of self-reflection and company-wide change. The “easy” changes are necessary, but they are not sufficient.\nSimilarly, collecting and sharing diversity data is a necessary step in determining if your current approach is working, but not something to celebrate on its own. It is just a means towards the end goal of creating an equitable and inclusive work environment. If the data shows your approach is not working, you need to change what you’re doing.\nOne of the results of the research on data from 708 companies is that “new programs decoupled from everyday practice often have no impact” and that it is more effective to rethink hiring and promotion structures entirely. This is similar to what Dr. Klein has found working on diversity issues for over a decade; Klein says that having “engineering deeply involved in company diversity and inclusion efforts is critically important to getting it right and not having it be a side annoying thing.”"
  },
  {
    "objectID": "posts/2015-12-07-diversity-washing/index.html#conclusion",
    "href": "posts/2015-12-07-diversity-washing/index.html#conclusion",
    "title": "Diversity Washing Makes Things Worse",
    "section": "Conclusion",
    "text": "Conclusion\nThis post is not an argument against diversity initiatives, but a reminder that diversity and inclusion aren’t automatically achieved when these programs are announced. We should all be on the lookout for concrete progress (not just raw hiring numbers, but also percentage of women and people of color in management and executive positions, salary parity, and retention rates), as well as for negative side effects. Any successful effort towards diversity and inclusion will need to involve ongoing self-reflection, comprehensive changes, and addressing hard problems. In my next post, I will write about examples of more substantial changes.\nMany thanks to Jeremy Howard for feedback on earlier drafts of this post.\nI further develop the ideas here in my next post about bullshit diversity strategies and some better ideas for enacting positive change. I later survey the research on why women are unable to advance in their careers and offer concrete strategies to address this problem. You may also be interested in my post debunking the pipeline myth, which shares my personal story of wanting to leave the tech industry, as well as practical tips."
  },
  {
    "objectID": "posts/2016-10-04-real-reason/index.html",
    "href": "posts/2016-10-04-real-reason/index.html",
    "title": "The Real Reason Women Quit Tech (and How to Address It)",
    "section": "",
    "text": "This article has been translated into Portuguese by the Maria Lab and translated into Spanish by Anastassia Ivaškiva of Tallín University.\nWomen are more than twice as likely to quit the tech industry as men (41% vs 17%). Why is this happening? A study of 4,000 women who had recently changed jobs found that the #1 reason women leave companies is because of “a concern for the lack of advancement opportunity.” An extensive survey of hundreds of books, articles, and white papers concludes that women leave the tech industry because “they’re treated unfairly; underpaid, less likely to be fast-tracked than their male colleagues, and unable to advance.” A study by the Center for Talent Innovation found that 27% of women in tech feel stalled in their careers and 32% are likely to quit within one year; 48% of Black women in tech feel stalled.\nIt’s harder for women to get promoted for a number of reasons. Women frequently experience being excluded from more creative and innovative roles and instead channeled into less fulfilling execution roles, not receiving high visibility “stretch” assignments, having to prove themselves again and again, and having their ideas ignored until a man makes the same suggestion later (a study from Harvard, Wharton, and MIT found that men’s voices are perceived as more persuasive, fact-based, and logical than women’s voices, even when they are reading identical pitches). Researchers have found that women receive more vague feedback and personality criticism in performance evaluations, whereas men receive actionable advice tied to business outcomes. Women in tech are less likely to get their ideas green-lighted for development than men (30% vs 37%). A study of 1,800 professionals found that without diverse leadership, women are 20% less likely than straight white men to win endorsement for their ideas."
  },
  {
    "objectID": "posts/2016-10-04-real-reason/index.html#the-secret-key-to-diversity",
    "href": "posts/2016-10-04-real-reason/index.html#the-secret-key-to-diversity",
    "title": "The Real Reason Women Quit Tech (and How to Address It)",
    "section": "The Secret Key to Diversity",
    "text": "The Secret Key to Diversity\nThe first step, and the most important step, to improving diversity is to make sure that you are treating the women and people of color who already work at your company very well. This includes: appreciate their contributions, assign them to high impact projects, bring up their accomplishments in high level meetings, pay them equitably, provide chances to grow their skillset, listen to them, help them prepare for promotions, give them good managers, believe them about their experiences, and generally support them. Because of unconscious bias, these are actions that we are more likely to do for white men.\nWithout completing this first step of treating current employees well, even if you are able to hire more women and people of color, you won’t be able to retain them, and a bad reputation would make it harder for your company to attract talent in the future. People talk and find out which companies are known for being toxic or biased environments, and which companies are giving people from underrepresented backgrounds opportunities to grow and thrive."
  },
  {
    "objectID": "posts/2016-10-04-real-reason/index.html#warning-mentorship-is-not-the-solution",
    "href": "posts/2016-10-04-real-reason/index.html#warning-mentorship-is-not-the-solution",
    "title": "The Real Reason Women Quit Tech (and How to Address It)",
    "section": "Warning: Mentorship is NOT the solution",
    "text": "Warning: Mentorship is NOT the solution\nOn the surface, mentorship seems like it could be a good way to help people advance their careers, but the data shows this is only true for men, because mentors provide different things for men than for women. A study of 4,000 women and men who graduated from top MBA programs (surveyed in 2008 and again in 2010) found that when women receive mentorship, it’s advice on how they should change and gain more self-knowledge. When men receive mentorship, it’s public endorsement of their authority and concrete steps to take charge and make career moves. Guess which one is more helpful? Men who received mentorship were statistically more likely to be promoted, but that was not true for women who were mentored.\nThe study found that women were more likely than men to have mentors; however the women were also in lower-level positions, paid less, less satisfied, and less likely to be promoted than the men. Sponsorship is when a mentor goes beyond giving advice to using their influence with senior executives to advocate for the mentee. With a sponsor, women in tech are 70% more likely to have their ideas endorsed, 119% more likely to see their ideas developed, and 200% more likely to see their ideas implemented, compared to women without a sponsor."
  },
  {
    "objectID": "posts/2016-10-04-real-reason/index.html#pipeline-to-senior-roles",
    "href": "posts/2016-10-04-real-reason/index.html#pipeline-to-senior-roles",
    "title": "The Real Reason Women Quit Tech (and How to Address It)",
    "section": "Pipeline to Senior Roles",
    "text": "Pipeline to Senior Roles\nA few years ago, women comprised only 10% of the leadership and staff engineer roles at Intuit despite making up 24% of the engineering team, and women were 3x less likely to be ready for promotion than their male counterparts. In response, Intuit created a talent pipeline for senior roles: director, manager of managers, architects, and principal engineer.\nVinay Pai, VP of Engineering at Intuit, asked directors, architects, and principal engineers to identify promising women managers and women staff engineers, and to provide mentorship with the explicit goal of helping prepare them for promotions to more senior roles. After 1 year, the promotion readiness gap between men and women had been eliminated, several women were promoted, retention increased, and employee engagement (of both men and women) was higher. Although Intuit’s program was only about promoting women, it should be expanded to other underrepresented groups to avoid “colorless diversity” that primarily benefits White women.\n\n\n\nPhoto from #WOCinTech Chat\n\n\nResearchers at Deutsche Bank hypothesized that women managing directors were leaving the firm to work for competitors because they were seeking greater work/life balance. However, they discovered instead that women were leaving because they were being offered higher ranking jobs by competitors that they weren’t being considered for internally. Deutsche bank responded by creating a sponsorship program to pair executive committee members with women, and one third of participants moved to larger roles at the bank within a year."
  },
  {
    "objectID": "posts/2016-10-04-real-reason/index.html#the-wage-gap-is-real",
    "href": "posts/2016-10-04-real-reason/index.html#the-wage-gap-is-real",
    "title": "The Real Reason Women Quit Tech (and How to Address It)",
    "section": "The Wage Gap is Real",
    "text": "The Wage Gap is Real\nThere are gender and racial pay gaps in tech, and things are worst for women of color, who experience the compound effects of both. A study by the American Institute for Economic Research found that on average Latina women software developers earn 20% less annually than White men. Latina, Asian, and Black women software developers all earn less than White, Black, and Asian men, and less than White women (Latino men are an exception, earning less than any other group except for Latina women). Another study found that just 1 year out of college, women make 88% of what men do in engineering and 77% of what men do in computer and information science (this gap will increase over time with the power of compounding).\nSome people dismiss the gender pay gap by arguing that women choose lower-paying fields; however, the causality is opposite: pay drops as women move into a previously male-dominated field. A study by Stanford and UPenn researchers analyzed 164 occupations using 50 years of US Census data (1950–2000) and found consistent evidence that pay drops as women move into a previously male-dominated field in areas as varied as recreation, ticket agent, designers, housekeepers, and biologists. When a job attracts more men, such as computer programming, which used to be predominately female, wages rise.\nAnother argument is that women earn less since they are less likely to negotiate; however a study of 4,600 employees across 840 workplaces found that women ask for pay raises at the same rate as men, but men are 25% more likely to receive yes as an answer (the study uses data from Australia, which is the only country in the world to collect this data. Maybe data-driven American tech companies could collect this data too…) Furthermore, a Harvard and CMU study found that when men and women negotiated a job offer by reading identical scripts, women who asked for a higher salary were rated as being more difficult to work with and less nice, but men were not perceived negatively for negotiating.\n\n\n\nPhoto from #WOCinTech Chat\n\n\nMIT Professor Emilio Castilla studies the culture of meritocracy and has found that organizations that promote meritocracy actually show greater bias in favor of men over equally performing women. When Professor Castilla studied a large private company, he found that women, ethnic minorities and employees not born in the U.S. received lower bonuses than White men with the same performance evaluations, working in the same job and in the same unit for the same manager. That is ridiculous enough that I need to repeat it: employees with the same performance evaluation scores, same job, same unit, and same manager were receiving different bonuses!\nAfter Microsoft publicized that it is paying men and women at the same level equally, engineer Leigh Honeywell pointed out that this is only part of the story since it’s not accompanied by promotion velocities and retention numbers (data which Microsoft has, but has not released). If women are not getting promoted as quickly as men, they are effectively earning less for the same experience level. Some possible reasons women may be getting promoted more slowly are biased performance reviews, less access to innovative projects, less visibility one level up, and pattern matching (leaders look for people who remind them of themselves). Equal pay is necessary but insufficient. Tech companies need to release the comparative rates at which people at their companies receive promotions, broken down by gender and race, as part of their diversity reports."
  },
  {
    "objectID": "posts/2016-10-04-real-reason/index.html#womens-ideas-arent-heard-until-a-man-suggests-them",
    "href": "posts/2016-10-04-real-reason/index.html#womens-ideas-arent-heard-until-a-man-suggests-them",
    "title": "The Real Reason Women Quit Tech (and How to Address It)",
    "section": "Women’s ideas aren’t heard until a man suggests them",
    "text": "Women’s ideas aren’t heard until a man suggests them\nA study from Harvard, Wharton, and MIT found that men’s voices are perceived as more persuasive, fact-based, and logical than women’s voices, even when they are saying the exact same sentences. Many women have had the experience of their idea being ignored, only for a man to suggest the same idea later and receive praise and credit for it.\nWomen in tech are less likely to get their ideas green-lighted for development than men (30% vs 37%) and a study of 1,800 professionals found that without diverse leadership, women are 20% less likely than straight white men to win endorsement for their ideas, people of color are 24% less likely, and LGBT people are 21% less likely.\n\n\n\nPhoto from #WOCinTech Chat\n\n\nA recent article shared a strategy used by women in meetings at the White House: “When a woman made a key point, other women would repeat it, giving credit to its author. This forced the men in the room to recognize the contribution — and denied them the chance to claim the idea as their own.” If the large number of women who posted, shared, liked, and tweeted the article is any indication, this is a problem and a solution that resonates with many. While the article was specifically about women supporting each other, men can do this too. Listen closely at meetings, and repeat unacknowledged good ideas, giving the person who first had the idea credit."
  },
  {
    "objectID": "posts/2016-10-04-real-reason/index.html#women-dont-receive-the-high-visibility-assignments-necessary-for-earning-promotions",
    "href": "posts/2016-10-04-real-reason/index.html#women-dont-receive-the-high-visibility-assignments-necessary-for-earning-promotions",
    "title": "The Real Reason Women Quit Tech (and How to Address It)",
    "section": "Women don’t receive the high-visibility assignments necessary for earning promotions",
    "text": "Women don’t receive the high-visibility assignments necessary for earning promotions\nTwo Stanford Professors ran a study in which they met with 240 senior leaders of a Silicon Valley tech company. These leaders identified visibility as the most critical factor for getting promoted to their high level: more important than technical competence, business results, or team leadership ability (these other attributes were all seen through a lens of visibility). The researchers observed dozens of hours of performance calibration reviews and concluded that women are at a distinct disadvantage.\nThe researchers found that men with a highly visible leadership style were described positively as “crushing it” or “killing it,” whereas women with a highly visible leadership style were criticized for being “abrasive” or “running over people.” Women and men with more collaborative leadership styles weren’t recognized as leaders, so this is a lose-lose situation for women.\n\n\n\nPhoto from #WOCinTech Chat\n\n\nWomen in the focus groups reported being turned down when they requested big opportunities. A typical comment was, “There are times where you are discouraged from taking on a stretch assignment. The manager says, ‘This will require extra hours, and you have to think about your family. This is not something for you.’ I have had that happen to me, and these were experiences needed for a promotion.”\nEven if you aren’t in a position of power, you can still praise the underrepresented minorities you work with to other coworkers, or suggest how you think X would be great at applying skill Y to high impact problem Z. If you are a leader, make sure that you are assigning underrepresented minorities to high-impact, high-visibility projects."
  },
  {
    "objectID": "posts/2016-10-04-real-reason/index.html#the-cost-of-untrained-managers",
    "href": "posts/2016-10-04-real-reason/index.html#the-cost-of-untrained-managers",
    "title": "The Real Reason Women Quit Tech (and How to Address It)",
    "section": "The Cost of Untrained Managers",
    "text": "The Cost of Untrained Managers\nIn a survey of 21 high-tech companies, technical women rated their supervisors lowest for communication skills, receptiveness to suggestions, availability, and feedback, compared to technical men and non-technical men and women. Technical women were less likely to agree that management decisions were fair, that management trusted their judgement, that performance evaluations were fair, or that it was safe to speak up, compared to every other subgroup. I once had a manager who was dismissive of or denied everything I said. Even in my exit interview, he denied that my reasons for quitting were actually issues at the company.\nIn the tech industry, many managers receive little or no training, and people are frequently promoted to manager because of their technical skills, not their people or leadership skills. Don’t underestimate the damage that an unskilled manager can cause. Many managers are completely unaware of the research science on motivation (autonomy, mastery, and purpose have been proven as the best motivators) or of unconscious bias and how it impacts performance reviews. You must provide training to managers.\nStanford researchers ran a gendered comparison of written performance reviews from 3 high-tech companies and 1 professional services firm. The researchers found that feedback to men contained more granular detail and actionable advice, whereas feedback to women was vague (e.g. “You had a great year” was a common one). When women did receive specific developmental feedback, it was overly focused on their communication style (e.g. “her speaking style is off-putting” or “too aggressive”). In contrast, men received actionable feedback linked to business outcomes, such as “You need to deepen your domain knowledge in the X space- once you have that understanding, you will be able to contribute to the design decisions that impact the customer.” Lack of relevant, actionable feedback holds women back from making the changes they need to advance their careers. Another study of 248 tech performance reviews found a strong gender bias; men received constructive criticism on skills they should develop, whereas women received personality criticism (“pay attention to your tone”)."
  },
  {
    "objectID": "posts/2016-10-04-real-reason/index.html#action-steps",
    "href": "posts/2016-10-04-real-reason/index.html#action-steps",
    "title": "The Real Reason Women Quit Tech (and How to Address It)",
    "section": "Action Steps",
    "text": "Action Steps\nHere are a few concrete steps that will improve diversity at your company. Please choose at least one of these and commit to take action on it this week:\n\nThink of 1 person from an underrepresented group at your workplace whose work you admire, and think of 1 way to champion their work: advocating for their ideas, praising them to leadership at your company, or recommending them for an interesting, high-impact work project.\nSet up a sponsorship program to help people from underrepresented groups prepare for promotions.\nTrain managers to give women fair and actionable performance reviews.\nIn meetings, listen to women and repeat unacknowledged good ideas, giving the originator credit.\nAnalyze your company’s data on performance reviews, salaries, and promotions. Correct any salary inequalities. If there are differences between groups in performance reviews or promotion rates, investigate why.\n\nIn general, be on the lookout for ways you can use whatever power or status you have to champion someone who doesn’t remind you of yourself.\nThis post is part 2 in a series. Part 1 discusses bullshit diversity strategies and offers some better ideas. Part 3 covers how to overhaul your interview process.\nMy post debunking the pipeline myth shares my personal story of wanting to leave the tech industry, as well as practical tips for change. My post on “diversity washing” shows the research that the wrong diversity initiatives can actually make things worse for women and people of color."
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html",
    "href": "posts/2016-10-08-teaching-philosophy/index.html",
    "title": "Providing a Good Education in Deep Learning",
    "section": "",
    "text": "Paul Lockhart, a Columbia math PhD, former Brown professor, and K-12 math teacher, writes in the influential essay A Mathematician’s Lament of a nightmare world where children are not allowed to listen to or play music until they have spent over a decade mastering music notation and theory, spending classes transposing sheet music into a different key. In art class, students study colors and applicators, but aren’t allowed to actually paint until college. Sound absurd? This is how math is taught–we require students to spend years doing rote memorization, and learning dry, disconnected “fundamentals” that we claim will pay off later, long after most of them quit the subject.\nUnfortunately, this is where several of the few resources on deep learning begin–asking learners to follow along with the definition of the Hessian and theorems for the Taylor approximation of your loss function, without ever giving examples of actual working code. I’m not knocking calculus. I love calculus and have even taught it at the college level, but I don’t think it’s a good or helpful introduction to deep learning.\nSo many students are turned off to math because of the dull way it’s taught, with tedious, repetitive exercises, and a curriculum that saves so many fun parts (such as graph theory, counting and permutations, group theory) to so late that everyone except math majors has abandoned the subject. And the gate-keepers of deep learning are doing something similar whenever they ask that you can derive the multivariate chain rule or give the theoretical underpinnings of KL divergence before they’ll teach you how to use a neural net to handle your own projects.\nWe’ll be leveraging the best available research on teaching methods to try to fix these problems with technical teaching, including:\nIn the end, what we’re talking about is good education. That’s what we most care about. Here are more of our thoughts on good education:"
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html#good-education-starts-with-the-whole-game",
    "href": "posts/2016-10-08-teaching-philosophy/index.html#good-education-starts-with-the-whole-game",
    "title": "Providing a Good Education in Deep Learning",
    "section": "Good education starts with “the whole game”",
    "text": "Good education starts with “the whole game”\nJust as kids have a sense of what baseball is before they start batting practice, we want you to have a sense of the big picture of deep learning well before you study calculus and the chain rule. We’ll move from the big picture down to the details (which is the opposite direction than most education, which tries to teach all the individual elements before putting them together). For a good example of how this works, watch Jeremy’s talk on recurrent neural networks–he starts with 3 line RNN using a highly featured library, then removes the library builds his own architecture using a GPU framework, and then removes the framework and builds everything from scratch in gritty detail just using basic python.\nIn a book that inspires us, David Perkins, a Harvard education professor with a PhD from MIT in Artificial Intelligence, calls the approach of not doing anything complicated until you’ve taught all the individual elements first a disease: “elementitis”. It’s like batting practice without knowing what the game baseball is. The elements can seem boring or pointless when you don’t know how they fit in with the big picture. And it’s hard to stay motivated when you’re not able to work on problems you care about, or have a sense of how the technical details fit into the whole. Perhaps this is why studies have shown that the intrinsic motivation of school children steadily declines from 3rd grade to 8th grade (the only range of years studied)."
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html#good-education-equips-you-to-work-on-the-questions-you-care-about",
    "href": "posts/2016-10-08-teaching-philosophy/index.html#good-education-equips-you-to-work-on-the-questions-you-care-about",
    "title": "Providing a Good Education in Deep Learning",
    "section": "Good education equips you to work on the questions you care about",
    "text": "Good education equips you to work on the questions you care about\nWhether you’re excited to identify if plants are diseased from pictures of their leaves, auto-generate knitting patterns, diagnose TB from x-rays, or determine when a raccoon is using your cat door, we will get you using deep learning on your own problems (via pre-trained models from others) as quickly as possible, and then will progressively drill into more details. You’ll learn how to use deep learning to solve your own problems at state-of-the-art accuracy within the first 30 minutes of the first lesson! There is a pernicious myth out there that you need to have computing resources and datasets the size of those at Google to be able to do deep learning, and it’s not true."
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html#good-education-is-not-overly-complicated.",
    "href": "posts/2016-10-08-teaching-philosophy/index.html#good-education-is-not-overly-complicated.",
    "title": "Providing a Good Education in Deep Learning",
    "section": "Good education is not overly complicated.",
    "text": "Good education is not overly complicated.\nHave you watched Jeremy implement modern deep learning optimization methods in Excel? If not, go watch it (starts at 4:50min in the video) and come back. This is often considered a complex topic, yet after weeks of work Jeremy figured out how to make it so easy it seems obvious. If you truly understand something, you can explain it in an accessible way, and maybe even implement it in Excel! Complicated jargon and obtuse technical definitions arise out of laziness, or when the speaker is unsure of the meat of what they’re saying and hides behind their peripheral knowledge."
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html#good-education-is-inclusive.",
    "href": "posts/2016-10-08-teaching-philosophy/index.html#good-education-is-inclusive.",
    "title": "Providing a Good Education in Deep Learning",
    "section": "Good education is inclusive.",
    "text": "Good education is inclusive.\nIt doesn’t put up any unnecessary barriers. It doesn’t make you feel bad if you didn’t start coding at age 12, if you have a non-traditional background, if you can’t afford a mac, if you’re working on a non-traditional problem, or if you didn’t go to an elite college. We want our course to be as accessible as possible. I care deeply about inclusion, and spent months researching and writing each of my widely read articles with practical tips on how we can increase diversity in tech, as well as spending a year and a half teaching full-stack software development to women full-time. Currently deep learning is even more homogenous than tech in general, which is scary for such a powerful and impactful field. We are going to change this."
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html#good-education-motivates-the-study-of-underlying-technical-concepts.",
    "href": "posts/2016-10-08-teaching-philosophy/index.html#good-education-motivates-the-study-of-underlying-technical-concepts.",
    "title": "Providing a Good Education in Deep Learning",
    "section": "Good education motivates the study of underlying technical concepts.",
    "text": "Good education motivates the study of underlying technical concepts.\nHaving a big picture understanding gives you more of a framework to place the fundamentals in. Seeing what deep learning is capable of and how you can use it is the best motivation for the more dry or tedious parts.\n“Playing baseball is more interesting than batting practice, playing pieces of music more interesting than practicing scales, and engaging in some junior version of historical or mathematical inquiry more interesting than memorizing dates or doing sums,” writes Perkins. Building a working model for a problem that interests you is more interesting than writing a proof (for most people!)"
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html#good-education-encourages-you-to-make-mistakes.",
    "href": "posts/2016-10-08-teaching-philosophy/index.html#good-education-encourages-you-to-make-mistakes.",
    "title": "Providing a Good Education in Deep Learning",
    "section": "Good education encourages you to make mistakes.",
    "text": "Good education encourages you to make mistakes.\nIn the most viewed TED talk of all time, education expert Sir Ken Robinson argues that by stigmatizing mistakes, our school systems destroy the children’s innate creative capacity. “If you’re not prepared to be wrong, you’ll never end up with anything original,” says Robinson.\nTeaching deep learning with a code-heavy approach in interactive Jupyter notebooks is a great setup for trying lots of things, making mistakes, and easily changing what you’re doing."
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html#good-education-leverages-existing-resources",
    "href": "posts/2016-10-08-teaching-philosophy/index.html#good-education-leverages-existing-resources",
    "title": "Providing a Good Education in Deep Learning",
    "section": "Good education leverages existing resources",
    "text": "Good education leverages existing resources\nThere is no need to reinvent teaching materials where good ones already exist. If you need to brush up on matrix multiplication, we’ll refer you to Khan Academy. If you’re fascinated by X and want to go deeper, we’ll recommend you read Y. Our goal is to help you achieve your deep learning goals, not to be the sole resource in getting you there."
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html#good-education-encourages-creativity",
    "href": "posts/2016-10-08-teaching-philosophy/index.html#good-education-encourages-creativity",
    "title": "Providing a Good Education in Deep Learning",
    "section": "Good education encourages creativity",
    "text": "Good education encourages creativity\n\n\n\nLockhart quote\n\n\nLockhart argues that it would be better to not teach math at all, then to teach such a mangled form of it that alienates most of the population from the beauty of math. He describes math as a “rich and fascinating adventure of the imagination” and defines it as “the art of explanation”, although it is rarely taught that way.\nThe biggest wins for deep learning will come when you apply it to the outside domains you’re an expert in and the problems you’re passionate about. This will require you to be creative."
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html#good-education-teaches-you-to-ask-questions-not-just-to-answer-them",
    "href": "posts/2016-10-08-teaching-philosophy/index.html#good-education-teaches-you-to-ask-questions-not-just-to-answer-them",
    "title": "Providing a Good Education in Deep Learning",
    "section": "Good education teaches you to ask questions, not just to answer them",
    "text": "Good education teaches you to ask questions, not just to answer them\nEven those who seem to thrive under traditional education methods are still poorly served by them. I received a mostly traditional approach to education (although I had a few exceptional teachers at all stages and particularly at Swarthmore). I excelled at school, aced exams, and generally enjoyed learning. I loved math, going on to earn a math PhD at Duke University. While I was great at problem sets and exams, this traditional approach did me a huge disservice when it came to preparing me for doctoral research and my professional career. I was no longer being given well-formulated, appropriately scoped problems by teachers. I could no longer learn every incremental building block before setting to work on a task. As Perkins writes about his struggles with finding a good dissertation topic, I too had learned how to solve problems I was given, but not how to find and scope interesting problems on my own. I now see my previous academic successes as a weakness I’ve had to overcome professionally. When I began studying deep learning, I enjoyed reading the math theorems and proofs, but this didn’t actually help me build deep learning models."
  },
  {
    "objectID": "posts/2016-10-08-teaching-philosophy/index.html#good-education-is-evidence-based",
    "href": "posts/2016-10-08-teaching-philosophy/index.html#good-education-is-evidence-based",
    "title": "Providing a Good Education in Deep Learning",
    "section": "Good education is evidence-based",
    "text": "Good education is evidence-based\nWe love data and the scientific method, and we are interested in techniques that have been supported by research.\nSpaced repetition learning is one such evidence-backed technique, where learners revisit a topic periodically, just before they would forget it. Jeremy used this technique to obtain impressive results in teaching himself Chinese. The whole game method of learning dovetails nicely with spaced repetition learning in that we will revisit topics, going into more and more low level details each time, but always returning to the big picture."
  },
  {
    "objectID": "posts/2016-11-17-not-all-the-same/index.html",
    "href": "posts/2016-11-17-not-all-the-same/index.html",
    "title": "Please don’t say ‘It used to be called big data and now it’s called deep learning’",
    "section": "",
    "text": "At the Financial Times-Nikkei conference on The Future of AI, Robots, and Us a few weeks ago, Andreessen Horowitz partner Chris Dixon spoke just before Jeremy Howard and I were on stage. Dixon said many totally reasonable things in his talk–but it’s no fun to comment on them, so I’m going to focus on something rather unreasonable that he said, which was: “A few years ago it was called big data and then it was machine learning and now it’s called deep learning”. It was not entirely clear if he was saying that these are all terms for the same thing (they are most definitely not!) or if he was suggesting that the “in” data-driven approach changes from year to year. Either way, this obscures what a complete game-changer deep learning is. It is not just the 2016 version of “big data” (which has always been an empty buzzword). It is going to have an impact the size of the impact of the internet, or as Andrew Ng suggests, the impact of electricity. It is going to affect every industry, and leaders of every type of organization are going to be wishing that they had looked into it sooner.\n\n\n\nJeremy and I speaking at the Financial-Nikkei Times conference\n\n\nFirst, to clear up some terms:\nBig Data: This was an empty marketing term that falsely convinced many people that the size of your data is what matters. It also cost companies huge sums of money on Hadoop clusters they didn’t actually need. Vendors of these clusters did everything they could to maintain momentum on this nonsense because when CEOs believe it’s the size of your hardware that counts, it’s a very profitable situation if you make, sell, install, or service that hardware…\nFrancois Chollet, creator of the popular deep learning library Keras and now at Google Brain, has an excellent tutorial entitled Building powerful image classification models using very little data in which he trains an image classifier on only 2,000 training examples. At Enlitic, Jeremy Howard led a team that used just 1,000 examples of lung CT scans with cancer to build an algorithm that was more accurate at diagnosing lung cancer than a panel of 4 expert radiologists. The C++ library Dlib has an example in which a face detector is accurately trained using only 4 images, containing just 18 faces!\n\n\n\nFace Recognition with Dlib\n\n\nMachine Learning: Machine learning is the science of getting computers to act without being explicitly programmed. For instance, instead of coding rules and strategies of chess into a computer, the computer can watch a number of chess games and learn by example. Machine learning encompasses a wide variety of algorithms.\nDeep Learning: Deep learning refers to many-layered neural networks, one specific class of machine learning algorithms. Deep learning is achieving unprecedented state of the art results, by an order of magnitude, in nearly all fields to which it’s been applied so far, including image recognition, voice recognition, and language translation. I personally think deep learning is an unfortunate name, but that’s no reason to dismiss it. If you studied neural networks in the 80s and are wondering what has changed since then, the answer is the development of: - Using multiple hidden layers instead of just one. (Even though the Universal Approximation Theorem shows that it’s theoretically possible to just have one hidden layer, it requires exponentially more hidden units, which means exponentially more parameters to learn.) - GPGPU, programmable libraries for GPUs that allow them to be used for applications other than video games, resulting in orders of magnitude faster training and inference for deep learning - A number of algorithmic tweaks (especially the Adam optimizer, ReLU activation functions, batch normalization, and dropout) that have made training faster and more resilient - Larger datasets– although this has been a driver of progress, it’s value is often over-emphasized, as the “little data” examples above show.\nAnother common misconception Chris Dixon stated was that deep learning talent is incredibly scarce, and it will take years for graduate programs at the top schools to catch up to the demand. Although in the past a graduate degree from one of just a handful of schools was necessary to become a deep learning expert, this is a completely artificial barrier and no longer the case. As Josh Schwartz, chief of engineering and data science at Chartbeat, writes in the Harvard Business Review, “machine learning is not just for experts”. There has been a proliferation of cutting-edge commercially usable machine learning frameworks, machine learning specific services being released from major cloud providers Amazon and Google, tutorials, publicly released code, and publicly released data sets.\nWe are currently in the middle of teaching 100 students deep learning from scratch, with the only prerequisite being one year of programming experience. This will be turned into a MOOC shortly after the in-person class finishes. We’re in the 4th week of the course, and already the students are building world-class image recognition models in Python.\nIt is far better to take a domain expert within your organization and teach them deep learning, than it is to take a deep learning expert and throw them into your organization. Deep learning PhD graduates are very unlikely to have the wide range of relevant experiences that you value in your most effective employees, and are much more likely to be interested in solving fun engineering problems, instead of keeping a razor-sharp focus on the most commercially important problems. In our experiences across many industries and many years of applying machine learning to a range of problems, we’ve consistently seen organizations under-appreciate and under invest in their existing in-house talent. In the days of the big data fad, this meant companies spent their money on external consultants. And in these days of the false “deep learning exclusivity” meme, it means searching for those unicorn deep learning experts, often including paying vastly inflated sums for failing deep learning startups."
  },
  {
    "objectID": "posts/2016-12-08-org-structure/index.html",
    "href": "posts/2016-12-08-org-structure/index.html",
    "title": "How should you structure your Data Science and Engineering teams?",
    "section": "",
    "text": "I sometimes receive emails asking for guidance related to data science, and I’m going to start sharing my answers here as a data science advice column. Note that questions may be edited for clarity or brevity.\nQ: Hello Rachel, I’m VP of Engineering at a start-up that is increasingly seeing our data & ML algorithms as our core asset. In thinking about the next few technical hires we want to make, we want to target engineers that will be able to accelerate the efforts of our Data Science team, so I’m trying to do some pre-recruiting research to understand how engineering teams focused on support of production ML pipelines are structured. Some of what I’m wondering about:\nA: This answer is based on my experience as a data scientist, my experience interviewing for data science roles, and talking with a number of data scientists. I’ve watched employers go through multiple data science re-orgs.\nThere are a lot of potential pitfalls related to data science and org structure (no matter what you choose). I’m going to take the liberty of expanding your question to cover the relationship between data science and other teams, as well as data engineering. Consider these scenarios:"
  },
  {
    "objectID": "posts/2016-12-08-org-structure/index.html#recommendations",
    "href": "posts/2016-12-08-org-structure/index.html#recommendations",
    "title": "How should you structure your Data Science and Engineering teams?",
    "section": "Recommendations",
    "text": "Recommendations\nHaving data scientists all on a separate team makes it nearly impossible for their work to be appropriately integrated with the rest of the company. Have your data scientists distributed throughout the company, but also have a team doing data science evangelism within the company. Vertical product teams need to know what is possible and how to best utilize data science. It’s too hard for a lone data scientist to advocate for the role of data-driven decisions within the team they’re embedded in.\nData scientists should report to both a data science manager and a manager within the product team. You need a lot of communication: to make sure that the team is getting the most value and that the data scientist is finding fulfilling work. One approach that can work really well is to have half the data scientists switch to a different group each year (or even more often).\nWhile it’s common to have machine learning, engineering, and data/pipeline/infrastructure engineering all as separate roles, try to avoid this as much as possible. This leads to a lot of duplicate or unused work, particularly when these roles are on separate teams. You want people who have some of all these skills: can build the pipelines for the data they need, create models with that data, and put those models in production. You’re not going to be able to hire many people who can do all of this. So you’ll need to provide them with training. In general, the most underused resource of most companies is their own employees, and the situation is even worse with data scientists (since “data science” encompasses such a wide variety of possible skills). Tech companies waste their employees’ potential by not offering enough opportunities for on-the-job learning, training, and mentoring. Your people are smart and eager to learn. Be prepared to offer training, pair-programming, or seminars to help your data scientists fill in skills gaps. I always tell students who are interested in both data science and engineering, that the more you know about software development, the better a data scientist you will be.\nEven when you have people who are both data scientists and engineers (that is, they can create machine learning models and put those models into production), you still need to have them embedded in other teams and not cordoned off together. Otherwise, there won’t be enough institutional understanding and buy-in of what they’re doing, and their work won’t be as integrated as it needs to be with other systems.\nThe term data scientist refers to at least 5 distinct jobs, so communication and clarity is key. Companies need to be clear on what their needs are and what they’re hiring for. I can tell you from firsthand experience as a job applicant that lots of companies want to hire a data scientist but aren’t sure why, or how they will use data science. You want to hire someone who is interested in the role they’d be working in. You probably won’t find a candidate that’s both interested in writing machine learning implementations in C and extensively using Google Analytics, although that is a real job description I’ve encountered. Note I say “interested in” and not “already has the skills”. Assume that any applicant will be learning lots of new skills on the job (if not, they will soon grow bored).\nFurther reading: After drafting this post, I came across an excellent article called The Data Science Delusion which details several additional problems companies may encounter when incorporating data science into their org."
  },
  {
    "objectID": "posts/2017-03-05-interviews/index.html",
    "href": "posts/2017-03-05-interviews/index.html",
    "title": "How to Make Tech Interviews a Little Less Awful",
    "section": "",
    "text": "Everything about the current interview process in tech is broken. I suspect that, no matter what, being evaluated and making judgements on others for a decision that has a big impact on you both is never going to be fun — especially since both of you only have a limited amount of time for the process. However, I think there is plenty of room to make tech interviews slightly less awful than they currently are.\nWhat is your goal in interviewing candidates? If you answered “to hire the best candidate” you should reconsider. Forming the best team is a goal that will better serve your company. Will finding candidates with the same strengths and same background as yourself add a lot to your company? Or might these similar candidates also have the same weaknesses, same blind spots, and same skills gaps that you have? What research on team performance should you be using when deciding who and how to hire?"
  },
  {
    "objectID": "posts/2017-03-05-interviews/index.html#smartest-individuals-smartest-team",
    "href": "posts/2017-03-05-interviews/index.html#smartest-individuals-smartest-team",
    "title": "How to Make Tech Interviews a Little Less Awful",
    "section": "Smartest Individuals != Smartest Team",
    "text": "Smartest Individuals != Smartest Team\nA team of CMU and MIT researchers published a series of studies in Science showing that the smartest individuals did not result in the smartest teams, and that individual IQs did not map directly to the collective intelligence of a team. The smartest teams had the following 3 characteristics:\n\nmembers contributed more equally rather than letting 1 or 2 people dominate\nmembers scored higher at reading complex emotional states from people’s faces\nhad more women. That’s right, having more women on a team predicted that the team would perform better.\n\nEven if you could gather a team consisting of only those mythical “10x rockstar ninjas,” it would perform worse than a team of qualified women who are good at reading emotional states.\nSo, what do we know about how hiring tends to work in practice — are companies behaviours in line with this research? The answer, as we’ll see, is 100% “no”!"
  },
  {
    "objectID": "posts/2017-03-05-interviews/index.html#people-like-to-hire-people-like-them",
    "href": "posts/2017-03-05-interviews/index.html#people-like-to-hire-people-like-them",
    "title": "How to Make Tech Interviews a Little Less Awful",
    "section": "People like to hire people like them",
    "text": "People like to hire people like them\nTriplebyte, a technical recruiting company, conducts technical interviews and then sends the best candidates on to interview at the most prestigious startups. They have collected extensive data, both from over 300 technical interviews and how those candidates fare in earning job offers. The #1 finding from Triplebyte’s research is that “the types of programmers that each company looks for often have little to do with what the company needs or does. Rather, they reflect company culture and the backgrounds of the founders.”\nTriplebyte’s key advice for job seekers is to read the bios of founders and apply to companies where the CTO shares your background. Since only 3% of VC funding goes to women and less than 1% goes to Black founders, this advice will be hard to follow for applicants from underrepresented groups or with non-traditional backgrounds. How rare is it for a founder to think that a Black woman candidate reminds him of himself? Before you suggest that the solution is just for women and people of color to found more companies, remember that investors prefer ideas pitched by a man more than an identical pitch from a woman, and that out of funded companies, those with a Black women founder raise an average of just $36,000 compared to an average of $1.3 million for companies founded by White men.\nPeople think that being smart means knowing the same things they know. Interviewers don’t realize that they’re bad at judging how much a question tests intelligence and how much it tests non-essential familiarity with particular concepts from the interviewer’s expertise. For crafting interview questions, this means that you must get an engineer with a very different background from the person who created a question to vet that question and confirm how difficult it would be for someone without a background in [insert non-essential framework or concept].\nI was once on a team where the manager frequently ignored the team’s feedback from interviews to unilaterally make hiring decisions. The only pattern I could find was that he liked candidates that were similar to himself, and had a demoralizing effect on team morale to have our feedback ignored."
  },
  {
    "objectID": "posts/2017-03-05-interviews/index.html#false-negatives",
    "href": "posts/2017-03-05-interviews/index.html#false-negatives",
    "title": "How to Make Tech Interviews a Little Less Awful",
    "section": "False Negatives",
    "text": "False Negatives\nPowerful voices in tech have given the advice to “avoid false positives” (accidentally hiring employees who turn out not to be good) at all costs, even though this will result in more false negatives (accidentally rejecting candidates who would’ve been great). Steve Yegge, senior staff engineer and manager at Google for 11 years, writes, “Google has a well-known false negative rate, which means we sometimes turn away qualified people, because that’s considered better than sometimes hiring unqualified people. This is actually an industry-wide thing, but the dial gets turned differently at different companies. At Google the false-negative rate is pretty high.” Max Levchin, cofounder and former CTO of PayPal, says of the early days of PayPal, “There are some legendary-ish tales of me not hiring people because they used the wrong word in an interview… I’m sure we had lots of false negatives, but we have very few false positives.”\nJoel Spolsky, cofounder of Stack Overflow, echoes this advice, adding “if you reject a good candidate, I mean, I guess in some existential sense an injustice has been done, but, hey, if they’re so smart, don’t worry, they’ll get lots of good job offers.” This would be reassuring if false negatives were random and nobody experienced more than a few. However, if an entire industry’s interview processes are biased against a particular group of people, members of that group will have a hard time getting hired anywhere, regardless of how talented they are. Consider false negatives in the light of the following two stories about the job searches of a Black engineer and a trans engineer.\n\nJustin Webb, a software engineer with a decade of experience, who had just graduated from Hack Reactor (the most selective coding academy, and one that boasts a 99% job placement rate) was the only Black student in his class and the only one who couldn’t find a job within 6 months of graduating. Webb describes his interview process at GitHub as typical of what he faced at many companies. At one point, what he believed to be the final interview for the job turned out to just be the 4th round in a 6-step process. Github eventually rejected him, saying that they didn’t see him as a good fit, despite his having passed their most recent interview challenge. “If the way I passed the test wasn’t right enough, I could’ve learned the right way to get the answers right,” Webb said. “But hearing ‘right’ wasn’t right enough was frustrating.”\nFebruary Keeney, an engineering manager at GitHub and a trans woman, writes “my career has become an A/B Test in gender. With the clear ‘winner’ being male.” For the first 15 years of her career in tech, she presented as male and received an offer every single time she had an on-site interview. After she transitioned, she spent a year job searching, receiving numerous rejections. Keeney reports that mid-transition, “Finally, one day, I gave up. I went to an interview without nail polish, no lip gloss. I presented as male as possible. Lo and behold: I got an offer.”\n\nKeeney recounts, “On a couple of occasions, I noticed a clear antagonistic shift when the interviewer realized I was trans. The questions got unfairly difficult and the tone more deeply interrogatory. It is not hard to ensure a candidate does poorly on an interview if you are really determined to undermine them.” She writes of the frustration of interviews that went well, where there were multiple rounds and onsites, and then finally at the end she would be rejected based on something that was discussed as a non-issue in the very first phone screen.\nIt’s easy for Spolsky to say that false negatives are not a big deal, but brutal for Webb or Keeney to endure a discouraging 12 months of job searching, receiving surprising rejections after interviews that went well. Companies with biased interview processes are not aware of how many great Black, Latino, and trans people they are rejecting who would have made excellent employees, and everybody is losing out.\nIf at all possible, track the people that you reject to see if any of those rejections were false negatives. Trek Glowacki, a software engineer at marketing startup Popular Pays, wrote “I’ve been twitter following the careers of people we interviewed but passed on at my last gig. Turns out we were almost always wrong. We had a group called ‘Bar Raisers’ who mainly torpedoed candidates that lacked ‘CS Fundamentals’. We passed on so many good people”.\nMarco Rogers, an engineering manager at tech startup Clover Health, aptly pointed out “Sometimes the reality of privilege is as subtle as a lack of extra scrutiny when you’re trying to get a job”."
  },
  {
    "objectID": "posts/2017-03-05-interviews/index.html#clear-consistent-data-driven",
    "href": "posts/2017-03-05-interviews/index.html#clear-consistent-data-driven",
    "title": "How to Make Tech Interviews a Little Less Awful",
    "section": "Clear, Consistent, Data-Driven",
    "text": "Clear, Consistent, Data-Driven\nIt is common in tech for interviewers to all have their own pet interview question, and for each candidate for a position to be interviewed by different sets of interviewers, thus receiving different questions. At many companies, interviewers produce a simple binary hire/no-hire vote (as opposed to gathering data across several criteria). Some of the interview questions may have little, if anything, to do with the day-to-day work required for the position.\nYour interview process should follow the following four simple guidelines:\n\nResemble actual work the candidate would be doing in their job\nClear rubrics\nConsistent and standardized\nDon’t have an elite-candidate fast path\n\nLet’s look at each of these in more detail.\n\nResemble actual work the candidate would be doing in their job.\nDeveloper Yegor Bugayenko (the author of a highly rated book about object oriented programming) writes about the huge waste of time it was when Amazon flew him out for an interview that consisted of 4 hours of whiteboarding algorithm questions. If the recruiter had said, “We’re looking for an algorithm expert,” he would have declined and both he and Amazon could ha’ve avoided wasting their time. “Clearly, I’m not an expert in algorithms. There is no point in giving me binary-tree-traversing questions; I don’t know those answers and will never be interested in learning them,” Bugayenko writes.\nThomas and Erin Ptacek, formerly founders and principals at Matasano, suggest constructing a test for a web developer position by taking an application you’ve written, removing some features (such as search or the customer order update), bundling up the app with all its assets onto a virtual machine, give it to the candidate, and have them add back the feature you removed. Popular communications app Slack has built an engineering team that is more diverse than many other tech companies, and Slack Director of Engineering Julia Grace describes a similar process of a real world coding challenge as part of the Slack engineering interview. Although Matasano and Slack give these challenges as take-home assignments, I think they could also work as part of an onsite interview [see the next section for the take-home debate]. For instance, the Airbnb data science team gives candidates a project to work on during the onsite, in which they have a few hours to work by themselves but can ask questions of the team during this time.\nLever, a tech start-up that builds popular recruiting software used by Netflix, Reddit, Yelp, and Lyft, describes mock code reviews as a key part of its own interview process for software engineers. Lever engineer Zach Millman writes, “Problem-solving and getting things done are important, but ideally we’d also hire engineers who are great at API design, naming, testing, maintenance, scaling, extensibility, etc. — all of those unglamorous things that actually make software projects successful and sustainable” and that mock code reviews are designed to get at these values. Importantly, mock code reviews assess a candidate’s communication skills and prioritization.\n\n\nClear rubrics\nResearch has found that the less well-defined the criteria for a hiring decision, the more that bias and post-hoc justifications play a role in decisions. A study by Yale researchers titled “Constructed Criteria: Redefining Merit to Justify Discrimination” found that when a male candidate had more practical experience and a female candidate had more academic experience, people chose the male candidate and ranked practical experience as more important. However, when the roles were flipped (the man had more academic experience and the woman more practical experience), people still chose the male candidate and instead ranked academic experience as more important. This effect was mitigated by forcing people to decide on their criteria before looking at the applications.\nElena Grewal, interim head of data science at Airbnb, identified that women were being disproportionately weeded out during the take-home exercise part of the interview process. “We took a close look at this and realized that the people who were grading the exercise had no clear rubric, so we changed this and made it clear what we were looking for, we made the grading consistent, and if a person was successful they were moved to the next round.” Since instituting that change, the percentage of women on the data science team has doubled from 15% to 30%.\nSlack’s exercise is graded against a set of over 30 predetermined criteria. As discussed in the Yale study above, deciding on a clear scoring rubric before evaluating candidates reduces bias. The Ptaceks recommend collecting as much data as possible, and that this data be objective facts, such as: unit test coverage, algorithmic complexity, and handling a known corner case.\n\n\nConsistent and standardized\nEvery candidate should receive the same test, be asked the same questions, and should be graded in the same way. One way that Slack further attempts to remove bias and standardize the results is by hiding the name and resume of the person submitting the test from the grader. (Reminder: resumes with traditionally African-American names are viewed more negatively than resumes with stereotypically White names, and job applications with female names are viewed more negatively than job applications with male names).\nTo reduce bias, tests don’t just need to be graded consistently; the entire process of evaluating a candidate needs to be consistent. Professor Lauren Rivera of Northwestern’s Business School “played a fly on the wall during hiring meetings at one consulting firm. She found that the team paid little attention when white men blew the math test but close attention when women and Black people did. Because decision makers (deliberately or not) cherry-picked results, the testing amplified bias rather than quashed it.”\n\n\nDon’t have an elite-candidate fast path\nIt’s valuable to know what your tests say about a candidate you know you want to hire. Seeing how your elite candidates perform may help you discover that some of your tests/questions/criteria aren’t useful. An HR director at a food company discovered “that white managers were making only strangers — most of them minorities — take supervisor tests and hiring white friends without testing them.” Hiring through the friend networks of current employees is one of the primary ways that tech companies recruit, and there can be a lot of variation in interview experience depending on who at the company you know and how well you know them. Given that three-quarters of White people have no non-White friends, we can guess the race of most referrals in tech companies with predominantly White employees."
  },
  {
    "objectID": "posts/2017-03-05-interviews/index.html#over-selecting-for-confidence",
    "href": "posts/2017-03-05-interviews/index.html#over-selecting-for-confidence",
    "title": "How to Make Tech Interviews a Little Less Awful",
    "section": "Over-Selecting For Confidence",
    "text": "Over-Selecting For Confidence\nWe often mistake confidence for competence. Women perceive their abilities as being worse than they are, whereas men have an inflated sense of their abilities. This difference causes groups to be more likely to pick male leaders because of their over-confidence, compared to more qualified women who are less confident. Tech interview practices lead us to rate confident candidates more highly than less confident candidates. The solution is not just to tell women to be more confident, because women are perceived negatively for displaying traits associated with confidence in men (negotiating for themselves, speaking more in meetings)."
  },
  {
    "objectID": "posts/2017-03-05-interviews/index.html#but-isnt-performing-well-under-pressure-important",
    "href": "posts/2017-03-05-interviews/index.html#but-isnt-performing-well-under-pressure-important",
    "title": "How to Make Tech Interviews a Little Less Awful",
    "section": "But isn’t performing well “under pressure” important?",
    "text": "But isn’t performing well “under pressure” important?\nNo. Performing under adversarial pressure from co-workers is not inherently part of the job of being a developer. Even for positions that would involve frequent on-calls or racing to fix failing production systems, developers can be a collaborative team uniting around an external issue, not adversaries against their fellow co-workers. If your company has a work environment of adversarial competition or hostility, you will have trouble retaining and attracting employees and should rethink your culture.\nEven the best interview process in the world will not be enough to overcome a biased or toxic culture. This is not just a theoretical issue — even a year before the most recent revelations about Uber’s toxic culture, engineers were sharing on Medium the responses they send to recruiters declining to even interview with Uber (see Tess Rinerson’s Dear Uber Recruiter and Tara Adiseshan’s Dear Uber Recruiter Part 2, both from March 2016). Note that it is not just Uber — there are many, many companies losing applicants as women warn each other about their experiences with toxic companies."
  },
  {
    "objectID": "posts/2017-03-05-interviews/index.html#take-home-assignments-yes-or-no",
    "href": "posts/2017-03-05-interviews/index.html#take-home-assignments-yes-or-no",
    "title": "How to Make Tech Interviews a Little Less Awful",
    "section": "Take-Home Assignments: Yes or No?",
    "text": "Take-Home Assignments: Yes or No?\nThis is a highly contested question and there are reasonable arguments on both sides.\nOne of the primary concerns about work sample tests is that they can be very time-consuming, which puts people with less time (parents, those caring for elderly or sick relatives, people with health challenges, or anyone needing to work multiple jobs) at an unfair disadvantage. On the other hand, given that any recruiting process must by definition take up a significant chunk of an applicant’s time, the flexibility of a take-home assignment can be the best option for many. Furthermore, it’s likely to more closely reflect the real work that an applicant will be doing, compared to white-board coding in 45 minutes with someone looking over their shoulder making comments.\nIf you do use a take-home assignment, you should:\n\nHave a time limit (with candidates choosing when they will receive the assignment, and then agreeing to email it in 1–3 hours later) and don’t be too time-consuming. I once spent two consecutive full weekends on take-home coding assignments, while working full-time, and I felt so exhausted afterwards that I planned to take a month off from my job search (fortunately, those assignments eventually led to offers). And this was before I had a child!\nHave a reasonable screen beforehand. It’s not fair to ask someone to do hours of work if there’s little chance you’ll invite them for an onsite. I feel angry when I hear about friends spending weeks on multiple take-home challenges (for a single company) and then being told that they don’t have enough experience. That is messed up! Companies should know whether a candidate has enough experience from the resume or initial phone screen, before wasting dozens of hours of time.\nConsider paying candidates for time spent. I have the financial means to hire a babysitter for my toddler if I wanted to do a take-home assignment for a company that I was excited about, but not everyone does.\nRealize that some people will choose not to apply. I’ve decided against applying to a number of companies once I realized how long or poorly constructed the take-home assignment was.\nConsider giving people options, because personal preferences vary. I find pair-programming interviews to be the most anxiety producing (they make me very self-conscious), but many people prefer them. I enjoy whiteboarding because of my math background. If you offer candidates a choice, be sure to gather data and make sure that you are fair across challenge-type.\n\nBoth Slack’s engineering team and Airbnb’s data science teams make use of take-homes, and they are doing better at diversity hiring than most tech companies. However, these teams also have rigorously pre-defined criteria for grading, so perhaps that is the reason behind their diversity. Maybe the reason I love the Ptaceks’ blog post isn’t that they use take-homes, but that it is so infused with respect, thoughtfulness and consideration for the interviewee. Perhaps any interviewing format could work as long as it is thoughtfully constructed, involves consideration and respect for the interviewee (and their time), has clear criteria, is consistent and fair, and gathers data."
  },
  {
    "objectID": "posts/2017-03-05-interviews/index.html#diversity-inclusion",
    "href": "posts/2017-03-05-interviews/index.html#diversity-inclusion",
    "title": "How to Make Tech Interviews a Little Less Awful",
    "section": "Diversity & Inclusion",
    "text": "Diversity & Inclusion\nHopefully, it is abundantly clear by now that if you want to increase diversity at your company, you will need to completely overhaul your interview process (and that if you don’t increase diversity, your organizational performance will suffer). However, the very first step to increasing diversity, which you MUST do before you do anything else, is to make sure you are treating everyone, including the women and people of Color who are already working at your company, VERY WELL. This includes helping them prepare for promotions, listening when they speak, and firing their harassers. Unfortunately, this is rare. You can read more here."
  },
  {
    "objectID": "posts/2017-03-05-interviews/index.html#next-steps",
    "href": "posts/2017-03-05-interviews/index.html#next-steps",
    "title": "How to Make Tech Interviews a Little Less Awful",
    "section": "Next Steps",
    "text": "Next Steps\nInterviewing is less fun than coding, and designing a thoughtful interview process is even less fun, but it is crucial that you put time into doing it well. Otherwise, you will not notice awesome candidates right under your nose, and your company is not assembling the best team possible. At most companies, technical interviewers receive no training, and interviewing is a thankless task, considered a distraction from real work. It will take more time than you are currently putting in to interviewing to start doing it better, and you need to appreciate your employees that do it well.\nThis post is part 3 in a series. Here is part 1 (about bullshit diversity strategies) and part 2 (about why women quit tech)."
  },
  {
    "objectID": "posts/2017-07-28-you-should-blog/index.html",
    "href": "posts/2017-07-28-you-should-blog/index.html",
    "title": "Why you (yes, you) should blog",
    "section": "",
    "text": "The top advice I would give my younger self would be to start blogging sooner. Here are some reasons to blog:\nTo inspire you, here are some sample blog posts from students in our fast.ai deep learning course:\nI enjoyed all of the above blog posts and also, I don’t think any of them are too intimidating. They’re meant to be accessible."
  },
  {
    "objectID": "posts/2017-07-28-you-should-blog/index.html#tips-for-getting-started-blogging",
    "href": "posts/2017-07-28-you-should-blog/index.html#tips-for-getting-started-blogging",
    "title": "Why you (yes, you) should blog",
    "section": "Tips for getting started blogging",
    "text": "Tips for getting started blogging\nMy partner, Jeremy, had been suggesting for years that I should start blogging, and I’d respond “I don’t have anything to say.” This wasn’t true. What I meant was that I didn’t feel confident, and I felt like the things I could write had already been written about by people with more expertise or better writing skills than me.\nIt turns out that is fine! Your posts don’t have to be earth-shattering or even novel to be read and shared. My writing skills were rather weak when I started (part of the reason I chose to study math and CS in college was because those courses requried the least amount of writing and also no labs), but my skills are improving with time.\nHere are some more tips to help you start your first post:\n\nMake a list of links to other blog posts, articles, or studies that you like, and write brief summaries or highlight what you particularly like about them. Part of my first blog post came from my making just such a list, because I couldn’t believe more people hadn’t read the posts and articles that I thought were awesome.\nSummarize what you learned at a conference you attended, or in a class you are taking.\nAny email you’ve written twice should be a blog post. Now, if I’m asked a question that I think someone else would also be interested in, I try to write it up.\nDon’t be a perfectionist. I spent 9 months on my first blog post, it went viral, and I have repeatedly hit new lows in readership ever since then. One of my personal goals for 2017 is to post my writing quicker and not to obsess so much before I post, because it just builds up pressure and I end up writing less.\nYou are best positioned to help people one step behind you. The material is still fresh in your mind. Many experts have forgotten what it was like to be a beginner (or an intermediate) and have forgotten why the topic is hard to understand when you first hear it. The context of your particular background, your particular style, and your knowledge level will give a different twist to what you’re writing about.\nWhat would have helped you a year ago? What would have helped you a week ago?\nIf you are a woman in NYC, Chicago, or San Francisco, I recommend joining your local chapter of Write/Speak/Code, a group that encourages women software developers to write blog posts, speak at conferences, and contribute to open source.\nGet angry. The catalyst that finally got me to start writing was when someone famous said something that made me angry. So angry that I had to explain all the ways his thinking was wrong.\nIf you’re wondering about the actual logistics, Medium makes it super simple to get started. Another option is to use Jekyll and Github pages. I can personally recommend both, as I have 2 blogs and use one for each (my other blog is here).\n\nThis was originally part of a longer post Alternatives to a Degree to Prove Yourself in Deep Learning on fast.ai"
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html",
    "href": "posts/2017-11-13-validation-sets/index.html",
    "title": "How (and why) to create a good validation set",
    "section": "",
    "text": "An all-too-common scenario: a seemingly impressive machine learning model is a complete failure when implemented in production. The fallout includes leaders who are now skeptical of machine learning and reluctant to try it again. How can this happen?\nOne of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). Depending on the nature of your data, choosing a validation set can be the most important step. Although sklearn offers a train_test_split method, this method takes a random subset of the data, which is a poor choice for many real-world problems.\nThe definitions of training, validation, and test sets can be fairly nuanced, and the terms are sometimes inconsistently used. In the deep learning community, “test-time inference” is often used to refer to evaluating on data in production, which is not the technical definition of a test set. As mentioned above, sklearn has a train_test_split method, but no train_validation_test_split. Kaggle only provides training and test sets, yet to do well, you will need to split their training set into your own validation and training sets. Also, it turns out that Kaggle’s test set is actually sub-divided into two sets. It’s no suprise that many beginners may be confused! I will address these subtleties below."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#first-what-is-a-validation-set",
    "href": "posts/2017-11-13-validation-sets/index.html#first-what-is-a-validation-set",
    "title": "How (and why) to create a good validation set",
    "section": "First, what is a “validation set”?",
    "text": "First, what is a “validation set”?\nWhen creating a machine learning model, the ultimate goal is for it to be accurate on new data, not just the data you are using to build it. Consider the below example of 3 different models for a set of data:\n\n\n\nunder-fitting and over-fitting (Source: Andrew Ng’s Machine Learning Coursera class)\n\n\nThe error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it’s not the best choice. Why is that? If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\nThe underlying idea is that:\n\nthe training set is used to train a given model\nthe validation set is used to choose between models (for instance, does a random forest or a neural net work better for your problem? do you want a random forest with 40 trees or 50 trees?)\nthe test set tells you how you’ve done. If you’ve tried out a lot of different models, you may get one that does well on your validation set just by chance, and having a test set helps make sure that is not the case.\n\nA key property of the validation and test sets is that they must be representative of the new data you will see in the future. This may sound like an impossible order! By definition, you haven’t seen this data yet. But there are still a few things you know about it."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#when-is-a-random-subset-not-good-enough",
    "href": "posts/2017-11-13-validation-sets/index.html#when-is-a-random-subset-not-good-enough",
    "title": "How (and why) to create a good validation set",
    "section": "When is a random subset not good enough?",
    "text": "When is a random subset not good enough?\nIt’s instructive to look at a few examples. Although many of these examples come from Kaggle competitions, they are representative of problems you would see in the workplace.\n\nTime series\nIf your data is a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates your are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future). If your data includes the date and you are building a model to use in the future, you will want to choose a continuous section with the latest dates as your validation set (for instance, the last two weeks or last month of the available data).\nSuppose you want to split the time series data below into training and validation sets:\n\n\n\nTime series data\n\n\nA random subset is a poor choice (too easy to fill in the gaps, and not indicative of what you’ll need in production):\n\n\n\na poor choice for your training set\n\n\nUse the earlier data as your training set (and the later data for the validation set):\n\n\n\na better choice for your training set\n\n\nKaggle currently has a competition to predict the sales in a chain of Ecuadorian grocery stores. Kaggle’s “training data” runs from Jan 1 2013 to Aug 15 2017 and the test data spans Aug 16 2017 to Aug 31 2017. A good approach would be to use Aug 1 to Aug 15 2017 as your validation set, and all the earlier data as your training set.\n\n\nNew people, new boats, new…\nYou also need to think about what ways the data you will be making predictions for in production may be qualitatively different from the data you have to train your model with.\nIn the Kaggle distracted driver competition, the independent data are pictures of drivers at the wheel of a car, and the dependent variable is a category such as texting, eating, or safely looking ahead. If you were the insurance company building a model from this data, note that you would be most interested in how the model performs on drivers you haven’t seen before (since you would likely have training data only for a small group of people). This is true of the Kaggle competition as well: the test data consists of people that weren’t used in the training set.\n\n\n\nTwo images of the same person drinking a soda while driving\n\n\nIf you put one of the above images in your training set and one in the validation set, your model will seem to be performing better than it would on new people. Another perspective is that if you used all the people in training your model, your model may be overfitting to particularities of those specific people, and not just learning the states (texting, eating, etc).\nA similar dynamic was at work in the Kaggle fisheries competition to identify the species of fish caught by fishing boats in order to reduce illegal fishing of endangered populations. The test set consisted of boats that didn’t appear in the training data. This means that you’d want your validation set to include boats that are not in the training set.\nSometimes it may not be clear how your test data will differ. For instance, for a problem using satellite imagery, you’d need to gather more information on whether the training set just contained certain geographic locations, or if it came from geographically scattered data."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#the-dangers-of-cross-validation",
    "href": "posts/2017-11-13-validation-sets/index.html#the-dangers-of-cross-validation",
    "title": "How (and why) to create a good validation set",
    "section": "The dangers of cross-validation",
    "text": "The dangers of cross-validation\nThe reason that sklearn doesn’t have a train_validation_test split is that it is assumed you will often be using cross-validation, in which different subsets of the training set serve as the validation set. For example, for a 3-fold cross validation, the data is divided into 3 sets: A, B, and C. A model is first trained on A and B combined as the training set, and evaluated on the validation set C. Next, a model is trained on A and C combined as the training set, and evaluated on validation set B. And so on, with the model performance from the 3 folds being averaged in the end.\nHowever, the problem with cross-validation is that it is rarely applicable to real world problems, for all the reasons describedin the above sections. Cross-validation only works in the same cases where you can randomly shuffle your data to choose a validation set."
  },
  {
    "objectID": "posts/2017-11-13-validation-sets/index.html#kaggles-training-set-your-training-validation-sets",
    "href": "posts/2017-11-13-validation-sets/index.html#kaggles-training-set-your-training-validation-sets",
    "title": "How (and why) to create a good validation set",
    "section": "Kaggle’s “training set” = your training + validation sets",
    "text": "Kaggle’s “training set” = your training + validation sets\nOne great thing about Kaggle competitions is that they force you to think about validation sets more rigorously (in order to do well). For those who are new to Kaggle, it is a platform that hosts machine learning competitions. Kaggle typically breaks the data into two sets you can download:\n\na training set, which includes the independent variables, as well as the dependent variable (what you are trying to predict). For the example of an Ecuadorian grocery store trying to predict sales, the independent variables include the store id, item id, and date; the dependent variable is the number sold. For the example of trying to determine whether a driver is engaging in dangerous behaviors behind the wheel, the independent variable could be a picture of the driver, and the dependent variable is a category (such as texting, eating, or safely looking forward).\na test set, which just has the independent variables. You will make predictions for the test set, which you can submit to Kaggle and get back a score of how well you did.\n\nThis is the basic idea needed to get started with machine learning, but to do well, there is a bit more complexity to understand. You will want to create your own training and validation sets (by splitting the Kaggle “training” data). You will just use your smaller training set (a subset of Kaggle’s training data) for building your model, and you can evaluate it on your validation set (also a subset of Kaggle’s training data) before you submit to Kaggle.\nThe most important reason for this is that Kaggle has split the test data into two sets: for the public and private leaderboards. The score you see on the public leaderboard is just for a subset of your predictions (and you don’t know which subset!). How your predictions fare on the private leaderboard won’t be revealed until the end of the competition. The reason this is important is that you could end up overfitting to the public leaderboard and you wouldn’t realize it until the very end when you did poorly on the private leaderboard. Using a good validation set can prevent this. You can check if your validation set is any good by seeing if your model has similar scores on it to compared with on the Kaggle test set.\nAnother reason it’s important to create your own validation set is that Kaggle limits you to two submissions per day, and you will likely want to experiment more than that. Thirdly, it can be instructive to see exactly what you’re getting wrong on the validation set, and Kaggle doesn’t tell you the right answers for the test set or even which data points you’re getting wrong, just your overall score.\nUnderstanding these distinctions is not just useful for Kaggle. In any predictive machine learning project, you want your model to be able to perform well on new data."
  },
  {
    "objectID": "posts/2017-11-16-what-you-need/index.html",
    "href": "posts/2017-11-16-what-you-need/index.html",
    "title": "What you need to do deep learning",
    "section": "",
    "text": "This post has been translated into Chinese here.\nI want to answer some questions that I’m commonly asked: What kind of computer do I need to do deep learning? Why does fast.ai recommend Nvidia GPUs? What deep learning library do you recommend for beginners? How do you put deep learning into production? I think these questions all fall under a general theme of What do you need (in terms of hardware, software, background, and data) to do deep learning? This post is geared towards those new to the field and curious about getting started."
  },
  {
    "objectID": "posts/2017-11-16-what-you-need/index.html#the-hardware-you-need",
    "href": "posts/2017-11-16-what-you-need/index.html#the-hardware-you-need",
    "title": "What you need to do deep learning",
    "section": "The hardware you need",
    "text": "The hardware you need\n\nWe are indebted to the gaming industry\nThe video game industry is larger (in terms of revenue) than the film and music industries combined. In the last 20 years, the video gaming industry drove forward huge advances in GPUs (graphical processing units), used to do the matrix math needed for rendering graphics. Fortunately, these are exactly the type of computations needed for deep learning. These advances in GPU technology are a key part of why neural networks are proving so much more powerful now than they did a few decades ago. Training a deep learning model without a GPU would be painfully slow in most cases.\n\n\n\nA GPU\n\n\n\n\nNot all GPUs are the same\nMost deep learning practitioners are not programming GPUs directly; we are using software libraries (such as PyTorch or TensorFlow) that handle this. However, to effectively use these libraries, you need access to the right type of GPU. In almost all cases, this means having access to a GPU from the company Nvidia.\nCUDA and OpenCL are the two main ways for programming GPUs. CUDA is by far the most developed, has the most extensive ecosystem, and is the most robustly supported by deep learning libraries. CUDA is a proprietary language created by Nvidia, so it can’t be used by GPUs from other companies. When fast.ai recommends Nvidia GPUs, it is not out of any special affinity or loyalty to Nvidia on our part, but that this is by far the best option for deep learning.\nNvidia dominates the market for GPUs, with the next closest competitor being the company AMD. This summer, AMD announced the release of a platform called ROCm to provide more support for deep learning. The status of ROCm for major deep learning libraries such as PyTorch, TensorFlow, MxNet, and CNTK is still under development. While I would love to see an open source alternative succeed, I have to admit that I find the documentation for ROCm hard to understand. I just read the Overview, Getting Started, and Deep Learning pages of the ROCm website and still can’t explain what ROCm is in my own words, although I want to include it here for completeness. (I admittedly don’t have a background in hardware, but I think that data scientists like me should be part of the intended audience for this project.)\n\n\nIf you don’t have a GPU…\nIf your computer doesn’t have a GPU or has a non-Nvidia GPU, you have several great options:\n\nUse Crestle, through your browser: Crestle is a service (developed by fast.ai student Anurag Goel) that gives you an already set up cloud service with all the popular scientific and deep learning frameworks already pre-installed and configured to run on a GPU in the cloud. It is easily accessed through your browser. New users get 10 hours and 1 GB of storage for free. After this, GPU usage is 59 cents per hour. I recommend this option to those who are new to AWS or new to using the console.\nSet up an AWS cloud instance through your console: You can create an AWS instance (which remotely provides you with Nvidia GPUs) by following the steps in this fast.ai setup lesson. AWS charges 90 cents per hour for this. Although our set-up materials are about AWS (and you’ll find the most forum support for AWS), one fast.ai student created a guide for Setting up an Azure Virtual Machine for Deep learning. And I’m happy to share and add a link if anyone writes a blog post about doing this with Google Cloud Engine.\nBuild your own box. Here’s a lengthy thread from our fast.ai forums where people ask questions, share what components they are using, and post other useful links and tips. The cheapest new Nvidia GPUs are around $300, with some students finding even cheaper used ones on eBay or Craigslist, and others paying more for more powerful GPUs. A few of our students wrote blog posts documenting how they built their machines:\n\nDeep Confusion: Misadventures in Building a Deep Learning Machine by Mariya Yao\nSetting up a Deep learning machine in a lazy yet quick way by Sravya Tirukkovalur\nBuilding your own deep learning box by Brendan Fortuner"
  },
  {
    "objectID": "posts/2017-11-16-what-you-need/index.html#the-software-you-need",
    "href": "posts/2017-11-16-what-you-need/index.html#the-software-you-need",
    "title": "What you need to do deep learning",
    "section": "The software you need",
    "text": "The software you need\nDeep learning is a relatively young field, and the libraries and tools are changing quickly. For instance, Theano, which we chose to use for part 1 of our course in 2016, was just retired. PyTorch, which we are using currently, was only released earlier this year (2017). As Jeremy wrote previously, you should assume that whatever specific libraries and software you learn today will be obsolete in a year or two. The most important thing is to understand the underlying concepts, and towards that end, we are creating our own library on top of Pytorch that we believe makes deep learning concepts clearer, as well as encoding best practices as defaults.\nPython is by far the most commonly used language for deep learning. There are a number of deep learning libraries available, with almost every major tech company backing a different library, although employees at those companies often use a mix of tools. Deep learning libraries include TensorFlow (Google), PyTorch (Facebook), MxNet (University of Washington, adapted by Amazon), CNTK (Microsoft), DeepLearning4j (Skymind), Caffe2 (also Facebook), Nnabla (Sony), PaddlePaddle (Baidu), and Keras (a high-level API that runs on top of several libraries in this list). All of these have Python options available.\n\nDynamic vs. Static Graph Computation\nAt fast.ai, we prioritize the speed at which programmers can experiment and iterate (through easier debugging and more intutive design) as more important than theoretical performance speed-ups. This is the reason we use PyTorch, a flexible deep learning library with dynamic computation.\nOne distinction amongst deep learning libraries is whether they use dynamic or static computations (some libraries, such as MxNet and now TensorFlow, allow for both). Dynamic computation mean that the program is executed in the order you wrote it. This typically makes debugging easier, and makes it more straightforward to translate ideas from your head into code. Static computation means that you build a structure for your neural network in advance, and then execute operations on it. Theoretically, this allows the compiler to do greater optimizations, although it also means there may be more of a disconnect between what you intended your program to be and what the compiler executes. It also means that bugs can seem more removed from the code that caused them (for instance, if there is an error in how you constructed your graph, you may not realize until you perform an operation on it later). Even though there are theoretical arguments that languages with static computation graphs are capable of better performance than languages with dynamic computation, we often find that is not the case for us in practice.\nGoogle’s TensorFlow mostly uses a static computation graph, whereas Facebook’s PyTorch uses dynamic computation. (Note: TensorFlow announced a dynamic computation option, Eager Execution, just two weeks ago, although it is still quite early and most TensorFlow documentation and projects use the static option). In September, fast.ai announced that we had chosen PyTorch over TensorFlow to use in our course this year and to use for the development of our own library (a higher-level wrapper for PyTorch that encodes best practices). Briefly, here are a few of our reasons for choosing PyTorch (explained in much greater detail here):\n\neasier to debug\ndynamic computation is much better suited for natural language processing\ntraditional Object Oriented Programming style (which feels more natural to us)\nTensorFlow’s use of unusual conventions like scope and sessions can be confusing and are more to learn\n\nGoogle has put far more resources into marketing TensorFlow than anyone else, and I think this is one of the reasons that TensorFlow is so well known (for many people outside deep learning, TensorFlow is the only DL framework that they’ve heard of). As mentioned above, TensorFlow released a dynamic computation option a few weeks ago, which addresses some of the above issues. Many people have asked fast.ai if we are going to switch back to TensorFlow. The dynamic option is still quite new and far less developed, so we will happily continue with PyTorch for now. However, the TensorFlow team has been very receptive to our ideas, and we would love to see our fastai library ported to TensorFlow.\nNote: The in-person version of our updated course, which uses PyTorch as well as our own fastai library, is happening currently. It will be released online for free after the course ends (estimated release: January)."
  },
  {
    "objectID": "posts/2017-11-16-what-you-need/index.html#what-you-need-for-production-not-a-gpu",
    "href": "posts/2017-11-16-what-you-need/index.html#what-you-need-for-production-not-a-gpu",
    "title": "What you need to do deep learning",
    "section": "What you need for production: not a GPU",
    "text": "What you need for production: not a GPU\nMany people overcomplicate the idea of using deep learning in production and believe that they need much more complex systems than they actually do. You can use deep learning in production with a CPU and the webserver of your choice, and in fact, this is what we recommend for most use cases. Here are a few key points:\n\nIt is incredibly rare to need to train in production. Even if you want to update your model weights daily, you don’t need to train in production. Good news! This means that you are just doing inference (a forward pass through your model) in production, which is much quicker and easier than training.\nYou can use whatever webserver you like (e.g. Flask) and set up inference as a simple API call.\nGPUs only provide a speed-up if you are effectively able to batch your data. Even if you are getting 32 requests per second, using a GPU would most likely slow you down, because you’d have to wait a second from when the 1st arrived to collect all 32, then perform the computation, and then return the results. We recommend using a CPU in production, and you can always add more CPUs (easier than using multiple GPUs) as needed.\n\nFor big companies, it may make sense to use GPUs in production for serving– however, it will be clear when your reach this size. Prematurely trying to scale before it’s needed will only add needless complexity and slow you down."
  },
  {
    "objectID": "posts/2017-11-16-what-you-need/index.html#the-background-you-need-1-year-of-coding",
    "href": "posts/2017-11-16-what-you-need/index.html#the-background-you-need-1-year-of-coding",
    "title": "What you need to do deep learning",
    "section": "The background you need: 1 year of coding",
    "text": "The background you need: 1 year of coding\nOne of the frustrations that inspired Jeremy and I to create Practical Deep Learning for Coders was (is) that most deep learning materials fall into one of two categories:\n\nso shallow and high-level as to not give you the information or skills needed to actually use deep learning in the workplace or create state-of-the-art models. This is fine if you just want a high-level overview, but disappointing if you want to become a working practitioner.\nhighly theoretical and assume a graduate level math background. This is a prohibitive barrier for many folks, and even as someone who has a math PhD, I found that the theory wasn’t particularly useful in learning how to code practical solutions. It’s not surprising that many materials have this slant. Until quite recently, deep learning was almost entirely an academic discipline and largely driven by questions of what would publish in top academic journals.\n\nOur free course Practical Deep Learning for Coders is unique in that the only pre-requisite is 1 year of programming experience, yet it still teaches you how to create state-of-the-art models. Your background can be in any language, although you might want to learn some Python before starting the course, since that is what we use. We introduce math concepts as needed, and we don’t recommend that you try to front-load studying math theory in advance.\nIf you don’t know how to code, I highly recommend learning, and Python is a great language to start with if you are interested in data science."
  },
  {
    "objectID": "posts/2017-11-16-what-you-need/index.html#the-data-you-need-far-less-than-you-think",
    "href": "posts/2017-11-16-what-you-need/index.html#the-data-you-need-far-less-than-you-think",
    "title": "What you need to do deep learning",
    "section": "The data you need: far less than you think",
    "text": "The data you need: far less than you think\nAlthough many have claimed that you need Google-size data sets to do deep learning, this is false. The power of transfer learning (combined with techniques like data augmentation) make it possible for people to apply pre-trained models to much smaller datasets. As we’ve talked about elsewhere, at medical start-up Enlitic, Jeremy Howard led a team that used just 1,000 examples of lung CT scans with cancer to build an algorithm that was more accurate at diagnosing lung cancer than a panel of 4 expert radiologists. The C++ library Dlib has an example in which a face detector is accurately trained using only 4 images, containing just 18 faces!\n\n\n\nFace Recognition with Dlib"
  },
  {
    "objectID": "posts/2017-11-16-what-you-need/index.html#a-note-about-access",
    "href": "posts/2017-11-16-what-you-need/index.html#a-note-about-access",
    "title": "What you need to do deep learning",
    "section": "A note about access",
    "text": "A note about access\nFor the vast majority of people I talk with, the barriers to entry for deep learning are far lower than they expected and the costs are well within their budgets. However, I realize this is not the case universally. I’m periodically contacted by students that want to take our online course but can’t afford the costs of AWS. Unfortunately, I don’t have a solution. There are other barriers as well. Bruno Sánchez-A Nuño has written about the challenges of doing data science in places that don’t have reliable internet access, and fast.ai international fellow Tahsin Mayeesha describes hidden barriers to MOOC access in countries such as Bangladesh. I care about these issues of access, and it is disatisifying to not have solutions."
  },
  {
    "objectID": "posts/2017-12-18-personal-brand/index.html",
    "href": "posts/2017-12-18-personal-brand/index.html",
    "title": "Making Peace with Personal Branding",
    "section": "",
    "text": "As a child, I was nerdy and shy. At my elementary and middle schools, we had to present our science projects to judges in the school science fair each year, and I noticed that students who were outgoing and good at presenting were more likely to win. I remember feeling indignant– shouldn’t we just be judged on scientific merit? Why should things like being able to smile, make eye contact, and show enthusiasm (all things I didn’t do) with the judges have any impact?\nBut it turns out that those other skills are actually useful! Personal branding is similar– we may want our professional work to stand on its own merit, but how we present and share it is important. And so, two weeks ago I found myself mentoring on the cringe-inducing topic of personal branding at the Women in Machine Learning Workshop, co-located with the deep learning conference NIPS. Part of me felt embarrassed to be talking about something as seemingly shallow as personal branding, while just a few tables away deep learning star Yoshua Bengio mentored on the more serious topic of deep learning. However, I’ve worked hard to make peace with the concept and wanted to share what I’ve discovered."
  },
  {
    "objectID": "posts/2017-12-18-personal-brand/index.html#what-is-personal-branding-and-why-is-it-useful",
    "href": "posts/2017-12-18-personal-brand/index.html#what-is-personal-branding-and-why-is-it-useful",
    "title": "Making Peace with Personal Branding",
    "section": "What is “personal branding” and why is it useful?",
    "text": "What is “personal branding” and why is it useful?\nOver the past two years, I’ve consistently put time in to twitter and blog posts. Here are a few ways this has been helpful to me:\n\nBeing invited to attend the TensorFlow Dev Summit\nBeing invited to keynote JupyterCon\nBeing interviewed and quoted in Wired (twice)\nBeing able to raise money for 18 AI diversity scholarships and $250,000 of AWS credits to give to fast.ai students\n\n\n\n\nme ontstage at JupyterCon in 2017\n\n\nI talked to a grad student who was giving an oral presentation at NIPS, and she noted how a classmate of hers with a much larger twitter following got significantly more retweets and attendees for his talk. This struck her as unfair since a larger twitter following doesn’t equate with better research, but it also convinced her that building a personal brand would be useful.\nI think of personal branding as anything that helps people find out about you and your work. This includes blogging, using twitter, and public speaking. Personal branding is a bit like a web: your blog post may lead to a job interview; you may get a speaking engagement from someone who follows you on twitter; and your conference talk may lead some of the audience members to read your blog or follow you on twitter, continuing the cycle.\nPersonal branding is no substitute for doing high-quality technical work; it’s just the means by which you can share this work with a broader audience."
  },
  {
    "objectID": "posts/2017-12-18-personal-brand/index.html#making-peace-with-personal-branding",
    "href": "posts/2017-12-18-personal-brand/index.html#making-peace-with-personal-branding",
    "title": "Making Peace with Personal Branding",
    "section": "Making peace with personal branding",
    "text": "Making peace with personal branding\nHere are a few things that helped me get okay with the idea of personal branding:\n\nPersonal branding sounds icky if you think of it as a shallow popularity contest, or of trying to trick people to click on links they don’t really want to click on. However, I now think about it as wanting people to know about high-quality work that I’m proud of and care about.\nRealizing that social skills and communication skills are things I could get better at with practice (I consider personal branding to be a subset of communication skills). I felt more resentful when as a kid I thought that people were either born outgoing or not, and that there wasn’t anything I could do, but as I started working on those skills and saw them improve, I was encouraged.\nPeople have a bias towards thinking the most valuable skills are the ones we are already good at and have already put a lot of time into (whether that’s a particular academic subject, programming language, or sport). I still catch myself feeling a particular affinity towards other mathematicians (I know firsthand that getting a math PhD was hard!) But a lot of other things are hard and valuable too."
  },
  {
    "objectID": "posts/2017-12-18-personal-brand/index.html#learn-by-observation",
    "href": "posts/2017-12-18-personal-brand/index.html#learn-by-observation",
    "title": "Making Peace with Personal Branding",
    "section": "Learn by observation",
    "text": "Learn by observation\nI recommend finding people who are doing personal branding well, and observing what they do and what works. What is it about that conference talk that made it so good? Why do you enjoy following X on twitter? What keeps you returning to Y’s blog?"
  },
  {
    "objectID": "posts/2017-12-18-personal-brand/index.html#twitter",
    "href": "posts/2017-12-18-personal-brand/index.html#twitter",
    "title": "Making Peace with Personal Branding",
    "section": "Twitter",
    "text": "Twitter\n\nIn defense of twitter\nTwitter seems really weird at first (I was a twitter skeptic for years, not starting to actively use it until 2014), but it’s actually really useful. I’ve met new people through twitter. I know people who have gotten jobs through twitter. There are some really interesting conversations that I see on twitter that I don’t see elsewhere, such as this discussion about what it means to do “more rigorous” deep learning experiments, or here where several genomics researchers responded to my question about whether Google’s DeepVariant is overhyped.\nApart from the “personal branding” aspects, twitter helps me practice being more concise. It’s been good for my writing skills. I also use it as a way of bookmarking blog posts I like and highlights from talks and conferences I attend, so sometimes I refer back to it a reference.\n\n\n\nBehaviors to avoid in code reviews:- stating opinion as fact- avalanche of comments- asking devs to fix problems they didn’t cause- judgemental questions- sarcasm @sandyaaaas #AlterConf pic.twitter.com/LX1AeG4JOk\n\n— Rachel Thomas (@math_rachel) December 10, 2017\n\n\nA tweet about a talk by Sandya Sankarram that I really enjoyed.\n\n\n\nTwitter for beginners\nYour enjoyment of twitter will vary greatly depending on who you follow. It will take some experimenting to get this right. Feel free to unfollow people if you realize you’re not getting anything out of their tweets. Whenever I read an article I like or hear a talk I like, I always look up the author/speaker on twitter and see if I find their tweets interesting. If so, I follow them. Also, there are people for whom you may love their writing/talks/other work, but don’t really enjoy their tweets. You don’t have to follow them. Twitter is it’s own distinct medium, and being good at something else doesn’t necessarily translate. If you are particularly looking for deep learning tweets, you can check out Jeremy Howard’s likes, and follow some of the accounts shared there.\nPeople use twitter in a variety of ways: as a social network, for political activism, for self-expression, and more. I use twitter primarily as a professional tool (I think of it as a more dynamic version of LinkedIn), so I try to keep most of my tweets related to data science. If your goal is personal branding or finding a job, I recommend keeping your tweets mostly focused on your field. Some people deal with this by having separate personal and professional twitter accounts (for instance, Data Science Renee does this).\n\n\n\nNew post: Please Don't Say \"It used to be called Big Data and now it's called Deep Learning\" https://t.co/4mm47R2uXE\n\n— Rachel Thomas (@math_rachel) November 18, 2016\n\n\n Above: Sharing one of my own blog posts on Twitter. \n\nFeel free to mute topics you don’t want to hear about (you can mute particular words), and mute people who bring you down. You are allowed to use twitter however you like, and you aren’t required to argue with anyone you don’t want to.\nTwitter can be a low time commitment. You don’t need to check it every day. It’s fine to just tweet once a week. When I started, I primarily used it as a way to bookmark blog posts or articles I liked. Building up followers can be a long, slow process. Be patient.\nObserve successful twitter accounts, of people who aren’t “famous” (famous people will have a ton of followers regardless of the quality of their tweets), to see what works. A few accounts you might want to check out for inspiration are: Mariya Yao, Julia Evans, Data Science Renee, and Stephanie Hurlburt. They each have built up over 20k followers, by providing thoughtful and interesting tweets, and generously promoting the work of others."
  },
  {
    "objectID": "posts/2017-12-18-personal-brand/index.html#speaking-at-meetups-or-conferences",
    "href": "posts/2017-12-18-personal-brand/index.html#speaking-at-meetups-or-conferences",
    "title": "Making Peace with Personal Branding",
    "section": "Speaking at Meetups or Conferences",
    "text": "Speaking at Meetups or Conferences\nMost people (including experts with tons of experience) are terrified and intimidated by public speaking, yet it is such a great way to share your work that it’s worth it.\nTwo years ago I decided I wanted to do more public speaking after not having done much for many years (my previous experience was primarily academic and from before I switched into the tech industry). I was nervous and also uncertain if I had anything of value to say. I started small, giving a 5 minute lightning talk at a PyLadies meetup to a particularly supportive audience, gradually working up through events with 50-100 people, to eventually presenting to 700 people at JupyterCon.\nI prepare a ton for talks, since it both helps me feel less anxious and results in stronger talks. I prepare for short talks and small audiences, as well as big talks, because I want to be respectful of the audience. I think it’s particularly important to go through your timing to make sure that you’ll be able to cover what you plan (I’ve seen some talks get cut off before the speaker even reached their main point).\nNothing is more irritating to me as an audience member than having to sit through an infomercial. It’s important to offer useful information to your audience, and not just advertise your product or company. My goal with all my talks is to have some information that will be useful or thought-provoking, even if the listeners never take a fast.ai course.\nFor every talk I give, I ask if the venue will be able to do a video-recording (here are professional recordings of me speaking at an ML meetup at AWS and at PyBay). If not, I will often do my own recording. I use the software Camtasia to capture my screen and video, and have my own microphone that plugs into my computer via usb. For instance, this is how I created the below tutorial on Word Embeddings. About 80 people attended the live workshop and now 2,400 have watched the recording online! Getting or making recordings allows you to reach a broader audience, and it will make it easier for you to get future speaking engagements as you build up a portfolio of your past talks.\n\n\n\n\n\nIf my talk involves code, I try to create a demo on github (like this or this) that has enough documentation to stand alone as a tutorial or guide. Even if I don’t plan to cover all the set-up or background in my talk, I want to give people a resource that they can use later. You don’t need to create a recording or a demo to give a talk (particularly if it will stress you out), but it’s worth considering.\n\nPublic Speaking Resources\nTechnically Speaking was an excellent newsletter sharing links to blog posts and videos with public speaking advice for those in tech, created by Cate Huston and Chiu-Ki Chan, senior developers with a ton of speaking experience. Although it is no longer active, you can still check out the archives here.\nIf you are a woman or non-binary person living in Atlanta, NYC, SF, Chicago, or LA, I highly recommend Write Speak Code meetups or workshops as a great place to practice technical talks and receive constructive feedback.\nI was scheduled to speak to an audience of 1,000 people at TEDx San Francisco in October (unfortunately, I ended up in the ICU with a life-threatening illness at the last minute and couldn’t attend, but I’d already gone through months of preparation and was completely ready). I was terrified, so I started working with a public speaking coach in preparation, and it was super helpful. I searched for coaches on yelp, and met with a few to find one that I particularly liked. From asking around, a lot of excellent and famous speakers have worked with speech coaches. They can help with anything– from your voice and body language, to crafting engaging intros and conclusions. In hindsight, I probably should’ve met with a speech coach even earlier in my public speaking journey; you certainly don’t need to be preparing for an audience of 1,000 to hire one.\nMany years ago I participated in a chapter of Toastmasters, and I enjoyed that. When I asked about speech coaches on twitter, several people told me that training in improv, theater, or singing had been helpful to them in the realm of public speaking."
  },
  {
    "objectID": "posts/2017-12-18-personal-brand/index.html#blogging",
    "href": "posts/2017-12-18-personal-brand/index.html#blogging",
    "title": "Making Peace with Personal Branding",
    "section": "Blogging",
    "text": "Blogging\nI’ve already written an entire post on blogging. Here are a few highlights:\n\nIt’s like a resume, only better. I know of a few people who have had blog posts lead to job offers!\nHelps you learn. Organizing knowledge always helps me synthesize my own ideas. One of the tests of whether you understand something is whether you can explain it to someone else. A blog post is a great way to do that.\nI’ve gotten invitations to conferences and invitations to speak from my blog posts. I was invited to the TensorFlow Dev Summit (which was awesome!) for writing a blog post about how I don’t like TensorFlow.\nMeet new people. I’ve met several people who have responded to blog posts I wrote.\nSaves time. Any time you answer a question multiple times through email, you should turn it into a blog post, which makes it easier for you to share the next time someone asks.\n\nIt can be intimidating to start blogging, but remember that your target audience is you-6-months-ago, not Geoffrey Hinton. What would have been most helpful to your slightly younger self? You are best positioned to help people one step behind you. The material is still fresh in your mind. Many experts have forgotten what it was like to be a beginner (or an intermediate). The context of your particular background, your particular style, and your knowledge level will give a different twist to what you’re writing about.\nAnd as inspiration, here are links to a few blogs that I consistently enjoy:\n\nJulia Evans\nStephen Merity\nCate Huston\nSlav Ivanov\nJulia Ferraioli"
  },
  {
    "objectID": "posts/2017-12-18-personal-brand/index.html#go-forth-and-personal-brand",
    "href": "posts/2017-12-18-personal-brand/index.html#go-forth-and-personal-brand",
    "title": "Making Peace with Personal Branding",
    "section": "Go Forth and Personal Brand",
    "text": "Go Forth and Personal Brand\nSharing high quality work (both your own and that of others) will help you develop a platform to further your goals. Observe people who are doing this well: who are writing blog posts you enjoy, producing tweets you like to follow, or giving engaging talks. While we may want technical expertise to stand on its own, communication skills are vital in helping your work reach an audience. Starting to work on any new skillset is often intimidating, but with small steps and practice you will improve. As a meta-exercise, I did some personal branding in this post by linking to my own work. It felt uncomfortable, but I followed my own advice and did it anyway!"
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html",
    "href": "posts/2018-04-29-categorical-embeddings/index.html",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "",
    "text": "There is a powerful technique that is winning Kaggle competitions and is widely used at Google (according to Jeff Dean), Pinterest, and Instacart, yet that many people don’t even realize is possible: the use of deep learning for tabular data, and in particular, the creation of embeddings for categorical variables.\nDespite what you may have heard, you can use deep learning for the type of data you might keep in a SQL database, a Pandas DataFrame, or an Excel spreadsheet (including time-series data). I will refer to this as tabular data, although it can also be known as relational data, structured data, or other terms (see my twitter poll and comments for more discussion).\nTabular data is the most commonly used type of data in industry, but deep learning on tabular data receives far less attention than deep learning for computer vision and natural language processing. This post covers some key concepts from applying neural networks to tabular data, in particular the idea of creating embeddings for categorical variables, and highlights 2 relevant modules of the fastai library:\nThe material from this post is covered in much more detail starting around 1:59:45 in the Lesson 3 video and continuing in Lesson 4 of our free, online Practical Deep Learning for Coders course. To see example code of how this approach can be used in practice, check out our Lesson 3 jupyter notebook."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#embeddings-for-categorical-variables",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#embeddings-for-categorical-variables",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Embeddings for Categorical Variables",
    "text": "Embeddings for Categorical Variables\nA key technique to making the most of deep learning for tabular data is to use embeddings for your categorical variables. This approach allows for relationships between categories to be captured. Perhaps Saturday and Sunday have similar behavior, and maybe Friday behaves like an average of a weekend and a weekday. Similarly, for zip codes, there may be patterns for zip codes that are geographically near each other, and for zip codes that are of similar socio-economic status.\n\nTaking Inspiration from Word Embeddings\nA way to capture these multi-dimensional relationships between categories is to use embeddings. This is the same idea as is used with word embeddings, such as Word2Vec. For instance, a 3-dimensional version of a word embedding might look like:\n\n\n\n\n\n\n\npuppy\n[0.9, 1.0, 0.0]\n\n\ndog\n[1.0, 0.2, 0.0]\n\n\nkitten\n[0.0, 1.0, 0.9]\n\n\ncat\n[0.0, 0.2, 1.0]\n\n\n\nNotice that the first dimension is capturing something related to being a dog, and the second dimension captures youthfulness. This example was made up by hand, but in practice you would use machine learning to find the best representations (while semantic values such as dogginess and youth would be captured, they might not line up with a single dimension so cleanly). You can check out my workshop on word embeddings for more details about how word embeddings work.\n\n\n\nillustration from my word embeddings workshop: vectors for baby animal words are closer together, and an unrelated word like ‘avalanche’ is further away\n\n\n\n\nApplying Embeddings for Categorical Variables\nSimilarly, when working with categorical variables, we will represent each category by a vector of floating point numbers (the values of this representation are learned as the network is trained).\nFor instance, a 4-dimensional version of an embedding for day of week could look like:\n\n\n\n\n\n\n\nSunday\n[.8, .2, .1, .1]\n\n\nMonday\n[.1, .2, .9, .9]\n\n\nTuesday\n[.2, .1, .9, .8]\n\n\n\nHere, Monday and Tuesday are fairly similar, yet they are both quite different from Sunday. Again, this is a toy example. In practice, our neural network would learn the best representations for each category while it is training, and each dimension (or direction, which doesn’t necessarily line up with ordinal dimensions) could have multiple meanings. Rich relationships can be captured in these distributed representations.\n\n\nReusing Pretrained Categorical Embeddings\nEmbeddings capture richer relationships and complexities than the raw categories. Once you have learned embeddings for a category which you commonly use in your business (e.g. product, store id, or zip code), you can use these pre-trained embeddings for other models. For instance, Pinterest has created 128-dimensional embeddings for its pins in a library called Pin2Vec, and Instacart has embeddings for its grocery items, stores, and customers.\n\n\n\nFrom the Instacart blog post ‘Deep Learning with Emojis (not Math)’\n\n\nThe fastai library contains an implementation for categorical variables, which work with Pytorch’s nn.Embedding module, so this is not something you need to code from hand each time you want to use it."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#treating-some-continuous-variables-as-categorical",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#treating-some-continuous-variables-as-categorical",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Treating some Continuous Variables as Categorical",
    "text": "Treating some Continuous Variables as Categorical\nWe generally recommend treating month, year, day of week, and some other variables as categorical, even though they could be treated as continuous. Often for variables with a relatively small number of categories, this results in better performance. This is a modeling decision that the data scientist makes. Generally, we want to keep continuous variables represented by floating point numbers as continuous.\nAlthough we can choose to treat continuous variables as categorical, the reverse is not true: any variables that are categorical must be treated as categorical."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#time-series-data",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#time-series-data",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Time Series Data",
    "text": "Time Series Data\nThe approach of using neural networks together with categorical embeddings can be applied to time series data as well. In fact, this was the model used by students of Yoshua Bengio to win 1st place in the Kaggle Taxi competition(paper here), using a trajectory of GPS points and timestamps to predict the length of a taxi ride. It was also used by the 3rd place winners in the Kaggle Rossmann Competition, which involved using time series data from a chain of stores to predict future sales. The 1st and 2nd place winners of this competition used complicated ensembles that relied on specialist knowledge, while the 3rd place entry was a single model with no domain-specific feature engineering.\n\n\n\nThe winning architecture from the Kaggle Taxi Trajectory Competition\n\n\nIn our Lesson 3 jupyter notebook we walk through a solution for the Kaggle Rossmann Competition. This data set (like many data sets) includes both categorical data (such as the state the store is located in, or being one of 3 different store types) and continuous data (such as the distance to the nearest competitor or the temperature of the local weather). The fastai library lets you enter both categorical and continuous variables as input to a neural network.\nWhen applying machine learning to time-series data, you nearly always want to choose a validation set that is a continuous selection with the latest available dates that you have data for. As I wrote in a previous post, “If your data is a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates your are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future).”\nOne key to successfully using deep learning with time series data is to split the date into multiple categorical variables (year, month, week, day of week, day of month, and Booleans for whether it’s the start/end of a month/quarter/year). The fastai library has implemented a method to handle this for you, as described below."
  },
  {
    "objectID": "posts/2018-04-29-categorical-embeddings/index.html#modules-to-know-in-the-fastai-library",
    "href": "posts/2018-04-29-categorical-embeddings/index.html#modules-to-know-in-the-fastai-library",
    "title": "An Introduction to Deep Learning for Tabular Data",
    "section": "Modules to Know in the Fastai Library",
    "text": "Modules to Know in the Fastai Library\nWe will be releasing more documentation for the fastai library in coming months, but it is already available on pip and on github, and it is used in the Practical Deep Learning for Coders course. The fastai library is built on top of Pytorch and encodes best practices and helpful high-level abstractions for using neural networks. The fastai library achieves state-of-the-art results and was recently used to win the Stanford DAWNBench competition (fastest CIFAR10 training).\n\nfastai.column_data\nfastai.column_data.ColumnarModelData takes a Pandas DataFrame as input and creates a type of ModelData object (an object which contains data loaders for the training, validation, and test sets, and which is the fundamental way of keeping track of your data while training models).\n\n\nfastai.structured\nThe fastai.structured module of the fastai library is built on top of Pandas, and includes methods to transform DataFrames in a number of ways, improving the performance of machine learning models by pre-processing the data appropriately and creating the right types of variables.\nFor instance, fastai.structured.add_datepart converts dates (e.g. 2000-03-11) into a number of variables (year, month, week, day of week, day of month, and booleans for whether it’s the start/end of a month/quarter/year.)\nOther useful methods in the module allow you to:\n\nFill in missing values with the median whilst adding a boolean indicator variable (fix_missing)\nChange any columns of strings in a Pandas DataFrame to a column of categorical values (train_cats)"
  },
  {
    "objectID": "posts/2018-07-12-automl1/index.html",
    "href": "posts/2018-07-12-automl1/index.html",
    "title": "What do machine learning practitioners actually do?",
    "section": "",
    "text": "This post is part 1 of a series. Part 2 is an opinionated introduction to AutoML and neural architecture search, and Part 3 looks at Google’s AutoML in particular.\nThere are frequent media headlines about both the scarcity of machine learning talent (see here, here, and here) and about the promises of companies claiming their products automate machine learning and eliminate the need for ML expertise altogether (see here, here, and here). In his keynote at the TensorFlow DevSummit, Google’s head of AI Jeff Dean estimated that there are tens of millions of organizations that have electronic data that could be used for machine learning but lack the necessary expertise and skills. I follow these issues closely since my work at fast.ai focuses on enabling more people to use machine learning and on making it easier to use.\nIn thinking about how we can automate some of the work of machine learning, as well as how to make it more accessible to people with a wider variety of backgrounds, it’s first necessary to ask, what is it that machine learning practitioners do? Any solution to the shortage of machine learning expertise requires answering this question: whether it’s so we know what skills to teach, what tools to build, or what processes to automate.\n\n\n\nWhat do machine learning practitioners do? (Image Source: #WOCinTech Chat)\n\n\nThis post is the first in a 3-part series. It will address what it is that machine learning practitioners do, with Part 2 explaining AutoML and neural architecture search (which several high profile figures have suggested will be key to decreasing the need for data scientists) and Part 3 will cover Google’s heavily hyped AutoML product in particular.\n\nBuilding Data Products is Complex Work\n\nWhile many academic machine learning sources focus almost exclusively on predictive modeling, that is just one piece of what machine learning practitioners do in the wild. The processes of appropriately framing a business problem, collecting and cleaning the data, building the model, implementing the result, and then monitoring for changes are interconnected in many ways that often make it hard to silo off just a single piece (without at least being aware of what the other pieces entail). As Jeremy Howard et al. wrote in Designing great data products, Great predictive modeling is an important part of the solution, but it no longer stands on its own; as products become more sophisticated, it disappears into the plumbing.\n\n\n\nBuilding Data Products is Complex Work (Source: Wikimedia Commons)\n\n\nA team from Google, D. Sculley et al., wrote the classic Machine Learning: The High-Interest Credit Card of Technical Debt about the code complexity and technical debt often created when using machine learning in practice. The authors identify a number of system-level interactions, risks, and anti-patterns, including:\n\nglue code: massive amount of supporting code written to get data into and out of general-purpose packages\npipeline jungles: the system for preparing data in an ML-friendly format may become a jungle of scrapes, joins, and sampling steps, often with intermediate files output\nre-use input signals in ways that create unintended tight coupling of otherwise disjoint systems\nrisk that changes in the external world may make models or input signals change behavior in unintended ways, and these can be difficult to monitor\n\nThe authors write, A remarkable portion of real-world “machine learning” work is devoted to tackling issues of this form… It’s worth noting that glue code and pipeline jungles are symptomatic of integration issues that may have a root cause in overly separated “research” and “engineering” roles… It may be surprising to the academic community to know that only a tiny fraction of the code in many machine learning systems is actually doing “machine learning”. (emphasis mine)\n\nWhen machine learning projects fail\n\nIn a previous post, I identified some failure modes in which machine learning projects are not effective in the workplace:\n\nThe data science team builds really cool stuff that never gets used. There’s no buy-in from the rest of the organization for what they’re working on, and some of the data scientists don’t have a good sense of what can realistically be put into production.\nThere is a backlog with data scientists producing models much faster than there is engineering support to put them in production.\nThe data infrastructure engineers are separate from the data scientists. The pipelines don’t have the data the data scientists are asking for now, and the data scientists are under-utilizing the data sources the infrastructure engineers have collected.\nThe company has definitely decided on feature/product X. They need a data scientist to gather some data that supports this decision. The data scientist feels like the PM is ignoring data that contradicts the decision; the PM feels that the data scientist is ignoring other business logic.\nThe data science team interviews a candidate with impressive math modeling and engineering skills. Once hired, the candidate is embedded in a vertical product team that needs simple business analytics. The data scientist is bored and not utilizing their skills.\n\nI framed these as organizational failures in my original post, but they can also be described as various participants being overly focused on just one slice of the complex system that makes up a full data product. These are failures of communication and goal alignment between different parts of the data product pipeline.\n\nSo, what do machine learning practitioners do?\n\nAs suggested above, building a machine learning product is a multi-faceted and complex task. Here are some of the things that machine learning practitioners may need to do during the process:\nUnderstanding the context:\n\nidentify areas of the business that could benefit from machine learning\ncommunicate with other stakeholders about what machine learning is and is not capable of (there are often many misconceptions)\ndevelop understanding of business strategy, risks, and goals to make sure everyone is on the same page\nidentify what kind of data the organization has\nappropriately frame and scope the task\nunderstand operational constraints (e.g. what data is actually available at inference time)\nproactively identify ethical risks, including how your work could be mis-used by harassers, trolls, authoritarian governments, or for propaganda/disinformation campaigns (and plan how to reduce these risks)\nidentify potential biases and potential negative feedback loops\n\nData: - make plans to collect more of different data (if needed and if possible) - stitch together data from many different sources: often this data has been collected in different formats or with inconsistent conventions - deal with missing or corrupted data - visualize the data - create appropriate training, validation, and test sets\nModeling: - choose which model to use - fit model resource needs into constraints (e.g. will the completed model need to run on an edge device, in a low memory or high latency environment, etc) - choose hyperparameters (e.g. in the case of deep learning, this includes choosing an architecture, loss function, and optimizer) - train the model (and debug why it’s not training). This can involve: - adjusting hyperparmeters (e.g. such as the learning rate) - outputing intermediate results to see how the loss, training error, and validation error are changing with time - inspecting the data the model is wrong on to look for patterns - identifying underlying errors or issues with the data - realizing you need to change how you clean and pre-process the data - realizing you need more or different data augmentation - realizing you need more or different data - trying out different models - identifying if you are under- or over-fitting\nProductionize: - creating an API or web app with your model as an endpoint in order to productionize - exporting your model into the needed format - plan for how often your model will need to be retrained with updated data (e.g. perhaps you will retrain nightly or weekly)\nMonitor: - track model performance over time - monitor the input data, to identify if it changes with time in a way that would invalidate your model - communicate your results to the rest of the organization - have a plan in place for how you will monitor and respond to mistakes or unexpected consequences\nCertainly, not every machine learning practitioner needs to do all of the above steps, but components of this process will be a part of many machine learning applications. Even if you are working on just a subset of these steps, a familiarity with the rest of the process will help ensure that you are not overlooking considerations that would keep your project from being successful!\n\nTwo of the hardest parts of Machine Learning\n\nFor myself and many others I know, I would highlight two of the most time-consuming and frustrating aspects of machine learning (in particular, deep learning) as:\n\nDealing with data formatting, inconsistencies, and errors is often a messy and tedious process.\nTraining deep learning models is a notoriously brittle process right now.\n\n\nIs cleaning data really part of ML? Yes.\n\nDealing with data formatting, inconsistencies, and errors is often a messy and tedious process. People will sometimes describe machine learning as separate from data science, as though for machine learning, you can just begin with your nicely cleaned, formatted data set. However, in my experience, the process of cleaning a data set and training a model are usually interwoven: I frequently find issues in the model training that cause me to go back and change the pre-processing for the input data.\n\n\n\nDealing with messy and inconsistent data is necessary\n\n\n\nTraining Deep Learning Models is Brittle and Finicky (for now)\n\nThe difficulty of getting models to train deters many beginners, who often wind up feeling discouraged. Even experts frequently complain of how frustrating and fickle the training process can be. One AI researcher at Stanford told me, I taught a course on deep learning and had all the students do their own projects. It was so hard. The students couldn’t get their models to train, and we were like “well, that’s deep learning”. Ali Rahimi, an AI researcher with over a decade of experience and winner of the NIPS 2017 Test of Time Award, complained about the brittleness of training in his NeurIPS award speech. How many of you have designed a deep net from scratch, built it from the ground up, architecture and all, and when it didn’t work, you felt bad about yourself? Rahimi asked the audience of AI researchers, and many raised their hands. Rahimi continued, This happens to me about every 3 months.\nThe fact that even AI experts sometimes have trouble training new models implies that the process has yet to be automated in a way where it could be incorporated into a general-purpose product. Some of the biggest advances in deep learning will come through discovering more robust training methods. We have already seen this some with advances like dropout, super convergence, and transfer learning, all of which make training easier. Through the power of transfer learning (to be discussed in Part 3) training can be a robust process when defined for a narrow enough problem domain; however, we still have a ways to go in making training more robust in general.\n\nFor Academic Researchers\n\nEven if you are working on theoretical machine learning research, it is useful to understand the process that machine learning practitioners working on practical problems go through, as that might provide insights on what the most relevant or high-impact areas of research are.\nAs Googler engineers D. Sculley et al. wrote, Technical debt is an issue that both engineers and researchers need to be aware of. Research solutions that provide a tiny accuracy benefit at the cost of massive increases in system complexity are rarely wise practice… Paying down technical debt is not always as exciting as proving a new theorem, but it is a critical part of consistently strong innovation. And developing holistic, elegant solutions for complex machine learning systems is deeply rewarding work. (emphasis mine)\n\nAutoML\n\nNow that we have an overview of some of the tasks that machine learning practitioners do as part of their work, we are ready to evaluate attempts to automate this work. As it’s name suggests, AutoML is one field in particular that has focused on automating machine learning, and a subfield of AutoML called neural architecture search is currently receiving a ton of attention. In part 2, I will explain what AutoML and neural architecture search are, and in part 3, look at Google’s AutoML in particular.\nBe sure to check out Part 2: An Opinionated Introduction to AutoML Neural Architecture Search, and Part 3: Google’s AutoML: Cutting Through the Hype"
  },
  {
    "objectID": "posts/2018-07-16-automl2/index.html",
    "href": "posts/2018-07-16-automl2/index.html",
    "title": "An Opinionated Introduction to AutoML and Neural Architecture Search",
    "section": "",
    "text": "This is part 2 in a series. Check out part 1 here and part 3 here.\nResearchers from CMU and DeepMind recently released an interesting new paper, called Differentiable Architecture Search (DARTS), offering an alternative approach to neural architecture search, a very hot area of machine learning right now. Neural architecture search has been heavily hyped in the last year, with Google’s CEO Sundar Pichai and Google’s Head of AI Jeff Dean promoting the idea that neural architecture search and the large amounts of computational power it requires are essential to making machine learning available to the masses. Google’s work on neural architecture search has been widely and adoringly covered by the tech media (see here, here, here, and here for examples).\n\n\n\nHeadlines from just a few of the many, many articles written about Google’s AutoML and Neural Architecture Search\n\n\nDuring his keynote (starts around 22:20) at the TensorFlow DevSummit in March 2018, Jeff Dean posited that perhaps in the future Google could replace machine learning expertise with 100x computational power. He gave computationally expensive neural architecture search as a primary example (the only example he gave) of why we need 100x computational power in order to make ML accessible to more people.\n\n\n\nSlide from Jeff Dean’s Keynote at the TensorFlow Dev Summit\n\n\nWhat is neural architecture search? Is it the key to making machine learning available to non-machine learning experts? I will dig into these questions in this post, and in my next post, I will look specifically at Google’s AutoML. Neural architecture search is a part of a broader field called AutoML, which has also been receiving a lot of hype and which we will consider first.\nPart 2 table of contents:\n\n\nWhat is AutoML?\n\n\nHow useful is AutoML?\n\n\nWhat is neural architecture search?\n\n\nWhat about DARTS?\n\n\nHow useful is Neural Architecture Search?\n\n\nHow else could we make machine learning practitioners more effective?\n\n\n\nWhat is AutoML?\n\nThe term AutoML has traditionally been used to describe automated methods for model selection and/or hyperparameter optimization. These methods exist for many types of algorithms, such as random forests, gradient boosting machines, neural networks, and more. The field of AutoML includes open-source AutoML libraries, workshops, research, and competitions. Beginners often feel like they are just guessing as they test out different hyperparameters for a model, and automating the process could make this piece of the machine learning pipeline easier, as well as speeding things up even for experienced machine learning practitioners.\nThere are a number of AutoML libraries, the oldest of which is AutoWEKA, which was first released in 2013 and automatically chooses a model and selects hyperparameters. Other notable AutoML libraries include auto-sklearn (which extends AutoWEKA to python), H2O AutoML, and TPOT. AutoML.org (formerly known as ML4AAD, Machine Learning for Automated Algorithm Design) has been organzing AutoML workshops at the academic machine learning conference ICML yearly since 2014.\n\nHow useful is AutoML?\n\nAutoML provides a way to select models and optimize hyper-parameters. It can also be useful in getting a baseline to know what level of performance is possible for a problem. So does this mean that data scientists can be replaced? Not yet, as we need to keep the context of what else it is that machine learning practitioners do.\nFor many machine learning projects, choosing a model is just one piece of the complex process of building machine learning products. As I covered in my previous post, projects can fail if participants don’t see how interconnected the various parts of the pipeline are. I thought of over 30 different steps that can be involved in the process. I highlighted two of the most time-consuming aspects of machine learning (in particular, deep learning) as cleaning data (and yes, this is an inseparable part of machine learning) and training models. While AutoML can help with selecting a model and choosing hyperparameters, it is important to keep perspective on what other data expertise is still needed and on the difficult problems remain.\nI will suggest some alternate approaches to AutoML for making machine learning practitioners more effective in the final section.\n\nWhat is neural architecture search?\n\nNow that we’ve covered some of what AutoML is, let’s look at a particularly active subset of the field: neural architecture search. Google CEO Sundar Pichai wrote that, “designing neural nets is extremely time intensive, and requires an expertise that limits its use to a smaller community of scientists and engineers. That’s why we’ve created an approach called AutoML, showing that  it’s possible for neural nets to design neural nets.”\nWhat Pichai refers to as using “neural nets to design neural nets” is known as neural architecture search; typically reinforcement learning or evolutionary algorithms are used to design the new neural net architectures. This is useful because it allows us to discover architectures far more complicated than what humans may think to try, and these architectures can be optimized for particular goals. Neural architecture search is often very computationally expensive.\nTo be precise, neural architecture search usually involves learning something like a layer (often called a “cell”) that can be assembled as a stack of repeated cells to create a neural network:\n\n\n\nDiagram from Zoph et. al. 2017. On the left is the full neural network of stacked cells, and on the right is the inside structure of a cell\n\n\nThe literature of academic papers on neural architecture search is extensive, so I will highlight just a few recent papers here:\n\nThe term AutoML jumped to “mainstream” prominence with work by Google AI researchers (paper here) Quoc Le and Barret Zoph, which was featured at Google I/O in May 2017. This work used reinforcement learning to find new architectures for the computer vision problem Cifar10 and the NLP problem Penn Tree Bank, and achieved similar results to existing architectures.\n\n\n\n\nDiagram from Le and Zoph’s blog post: the simpler architecture on the left was designed by a human and the more complicated architecture on the right was designed by a neural net.\n\n\n\nNASNet from Learning Transferable Architectures for Scalable Image Recognition (blog post here). This work searches for an architectural building block on a small data set (Cifar10) and then builds an architecture for a large data set (ImageNet). This research was very computationally intensive with it taking 1800 GPU days (the equivalent of almost 5 years for 1 GPU) to learn the architecture (the team at Google used 500 GPUs for 4 days!).\nAmoebaNet from Regularized Evolution for Image Classifier Architecture Search This research was even more computationally intensive than NASNet, with it taking the equivalent of 3150 GPU days (the equivalent of almost 9 years for 1 GPU) to learn the architecture (the team at Google used 450 K40 GPUs for 7 days!). AmoebaNet consists of “cells” learned via an evolutionary algorithm, showing that artificially-evolved architectures can match or surpass human-crafted and reinforcement learning-designed image classifiers. After incorporating advances from fast.ai such as an aggressive learning schedule and changing the image size as training progresses, AmoebaNet is now the cheapest way to train ImageNet on a single machine.\nEfficient Neural Architecture Search (ENAS): used much fewer GPU-hours than previously existing automatic model design approaches, and notably, was 1000x less expensive than standard Neural Architecture Search. This research was done using a single GPU for just 16 hours.\n\n\nWhat about DARTS?\n\nDifferentiable architecture search (DARTS). This research was recently released from a team at Carnegie Mellon University and DeepMind, and I’m excited about the idea. DARTS assumes the space of candidate architectures is continuous, not discrete, and this allows it to use gradient-based aproaches, which are vastly more efficient than the inefficient black-box search used by most neural architecture search algorithms.\n\n\n\nDiagram from DARTS, which treats the space of all possible architectures as continuous, not discrete\n\n\nTo learn a network for Cifar-10, DARTS takes just 4 GPU days, compared to 1800 GPU days for NASNet and 3150 GPU days for AmoebaNet (all learned to the same accuracy). This is a huge gain in efficiency! Although more exploration is needed, this is a promising research direction. Given how Google frequently equates neural architecture search with huge computational expense, efficient ways to do architecture search have most likely been under-explored.\n\nHow useful is Neural Architecture Search?\n\nIn his TensorFlow DevSummit keynote (starts around 22:20), Jeff Dean suggested that a significant part of deep learning work is trying out different architectures. This was the only step of machine learning that Dean highlighted in his short talk, and I was surprised by his emphasis. Sundar Pichai’s blog post contained a similar assertion.\n\n\n\nJeff Dean’s slide showing that neural architecture search can try 20 different models to find the most accurate\n\n\nHowever, choosing a model is just one piece of the complex process of building machine learning products. In most cases, architecture selection is nowhere near the hardest, most time-consuming, or most significant part of the problem. Currently, there is no evidence that each new problem would be best modeled with it’s own unique architecture, and most practitioners consider it unlikely this will ever be the case.\nOrganizations like Google working on architecture design and sharing the architectures they discover with the rest of us are providing an important and helpful service. However, the underlying architecture search method is only needed for that tiny fraction of researchers that are working on foundational neural architecture design. The rest of us can just use the architectures they find via transfer learning.\n\nHow else could we make machine learning practitioners more effective? AutoML vs. Augmented ML\n\nThe field of AutoML, including neural architecture search, has been largely focused on the question: how can we automate model selection and hyperparameter optimization? However, automation ignores the important role of human input. I’d like to propose an alternate question: how can humans and computers work together to make machine learning more effective? The focus of augmented ML is on figuring out how a human and machine can best work together to take advantage of their different strengths.\nAn example of augmented ML is Leslie Smith’s learning rate finder (paper here), which is implemented in the fastai library (a high level API that sits on top of PyTorch) and taught as a key technique in our free deep learning course. The learning rate is a hyperparameter that can determine how quickly your model trains, or even whether it successfully trains at all. The learning rate finder allows a human to find a good learning rate in a single step, by looking at a generated chart. It’s faster than AutoML approaches to the same problem, improves the data scientist’s understanding of the training process, and encourages more powerful multi-step approaches to training models.\n\n\n\nDiagram from Surmenok’s blog post on the learning rate finder, showing relationship between learning rate and loss\n\n\nThere’s another problem with the focus on automating hyperparameter selection: it overlooks the possibility that some types of model are more widely useful, have fewer hyperparameters to tune, and are less sensitive to choice of hyperparameters. For example, a key benefit of random forests over gradient boosting machines (GBMs) is that random forests are more robust, whereas GBMs tend to be fairly sensitive to minor changes in hyperparameters. As a result, random forests are widely used in industry. Researching ways to effectively remove hyperparameters (through smarter defaults, or through new models) can have a huge impact. When I first became interested in deep learning in 2013, it was overwhelming to feel that there were such a huge number of hyperparameters, and I’m happy that newer research and tools has helped eliminate many of those (especially for beginners). For instance, in the fast.ai course, beginners start by only having to choose a single hyperparameter, the learning rate, and we even give you a tool to do that!\n\nStay tuned…\n\nNow that we have an overview of what the fields of AutoML and neural architecture search are, we can take a closer look at Google’s AutoML in the next post.\nIf you haven’t already, check out Part 1: What is it that machine learning practitioners do? and Part 3: Google’s AutoML: Cutting Through the Hype of this series.\nPlease be sure to check out Part 3 of this post next week!"
  },
  {
    "objectID": "posts/2018-07-23-automl3/index.html",
    "href": "posts/2018-07-23-automl3/index.html",
    "title": "Google’s AutoML: Cutting Through the Hype",
    "section": "",
    "text": "This is part 3 in a series. Part 1 is here and Part 2 is here.\nTo announce Google’s AutoML, Google CEO Sundar Pichai wrote, “Today, designing neural nets is extremely time intensive, and requires an expertise that limits its use to a smaller community of scientists and engineers. That’s why we’ve created an approach called AutoML, showing that it’s possible for neural nets to design neural nets. We hope AutoML will take an ability that a few PhDs have today and will make it possible in three to five years for hundreds of thousands of developers to design new neural nets for their particular needs.” (emphasis mine)\n\n\n\nGoogle CEO Sundar Pichai says that we all need to design our own neural nets\n\n\nWhen Google’s Head of AI, Jeff Dean, suggested that 100x computational power could replace the need for machine learning expertise, computationally expensive neural architecture search was the only example he gave to illustrate this point. (around 23:50 in his TensorFlow DevSummit keynote)\nThis raises a number of questions: do hundreds of thousands of developers need to “design new neural nets for their particular needs” (to quote Pichai’s vision), or is there an effective way for neural nets to generalize to similar problems? Can large amounts of computational power really replace machine learning expertise?\nIn evaluating Google’s claims, it’s valuable to keep in mind Google has a vested financial interest in convincing us that the key to effective use of deep learning is more computational power, because this is an area where they clearly beat the rest of us. If true, we may all need to purchase Google products. On its own, this doesn’t mean that Google’s claims are false, but it’s good be aware of what financial motivations could underlie their statements.\nIn my previous posts, I shared an introduction to the history of AutoML, defined what neural architecture search is, and pointed out that for many machine learning projects, designing/choosing an architecture is nowhere near the hardest, most time-consuming, or most painful part of the problem. In today’s post, I want to look specifically at Google’s AutoML, a product which has received a lot of media attention, and address the following:\n\n\nWhat is Google’s AutoML?\n\n\nWhat is transfer learning?\n\n\nNeural architecture search vs. transfer learning: two opposite approaches\n\n\nIn need of more evidence\n\n\nWhy all the hype about Google’s AutoML?\n\n\nHow can we address the shortage of machine learning expertise?\n\n\n\nWhat is Google’s AutoML?\n\nAlthough the field of AutoML has been around for years (including open-source AutoML libraries, workshops, research, and competitions), in May 2017 Google co-opted the term AutoML for its neural architecture search. In blog posts accompanying announcements made at the conference Google I/O, Google CEO Sundar Pichai wrote, “That’s why we’ve created an approach called AutoML, showing that it’s possible for neural nets to design neural nets” and Google AI researchers Barret Zoph and Quoc Le wrote “In our approach (which we call”AutoML”), a controller neural net can propose a “child” model architecture…“\nGoogle’s Cloud AutoML was announced in January 2018 as a suite of machine learning products. So far it consists of one publicly available product, AutoML Vision, an API that identifies or classifies objects in pictures. According to the product page, Cloud AutoML Vision relies on two core techniques: transfer learning and neural architecture search. Since we’ve already explained neural architecture search, let’s now take a look at transfer learning, and see how it relates to neural architecture search.\n\n\n\nHeadlines from just a few of the many, many articles written about Google’s AutoML and Neural Architecture Search\n\n\nNote: Google Cloud AutoML also has a drag-and-drop ML product that is still in alpha. I applied for access to it over 2 months ago, but I have not heard back from Google yet. I plan to write a post once it’s released.\n\nWhat is transfer learning?\n\nTransfer learning is a powerful technique that lets people with smaller datasets or less computational power achieve state-of-the-art results, by taking advantage of pre-trained models that have been trained on similar, larger data sets. Because the model learned via transfer learning doesn’t have to learn from scratch, it can generally reach higher accuracy with much less data and computation time than models that don’t use transfer learning.\nTransfer learning is a core technique that we use throughout our free Practical Deep Learning for Coders course– and that our students have been applying in production in everything from their own startups to Fortune 500 companies. Although transfer learning seems to be considered “less sexy” than neural architecture search, it is being used to achieve ground-breaking academic results, such as in Jeremy Howard and Sebastian Ruder’s application of transfer learning to NLP, which achieved state-of-the-art classification on 6 datasets and is serving as a basis for further research in this area at OpenAI.\n\nNeural architecture search vs. transfer learning: two opposing approaches\n\nThe underlying idea of transfer learning is that neural net architectures will generalize for similar types of problems: for example, that many images have underlying features (such as corners, circles, dog faces, or wheels) that show up in a variety of different types of images. In contrast, the underlying idea of promoting neural architecture search for every problem is the opposite: that each dataset has a unique, highly specialized architecture it will perform best with.\n\n\n\nExamples from Matthew Zeiler and Rob Fergus of 4 features learned by image classifiers: corners, circles, dog faces, and wheels\n\n\nWhen neural architecture search discovers a new architecture, you must learn weights for that architecture from scratch, while with transfer learning, you begin with existing weights from a pre-trained model. In this sense, you you can’t use neural architecture search and transfer learning on the same problem: if you’re learning a new architecture, you would need to train new weights for it; whereas if you are using transfer learning on a pretrained model, you can’t make substantial changes to the architecture.\nOf course, you can apply transfer learning to an architecture learned through neural architecture search (which I think is a good idea!). This requires only that a few researchers use neural architecture search and open-source the models that they find. It is not necessary for all machine learning practitioners to be using neural architecture search themselves on all problems when they can instead use transfer learning. However, Jeff Dean’s keynote, Sundar Pichai’s blog post, Google Cloud’s promotional materials, and the media coverage all suggest the opposite: that everybody needs to be able to use neural architecture search directly.\n\nWhat Neural Architecture Search is good for\n\nNeural architecture search is good for finding new architectures! Google’s AmoebaNet was learned via neural architecture search, and (with the inclusion of fast.ai advances such as an aggressive learning schedule and changing the image size as training progresses) is now the cheapest way to train ImageNet on a single machine!\nAmoebaNet was not designed with a reward function that involved the ability to scale, and so it didn’t scale as well as ResNet to multiple machines, but a neural net that scales well could potentially be learned in the future, optimized for different qualities.\n\nIn need of more evidence\n\nWe haven’t seen evidence that every dataset would be best modeled with its own custom model, as opposed to instead fine-tuning an existing model. Since neural architecture search requires a larger training set, this would particularly be an issue for smaller data sets. Even some of Google’s own research uses transferable techniques instead of finding a new architecture for each data set, such as NASNet (blog post here), which learned an architectural building block on Cifar10 and then used that building block to create an architecture for ImageNet. I don’t know of any widely-entered machine learning competitions that have been won using neural architectures search yet.\nFurthermore, we don’t know that the mega-computationally expensive approach to neural architecture search that Google touts is the superior approach. For instance, more recent papers such as Efficient Neural Architecture Search (ENAS) and\nDifferentiable architecture search (DARTS) propose significantly more efficient algorithms. DARTS takes just 4 GPU days, compared to 1800 GPU days for NASNet and 3150 GPU days for AmoebaNet (all learned to the same accuracy on Cifar-10). Jeff Dean is an author on the ENAS paper, which proposed a technique that is 1000x less computationally expensive, which seems inconsistent with his emphasis at the TF DevSummit one month later on using approaches that are 100x more computationally expensive.\n\nThen why all the hype about Google’s AutoML?\n\nGiven the above limitations, why has Google AutoML’s hype been so disproportionate to its proven usefulness (at least so far)? I think there are a few explanations:\n\nGoogle’s AutoML highlights some of the dangers of having an academic research lab embedded in a for-profit corporation. There is a temptation to try to build products around interesting academic research, without assessing if they fulfill an actual need. This is also the story of many AI start-ups, such as MetaMind or Geometric Intelligence, that end up as acquihires without ever having produced a product. My advice for startup founders is to avoid productionizing your PhD thesis and to avoid hiring only academic researchers.\nGoogle excels at marketing. Artificial intelligence is seen as an inaccessible and intimidating field by many outsiders, who don’t feel that they have a way to evaluate claims, particularly from lionized companies like Google. Many journalists fall prey to this as well, and uncritically channel Google’s hype into glowing articles. I periodically talk to people that do not work in machine learning, yet are excited about various Google ML products that they’ve never used and can’t explain anything about.\nOne example of Google’s misleading coverage of its own achievements occurred when Google AI researchers released “a deep learning technology to reconstruct the true human genome”, compared their own work to Nobel prize-winning discoveries (the hubris!), and the story was picked up by Wired. However, Steven Salzberg, a distinguished professor of Biomedical Engineering, Computer Science, and Biostatistics at Johns Hopkins University debunked Google’s post. Salzberg pointed out that the research didn’t actually reconstruct the human genome and was “little more than an incremental improvement over existing software, and it might be even less than that.” A number of other genomics researchers chimed in to agree with Salzberg.\nThere is some great work happening at Google, but it would be easier to appreciate if we didn’t have to sift through so much misleading hype to figure out what is legitimate.\n\n\n\nGoogle’s DeepVariant “is little more than an incremental improvement over existing software, and it might be even less than that.” @StevenSalzberg1 What do others genomics researchers think?https://t.co/vaAECQhvSi\n\n— Rachel Thomas (@math_rachel) December 12, 2017\n\n\n\nGoogle has a vested interest in convincing us that the key to effective use of deep learning is more computational power, because this is an area where they clearly beat the rest of us. AutoML is often very computationally expensive, such as in the examples of Google using 450 K40 GPUs for 7 days (the equivalent of 3150 GPU days) to learn AmoebaNet.\nWhile engineers and the media often drool over bare-metal power and anything bigger, history has shown that innovation is often birthed instead by constraint and creativity. Google works on the biggest data possible using the most expensive computers possible; how well can this really generalize to the problems that the rest of us face living in a constrained world of limited resources?\nInnovation comes from doing things differently, not from doing things bigger. The recent success of fast.ai in the Stanford DAWNBench competition is one example of this.\n\n\n\n\nInnovation come from doings things differently, not doing things bigger. @jeremyphoward https://t.co/3TJYs8OCbr pic.twitter.com/I55a6gT1OF\n\n— Rachel Thomas (@math_rachel) May 2, 2018\n\n\n\n\nHow can we address the shortage of machine learning expertise?\n\nTo return to the issue that Jeff Dean raised in his TensorFlow DevSummit keynote about the global shortage of machine learning practitioners, a different approach is possible. We can remove the biggest obstacles to using deep learning in several ways by:\n\nmaking deep learning easier to use\ndebunking myths about what it takes to do deep learning\nincreasing access for people that lack the money or credit cards needed to use a cloud GPU\n\n\nMaking Deep Learning Easier to Use\n\nResearch to make deep learning easier to use has a huge impact, making it faster and simpler to train better networks. Examples of exciting discoveries that have now become standard practice are:\n\nDropout allows training on smaller datasets without over-fitting.\nBatch normalization allows for faster training.\nRectified linear units help avoid gradient explosions.\n\nNewer research to improve ease of use includes: - The learning rate finder makes the training process more robust. - Super convergence speeds up training, requiring fewer computational resources. - “Custom heads” for existing architectures (e.g. modifying ResNet, which was initially designed for classification, so that it can be used to find bounding boxes or perform style transfer) allow for easier architecture reuse across a range of problems.\nNone of the above discoveries involve bare-metal power; instead, all of them were creative ideas of ways to do things differently.\n\nAddress Myths About What it Takes to Do Deep Learning\n\nAnother obstacle is the many myths that cause people to believe that deep learning isn’t for them: falsely believing that their data is too small, that they don’t have the right education or background, or that their computers aren’t big enough. One such myth says that only machine learning PhDs are capable of using deep learning, and many companies that can’t afford to hire expensive experts don’t even bother trying. However, it’s not only possible for companies to train the employees they already have to become machine learning experts, it’s even preferable, because your current employees already have domain expertise for the area you work in!\nFor the vast majority of people I talk with, the barriers to entry for deep learning are far lower than they expected: one year of coding experience and access to a GPU.\n\nIncreasing Access: Google Colab Notebooks\n\nAlthough the cost of cloud GPUs (around 50 cents per hour) are within the budgets of many of us, I’m periodically contacted by students from around the world that can’t afford any GPU use at all. In some countries, rules about banking and credit cards can make it difficult for students to use services like AWS, even when they have the money. Google Colab notebooks are a solution! Colab notebooks provide a Jupyter notebook environment that requires no setup to use, runs entirely in the cloud, and gives users access to a free GPU (although long-running GPU use is not allowed). They can also be used to create documentation that contains working code samples running in an interactive environment. Google colab notebooks will do much more to democratize deep learning than Google’s AutoML will; perhaps this would be a better target for Google’s marketing machine in the future.\nIf you haven’t already, check out Part 1: What is it that machine learning practitioners do? and Part 2: An Opinionated Introduction to AutoML Neural Architecture Search of this series."
  },
  {
    "objectID": "posts/2018-08-07-hbr-bias-algorithms/index.html",
    "href": "posts/2018-08-07-hbr-bias-algorithms/index.html",
    "title": "What Harvard Business Review Gets Wrong About Algorithms and Bias",
    "section": "",
    "text": "The Harvard Business Review recently published an article, Want Less-Biased Decisions? Use Algorithms. by Alex P. Miller. The article focuses on the fact that humans make very biased decisions (which is true), yet ignores many important related issues, including:\n\nalgorithms are often implemented without any appeals method in place (due to the misconception that algorithms are objective, accurate, and won’t make mistakes)\nalgorithms are often used at a much larger scale than human decision makers, in many cases, replicating an identical bias at scale (part of the appeal of algorithms is how cheap they are to use)\nusers of algorithms may not understand probabilities or confidence intervals (even if these are provided), and may not feel comfortable overriding the algorithm in practice (even if this is technically an option)\ninstead of just focusing on the least-terrible existing option, it is more valuable to ask how we can create better, less biased decision-making tools by leveraging the strengths of humans and machines working together\n\n\n\n\nPhoto Credit: Jonathan Lidbeck https://www.flickr.com/photos/jondissed/2278335691\n\n\nMiller acknowledges that critics of the “algorithmic revolution” are “concerned that algorithms are often opaque, biased, and unaccountable tools being wielded in the interests of institutional power”, although he then focuses exclusively on the biased part for the remainder of the article, without addressing the opaque or unaccountable charges (as well as how these interact with bias).\n\nHumans vs. machines is not a helpful framing\n\nThe media often frames advances in AI through a lens of humans vs. machines: who is the champion at X task. This framework is both inaccurate as to how most algorithms are used, as well as a very limited way to think about AI. In all cases, algorithms have a human component, in terms of who gathers the data (and what biases they have), which design decisions are made, how they are implemented, how results are used to make decisions, the understanding various stakeholders have of correct uses and limitations of the algorithm, and so on.\nMost people working on medical applications of AI are not trying to replace doctors; they are trying to create tools that will allow doctors to be more accurate and more efficient, improving quality of care. The best chess “players” are neither humans nor computers, but rather, teams of humans and computers working together.\nMiller’s HBR article points out (correctly) that humans are very biased, and then compares our current not-so-great approaches to see which is less terrible. The article does not ask the question, how can we develop less biased ways to make decisions (perhaps using some combination of humans and algorithms)? which is a far more interesting and important question.\n\nAlgorithms are often used differently than human decision makers\n\nAlgorithms are often used at a larger scale, mass-producing identical biases, and assumed to be error-proof or objective. The studies that Miller shares compares them in an apples-to-apples way, which doesn’t acknowledge how differently they are often used in practice.\nCathy O’Neil writes in Weapons of Math Destruction that the algorithms she is critiquing tend to punish the poor. They specialize in bulk, and they’re cheap. That’s part of their appeal. The wealthy, by contrast, often benefit from personal input. A white-shoe law firm or an exclusive prep school will lean far more on recommendations and face-to-face interviews than will a fast-food chain or a cash-strapped urban school district. The privileged, we’ll see time and again, are processed more by people, the masses by machines. (emphasis mine)\nOne example from O’Neil’s book is that of a college student with bipolar disorder who wanted to get a summer job bagging groceries. Every store he applied to was using the same pyschometric evaluation software to screen candidates, and he was rejected from every store. This captures another danger of algorithms: even though humans often have similar biases, not all humans will make the exact same decisions (e.g. perhaps that college student would have been able to find one place to hire him, even if some of the people making decisions had biases about mental health).\nMany people will put more trust in algorithmic decisions than they might in human decisions. While the researchers designing the algorithms may have a good grasp on probability and confidence intervals, often the general public using them will not. Even if people are given the power to override algorithmic decisions, it is crucial to understand if they will feel comfortable doing so in practice.\n\nThe need for meaningful appeals and explanations\n\nMany of the most chilling stories of algorithmic bias don’t involve meaningful explanations or a meaningful appeals process. This seems to be a particular trend amongst algorithmic decision making systems, perhaps since people mistakenly assume algorithms are objective, they believe there is no need for appeals. Also, as explained above, algorithmic decision making systems are often used as a cost-cutting device, and allowing appeals would be more expensive.\nCathy O’Neil writes the account of a teacher who is beloved by her students, their parents, and the principal, yet is inexplicably fired by an algorithm. She is never able to get an answer as to why she was fired. Stories like this would be somewhat less disturbing if there had been a relatively quick and simple way for her to appeal the decision, or even to know for sure what factors it was related to.\nThe Verge investigated software used in over half of U.S. states to determine how much healthcare people receive. After its implemention in Arkansas, people (many with severe disabilities) drastically had their healthcare cut. For instance, Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week. She couldn’t get any explanation for why her healthcare was cut. Eventually, a court case revealed that there were mistakes in the software implementation of the algorithm, negatively impacting people with diabetes or cerebral palsy. However, Dobbs and many other people reliant on these health care benefits live in fear that their benefits could again be cut suddenly and inexplicably.\n\n\n\nimage source: wikimedia commons\n\n\nThe creator of the algorithm, who is a professor and earning royalties off of this software, was asked whether there should be a way to communicate decisions, “It’s probably something we should do. I should also probably dust under my bed.” He later clarified that he thought it was someone else’s responsibility. We can not keep claiming the problems caused by our technology are someone else’s responsibility.\nFor a separate computer system used in Colorado to determine public benefits in the mid-2000s, it was discovered that more than 900 incorrect rules had been coded into the system, resulting in problems like pregnant women being denied Medicaid. It is often hard for lawyers to even discover these flaws, since the inner-workings of the algorithms are typically protected as trade secrets. Systems used to make decisions related to healthcare, hiring/firing, criminal justice, and other life-altering areas should include some sort of human appeals process, that is relatively fast and easy to navigate. Many of the most chilling stories of algorithmic decision making would not be nearly as concerning if there had been an easy way to appeal and correct faulty decisions. Mistakes are possible in anything we do, so it’s important to have a tight loop in which we make it easy to discover and correct mistakes.\n\nComplicated, real-world systems\n\nWhen we think about AI, we need to think about complicated, real-world systems. The studies in the HBR article treat decision making as an isolated action, without taking into account that this decision-making happens within complicated real-world systems. A decision about whether someone is likely to commit another crime is not an isolated decision: it lives within the complicated system of our criminal justice system. We have a responsibility to understand the real-world systems with which our work will interact, and to not lose sight of the actual people who will be impacted.\nThe COMPAS recidivism algorithm is used in some US courtrooms for decisions related to pre-trial bail, sentencing, and parole. It was the subject of a ProPublica investigation finding that the false positive rate (people that were labeled “high risk” but were not re-arrested) for white defendants was 24% and for Black defendants was 45%. Later research found that COMPAS (which uses 137 inputs in a black-box algorithm) was no more accurate than a simple linear equation on two variables. COMPAS was also not more accurate than untrained Mechanical Turk workers. (You can find out more about various definitions of fairness in Princeton CS Professor Arvind Narayanan’s excellent 21 Definitions of Fairness talk).\nKristian Lum, statistics PhD and lead data scientist at the Human Rights Digital Analysis Group, organized a workshop together with Elizabeth Bender, a staff attorney for the NY Legal Aid Society and former public defender, and Terrence Wilkerson, an innocent man who had been arrested and could not afford bail. Together, they shared first hand experience about the obstacles and inefficiencies that occur in the legal system, providing valuable context to the debate around COMPAS. Bender shared that for public defenders meet with defendants at Rikers Island, where many pre-trial detainees in NYC who can’t afford bail are held, involves a bus ride that is two hours each way and they then only get 30 minutes to see the defendant, assuming the guards are on time (which is not always the case). Wilkerson explained how frequently innocent defendents who can’t afford bail accept guilty plea bargains just so they can get out of jail faster. Again, all this is for people that have not even faced a trial yet! This panel was an excellent way to illuminate the real-world systems and educate about the first-hand impact. I hope more statisticians and computer scientists will follow this example.\n\n\n\nDr. Kristian Lum’s Panel at the Fairness, Accountability, and Transparency in ML Conference\n\n\nAs this example shows, algorithms can often exacerbate underlying societal problems. There are deep, structural problems with the US courts and prison systems, including racial bias, the use of cash bail (nearly half a million people in the USA are languishing in jail before even facing a trial, because they are too poor to afford bail), predatory for-profit prisons, and extreme over-use of prisons (the US is home to 4% of the world’s population and 22% of the world’s prisoners). We have a responsibility to understand the systems and underlying problems our algorithms may interact with.\n\nMost critics of unjust bias aren’t anti-algorithm\n\nMost critics of biased algorithms are opposed to unjust bias; they are not people who hate algorithms. Miller says that critics of biased algorithms “rarely ask how well the systems they analyze would operate without algorithms,” suggesting that those speaking out against biased algorithms are perhaps unaware of how biased humans are or perhaps just don’t like algorithms. I spent a great deal of time researching and writing about studies of human bias (particularly as to how they pertain to the tech industry), long before I began writing about bias in machine learning.\nWhen I tweet or share about biased or unethical algorithms, I frequently encounter push-back that I must be anti-algorithms or opposed to tech. This couldn’t be further from the truth: I have a PhD in math; I’ve worked as a quant, data scientist, and software engineer; I created a free, online Computational Linear Algebra course and co-founded fast.ai, which runs Practical Deep Learning for Coders and won Stanford’s Computer Vision Speed Test through clever use of algorithms.\nI’m in no way unique in this: most of the outspoken critics of biased algorithms that come to mind have PhDs in computer science, math, or statistics, and continue to be active in their fields. Just check out some of the speakers from the Fairness Accountability and Transparency Conference (and watch their talks!). One such example is Arvind Narayanan, a computer science professor at Princeton, winner of the Kaggle Social Network Challenge, teacher of a popular cryptocurrency course, and also speaks out against algorithmic bias.\nI hope that the popular discussion of biased algorithms can move beyond unnuanced rebuttals and more deeply engage with the issues involved."
  },
  {
    "objectID": "posts/2019-01-21-should-know/index.html",
    "href": "posts/2019-01-21-should-know/index.html",
    "title": "Things everyone should know about chronic illness & disability",
    "section": "",
    "text": "This was originally published on Medium as part of a longer post, which I have now split in two."
  },
  {
    "objectID": "posts/2019-01-21-should-know/index.html#health-is-not-binary",
    "href": "posts/2019-01-21-should-know/index.html#health-is-not-binary",
    "title": "Things everyone should know about chronic illness & disability",
    "section": "Health is not binary",
    "text": "Health is not binary\nIllness & disability can fluctuate. Perhaps some days a person needs a wheelchair or cane and other days they don’t. Some days they are capable of tasks that other days they can’t do. Google developer advocate, Julia Ferraioli, who has had a vision impairment off-and-on for the last two years, shared in her GopherCon keynote, that “Disability is not binary. There are some days when I can churn out code, and some when I can’t read anything at all.”\nI have some dramatic stories of my most acute health crises, but what is harder for me to explain is the long stretches of time where my health was in-between — when I was not fully well, but could give high-profile talks (although it would take days to recover), attend conferences (even if just for a few hours each day), or teach (even if I felt awful and spent all my time outside of class resting). I hear similar accounts from many people whose symptoms fluctuate over time, or who experience flares alternating with periods of reduced symptoms. Even the recovery from a particular illness or surgery is rarely linear, and a better week may be followed by a worse week. Many people worry that if they share about their disability, people may accuse them of “faking it” if they are not perfectly consistent in their behavior.\n\n\n\nImage of wheelchair. Source: https://www.publicdomainpictures.net/en/view-image.php?image=167689\n\n\nGabe Moses, who often uses a wheelchair and sometimes uses crutches, wrote a post addressing how wheelchair users are often wrongly accused of faking their disability. Gabe has unstable joints, unpredictable muscle spasms, and sudden numbness, which makes getting around without a wheelchair exhausting, painful, and dangerous (although possible, at least sometimes). He writes, “The black-and-white narrative of disability we’ve been sold makes people who haven’t experienced disability themselves see wheelchair users this way: wheelchair users are people who can’t walk, at all. Anyone else who dares to use one is either lazy or faking. The truth is, wheelchair users are a highly varied group of people. Some people do need their wheelchairs because of paralysis or other conditions that make walking impossible. Others use them because of fatigue, chronic pain, balance problems, or other conditions that make it impossible to walk long distances, even if they are capable of standing and walking to some degree.”"
  },
  {
    "objectID": "posts/2019-01-21-should-know/index.html#illness-disability-can-be-invisible",
    "href": "posts/2019-01-21-should-know/index.html#illness-disability-can-be-invisible",
    "title": "Things everyone should know about chronic illness & disability",
    "section": "Illness & disability can be invisible",
    "text": "Illness & disability can be invisible\n“You seem like you are doing great!” people tell me when I bring up my health challenges. Sometimes people who see me seeming upbeat during a social event or watch one of my talks assume that I must be fully recovered. Whereas in reality, it is common to feel ok at some times but ill at others (and this can vary from week to week, or even throughout a given day), to appear healthier than I feel inside (particularly if I’m trying to show my best side at a professional event), or to need an unusually long amount of time to recover from events where I seemed “normal”. People are complex and multi-faceted, and you can’t assess their health based on appearance or mood.\nProduct designer Amélie Lamont shared that while on disability leave from work, she spoke at the conference (after clearing it with her doctor and after having had to cancel her talk the previous year due to her health). It took her days to recover, yet her employer assumed since she seemed fine during the 30-minute talk, that she must be well enough to return to work full-time. In doing so, her employer was failing to understand that illness can be invisible, and that Amélie and her doctor are the only ones who can determine when she is actually ready to return to work.\n\n\n\nImage of tweet saying, “The top two recommended accounts for the #NIPS2017 hashtag are women! math_rachel and hannawallach”\n\n\nTo give one small personal example, at NeurIPS 2017, I attended interesting talks, mentored at two round table discussions for the Women in ML workshop (which inspired me to later write blog posts on personal branding and founding startups), and met many new people. At one point, I was one of the two top people recommended to follow for the #NIPS2017 hashtag. Unless I spoke about it, most people I talked to at the conference probably wouldn’t have guessed how unwell I was (I was recovering from a life-threatening brain infection, and had an infected brain cyst which would require an additional surgery), or that I only attended a few hours of talks or events each day, took breaks to lie down on the floor of the conference center, had daily headaches, and went to bed early each evening. This paints two different pictures of my time at the same event.\nA friend, who has a disease which impacts her mobility, told me that at NeurIPS 2018 there were seats that were supposed to be reserved for people with disabilities, yet they were always full. Since my friend “looks healthy”, she felt too uncomfortable to ask someone else who “looks healthy” if she could have one of these seats; perhaps they have an invisible illness as well. She also felt unsure whether she even wanted other people in the AI community to know about her disability, as this would change how they view her."
  },
  {
    "objectID": "posts/2019-01-21-should-know/index.html#trust-people-to-understand-their-own-bodies-experiences",
    "href": "posts/2019-01-21-should-know/index.html#trust-people-to-understand-their-own-bodies-experiences",
    "title": "Things everyone should know about chronic illness & disability",
    "section": "Trust people to understand their own bodies & experiences",
    "text": "Trust people to understand their own bodies & experiences\nMany people with illness and disabilities have spent years trying to get diagnosed, to get their symptoms taken seriously, to get treatments they needed approved or covered by insurance, or to get accommodations they need at work. For instance, a study in the UK found that almost a third of patients with a brain tumor had to visit a doctor more than 5 times before receiving their diagnosis (these times are longer for women and low-income patients). One woman with a brain tumor recalled: “One of the GPs I saw actually made fun of me, saying ‘what did I think my headaches were, a brain tumor?’ I had to request a referral to neurology. I went back repeated times to be given antidepressants, sleep charts, analgesia, etc. No one took me seriously.” On average, patients with autoimmune diseases have to visit 5 doctors over a 5-year period before receiving a proper diagnosis.\nResearch shows that doctors take the pain of women less seriously than the pain of men, and the pain of people of color less seriously than the pain of white people (resulting in longer time delays to receive treatment, lower quality of care, and worse outcomes). Since so many doctors were dismissive of her symptoms, it took author Aubrey Hirsch 6 years and a dozen doctors to get her Grave’s disease diagnosed. By that point, the disease had done permanent damage to her bones, heart, and eyes, which could have been prevented by earlier treatment. Hundreds of people responded to the powerful comic she created with their own experiences of having severe symptoms dismissed.\n\n\n\nImage of an ICU from Wikimedia Commons\n\n\nA few years ago, I went to the ER in the worst pain of my life, and was given no tests, but just sent home with aspirin (it turned out I actually needed brain surgery). Another time, I had not slept in days because of heart problems, and an ER doctor told me to “relax and take melatonin” when in reality my PICC line had been inserted 5cm too deep and was mistakenly inside my heart. I was once not believed about having dislocated my shoulder, which later resulted in my needing a surgery which my surgeon said could have been avoided if I had gotten treatment at that earlier time."
  },
  {
    "objectID": "posts/2019-01-21-should-know/index.html#the-trauma-of-the-medical-system",
    "href": "posts/2019-01-21-should-know/index.html#the-trauma-of-the-medical-system",
    "title": "Things everyone should know about chronic illness & disability",
    "section": "The trauma of the medical system",
    "text": "The trauma of the medical system\nIf you don’t have first-hand experience with chronic illness or disability, it can be difficult to understand how encompassing it can be. If you do not have first-hand experience trying to navigate the byzantine and traumatizing medical system, it can be surprising just how complicated, error-prone, and often hostile to patients it is. A growing body of research is linking medical trauma to PTSD.\n\n\nreasons it can be hard to disclose an invisible disability https://t.co/RK15Bc8IQG pic.twitter.com/CdxsoATLLa\n\n— Rachel Thomas (@math_rachel) December 11, 2018\n\n\nTweet showing a paragraph from this blog post, listing reasons it is hard to disclose an invisible disability.\nLiz Allen, who works in tech and has a complex range of immune issues, gives many reasons why she was hesitant to ask HR for the accommodations she needed, including that her story is long and traumatic to retell, educating others can be emotionally exhausting, she is often doubted or disbelieved, and her fear of being viewed as incompetent. Remember that it takes bravery for someone to share about their health with you; honor that and respond in a supportive and accommodating way."
  },
  {
    "objectID": "posts/2019-01-21-should-know/index.html#avoid-giving-unsolicited-health-advice.",
    "href": "posts/2019-01-21-should-know/index.html#avoid-giving-unsolicited-health-advice.",
    "title": "Things everyone should know about chronic illness & disability",
    "section": "Avoid giving unsolicited health advice.",
    "text": "Avoid giving unsolicited health advice.\nMost people with chronic illnesses or disabilities have spent hundreds of hours researching their conditions, met with dozens of specialists, and tried a variety of treatments (both medical and lifestyle). Even when it’s well-intentioned, offering unsolicited health advice can come across as patronizing or dismissive. There is a huge amount of variety in different conditions and what works for different people, so avoid statements like “{yoga, x diet, physical therapy} is good for everyone.” Know that if a simple solution existed, the person would have already figured it out. Trust people to know their own bodies and to be experts on their own lives."
  },
  {
    "objectID": "posts/2019-01-21-should-know/index.html#the-limits-of-language",
    "href": "posts/2019-01-21-should-know/index.html#the-limits-of-language",
    "title": "Things everyone should know about chronic illness & disability",
    "section": "The limits of language",
    "text": "The limits of language\n\n\n\nImage from Wikimedia Commons of a Latin Dictionary\n\n\nDon’t compare everyday experiences to someone else’s chronic condition. Feeling tired is not the same as chronic fatigue. Being sad is not the same as clinical depression. Doing Vipassana meditation for an hour is not the same as living with chronic pain for years. Part of this is a failure of language: words like “pain” and “fatigue” are used to describe a wide range of experiences. A friend with multiple sclerosis told me how regular fatigue (after a hike or long day) is very different from multiple sclerosis fatigue (which can involve nerve pain, muscle pain, and double vision), yet they are described with the same word, which leads to poor understanding. Brianne Benness, who has a chronic illness and started a podcast about chronic illness, also spoke about this limit of language, in response to an interviewee with epilepsy, Ehlers-Danlos syndrome, and cochlear implants, “I think I end up talking with people a lot right now about how we don’t have good words for what that feels like.” People who have never experienced severe pain may not be able to imagine what it feels like, yet it is still possible and necessary to practice empathy."
  },
  {
    "objectID": "posts/2019-01-21-should-know/index.html#medical-leave-is-not-a-vacation",
    "href": "posts/2019-01-21-should-know/index.html#medical-leave-is-not-a-vacation",
    "title": "Things everyone should know about chronic illness & disability",
    "section": "Medical Leave is not a “vacation”",
    "text": "Medical Leave is not a “vacation”\nDon’t tell someone that you wish you could spend the day in bed or take a month off from work like them — nobody wants to be in pain or suffering. Comparing medical leave to a “vacation” is dismissive of the pain, limitations, and actual work that go into medical leave. I also want to address the concern that an employee going on disability leave or requiring special accomodations results in extra work for their coworkers. If this does occur, it is not the fault of the ill or disabled person; it is the fault of the employer for not adjusting workloads, dropping lower priority tasks, or hiring additional temporary help (ditto for when child-free people are given extra work to cover for parents — this is the employer’s fault for not ensuring humane and fair work conditions for everyone). While the moral and legal cases for inclusion should be more than enough, note that companies that champion people with disabilities outperform other companies financially, with 28% higher revenues, 200% higher net income, and 30% higher profit margins."
  },
  {
    "objectID": "posts/2019-01-21-should-know/index.html#listen-to-other-peoples-experiences",
    "href": "posts/2019-01-21-should-know/index.html#listen-to-other-peoples-experiences",
    "title": "Things everyone should know about chronic illness & disability",
    "section": "Listen to other people’s experiences",
    "text": "Listen to other people’s experiences\nEven if you don’t realize it, you are interacting with chronically ill and disabled people regularly. I hope the experiences, advice, and quotes shared in this post will be helpful in increasing empathy and understanding, so that we can make the tech industry more inclusive. I don’t and can’t speak for everyone with chronic illnesses or disabilities, so I hope that you will seek out and listen to the experiences of other people with chronic illnesses or disabilities (and the links in this post are one way to get started!)\nThis is part of a series. The next post covers one aspect of the tech industry that excludes many people with disabilities (and is contrary to productivity research): the glorification of long hours.\nA huge thank you to Julia Ferraioli, Jeremy Howard, and Negar Rostamzadeh for giving me valuable feedback on earlier drafts of this post."
  },
  {
    "objectID": "posts/2019-01-29-five-scary-things/index.html",
    "href": "posts/2019-01-29-five-scary-things/index.html",
    "title": "Five Things That Scare Me About AI",
    "section": "",
    "text": "AI is being increasingly used to make important decisions. Many AI experts (including Jeff Dean, head of AI at Google, and Andrew Ng, founder of Coursera and deeplearning.ai) say that warnings about sentient robots are overblown, but other harms are not getting enough attention. I agree. I am an AI researcher, and I’m worried about some of the societal impacts that we’re already seeing. In particular, these 5 things scare me about AI:\n\n\nAlgorithms are often implemented without ways to address mistakes.\n\n\nAI makes it easier to not feel responsible.\n\n\nAI encodes & magnifies bias.\n\n\nOptimizing metrics above all else leads to negative outcomes.\n\n\nThere is no accountability for big tech companies.\n\n\nAt the end, I’ll briefly share some positive ways that we can try to address these.\nBefore we dive in, I need to clarify one point that is important to understand: algorithms (and the complex systems they are a part of) can make mistakes. These mistakes come from a variety of sources: bugs in the code, inaccurate or biased data, approximations we have to make (e.g. you want to measure health and you use hospital readmissions as a proxy, or you are interested in crime and use arrests as a proxy. These things are related, but not the same), misunderstandings between different stakeholders (policy makers, those collecting the data, those coding the algorithm, those deploying it), how computer systems interact with human systems, and more.\nThis article discusses a variety of algorithmic systems. I don’t find debates about definitions particularly interesting, including what counts as “AI” or if a particular algorithm qualifies as “intelligent” or not. Please note that the dynamics described in this post hold true both for simpler algorithms, as well as more complex ones.\n\n\nAlgorithms are often implemented without ways to address mistakes.\n\n\nAfter the state of Arkansas implemented software to determine people’s healthcare benefits, many people saw a drastic reduction in the amount of care they received, but were given no explanation and no way to appeal. Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week, transforming her life for the worse. Eventually, a lengthy court case uncovered errors in the software implementation, and Tammy’s hours were restored (along with those of many others who were impacted by the errors).\nObservations of 5th grade teacher Sarah Wysocki’s classroom yielded positive reviews. Her assistant principal wrote, “It is a pleasure to visit a classroom in which the elements of sound teaching, motivated students and a positive learning environment are so effectively combined.” Two months later, she was fired by an opaque algorithm, along with over 200 other teachers. The head of the PTA and a parent of one of Wyscoki’s students described her as “One of the best teachers I’ve ever come in contact with. Every time I saw her, she was attentive to the children, went over their schoolwork, she took time with them and made sure.” That people are losing needed healthcare without an explanation or being fired without explanation is truly dystopian!\n\n\n\nHeadlines from the Verge and the Washington Post\n\n\nAs I covered in a previous post, people use outputs from algorithms differently than they use decisions made by humans:\n\nAlgorithms are more likely to be implemented with no appeals process in place.\nAlgorithms are often used at scale.\nAlgorithmic systems are cheap.\nPeople are more likely to assume algorithms are objective or error-free. As Peter Haas said, “In AI, we have Milgram’s ultimate authority figure,” referring to Stanley Milgram’s famous experiments showing that most people will obey orders from authority figures, even to the point of harming or killing other humans. How much more likely will people be to trust algorithms perceived as objective and correct?\n\nThere is a lot of overlap between these factors. If the main motivation for implementing an algorithm is cost-cutting, adding an appeals process (or even diligently checking for errors) may be considered an “unnecessary” expense. Cathy O’Neill, who earned her math PhD at Harvard, wrote a book Weapons of Math Destruction, in which she covers how algorithms are disproportionately impacting poor people, whereas the privileged are more likely to still have access to human attention (in hiring, education, and more).\n\n\nAI makes it easier to not feel responsible.\n\n\nLet’s return to the case of the buggy software used to determine health benefits in Arkansas. How could this have been prevented? In order to prevent severely disabled people from mistakenly losing access to needed healthcare, we need to talk about responsibility. Unfortunately, complex systems lend themselves to a dynamic in which nobody feels responsible for the outcome.\nThe creator of the algorithm for healthcare benefits, Brant Fries (who has been earning royalties off this algorithm, which is in use in over half the 50 states), blamed state policy makers. I’m sure the state policy makers could blame the implementers of the software. When asked if there should be a way to communicate how the algorithm works to the disabled people losing their healthcare, Fries callously said, “It’s probably something we should do. Yeah, I also should probably dust under my bed,” and then later clarified that he thought it was someone else’s responsibility.\nThis passing of the buck and failure to take responsibility is common in many bureaucracies. As danah boyd observed, “Bureaucracy has often been used to shift or evade responsibility. Who do you hold responsible in a complex system?” Boyd gives the examples of high-ranking bureaucrats in Nazi Germany, who did not see themselves as responsible for the Holocaust. boyd continues, “Today’s algorithmic systems are extending bureaucracy.”\nAnother example of nobody feeling responsible comes from the case of research to classify gang crime. A database of gang members assembled by the Los Angeles Police Department (and 3 other California law enforcement agencies) was found to have 42 babies who were under the age of 1 when added to the gang database (28 were said to have admitted to being gang members). Keep in mind these are just some of the most obvious errors- we don’t know how many other people were falsely included.\nI don’t bring this up for the primary purpose of pointing fingers or casting blame. However, a world of complex systems in which nobody feels responsible for the outcomes (which can include severely disabled people losing access to the healthcare they need, or innocent people being labeled as gang members) is not a pleasant place. Our work is almost always a small piece of a larger whole, yet a sense of responsibility is necessary to try to address and prevent negative outcomes.\n\n\nAI encodes & magnifies bias.\n\n\nBut isn’t algorithmic bias just a reflection of how the world is? I get asked a variation of this question every time I give a talk about bias. To which my answer is: No, our algorithms and products impact the world and are part of feedback loops. Consider an algorithm to predict crime and determine where to send police officers: sending more police to a particular neighhorhood is not just an effect, but also a cause. More police officers can lead to more arrests in a given neighborhood, which could cause the algorithm to send even more police to that neighborhood (a mechanism described in this paper on runaway feedback loops).\nBias is being encoded and even magnified in a variety of applications:\n\nsoftware used to decide prison sentences that has twice as high a false positive rate for Black defendents as for white defendents\ncomputer vision software from Amazon, Microsoft, and IBM performs significantly worse on people of color\n\n\n\n\nResearch by Joy Buolamwini and Timnit Gebru found that commercial computer vision software performed significantly worse on women with dark skin. Gendershades.org\n\n\n\nWord embeddings, which are a building block for language tools like Gmail’s SmartReply and Google Translate, generate useful analogies such as Rome:Italy :: Madrid:Spain, as well as biased analogies such as man:computer programmer :: woman: homemaker.\nMachine learning used in recruiting software developed at Amazon penalized applicants who attended all-women’s colleges, as well as any resumes that contained the word “women’s.”\nOver 2/3 of the images in ImageNet, the most studied image data set in the world, are from the Western world (USA, England, Spain, Italy, Australia).\n\n\n\n\nChart from ‘No Classification without Representation’ by Shankar, et. al, shows the origin of ImageNet photos: 45% US, 8% UK, 6% Italy, 3% Canada, 3% Australia, 3% Spain,…\n\n\nSince a Cambrian explosion of machine learning products is occuring, the biases that are calcified now and in the next few years may have a disproportionately huge impact for ages to come (and will be much harder to undo decades from now).\n\n\nOptimizing metrics above all else leads to negative outcomes.\n\n\nWorldwide, people watch 1 billion hours of YouTube per day (yes, that says PER DAY). A large part of YouTube’s successs has been due to its recommendation system, in which a video selected by an algorithm automatically begin playing once the previous video is over. Unfortunately, these recommendations are disproportionately for conspiracy theories promoting white supremacy, climate change denial, and denial of the mass shootings that plague the USA. What is going on? YouTube’s algorithm is trying to maximize how much time people spend watching YouTube, and conspiracy theorists watch significantly more YouTube than people who trust a variety of media sources. Unfortunately, a recommendation system trying only to maximize time spent on its own platform will incentivize content that tells you the rest of the media is lying.\n“YouTube may be one of the most powerful radicalizing instruments of the 21st century,” Professor Zeynep Tufekci wrote in the New York Times. Guillaume Chaslot is a former YouTube engineer turned whistleblower. He has been outspoken about the harms caused by YouTube, and he partnered with the Guardian and the Wall Street Journal to study the extremism and bias in YouTube’s recommendations.\n[Photo of Guillaume Chaslot from the Guardian article]{chaslot.png){width=60%}\nYouTube is owned by Google, which is earning billions of dollars by aggressively introducing vulnerable people to conspiracy theories, while the rest of society bears the externalized costs of rising authoritarian governments, a resurgence in white supremacist movements, failure to act on climate change (even as extreme weather is creating increasing numbers of refugees), growing distrust of mainstream news sources, and a failure to pass sensible gun laws.\nThis problem is an example of the tyranny of metrics: metrics are just a proxy for what you really care about, and unthinkingly optimizing a metric can lead to unexpected, negative results. One analog example is that when the UK began publishing the success rates of surgeons, heart surgeons began turning down risky (but necessary) surgeries to try to keep their scores as high as possible.\nReturning to the account of the popular 5th grade teacher who was fired by an algorithm, she suspects that the underlying reason she was fired was that her incoming students had unusually high test scores the previous year (making it seem like their scores had dropped to a more average level after her teaching), and that their former teachers may have cheated. As USA education policy began over-emphasizing student test scores as the primary way to evaluate teachers, there have been widespread scandals of teachers and principals cheating by altering students scores, in Georgia, Indiana, Massachusetts, Nevada, Virginia, Texas, and elsewhere. When metrics are given undue importance, attempts to game those metrics become common.\n\n\nThere is no accountability for big tech companies.\n\n\nMajor tech companies are the primary ones driving AI advances, and their algorithms impact billions of people. Unfortunately, these companies have zero accountability. YouTube (owned by Google) is helping to radicalize people into white supremacy. Google allowed advertisers to target people who search racist phrases like “black people ruin neighborhoods” and Facebook allowed advertisers to target groups like “jew haters”. Amazon’s facial recognition technology misidentified 28 members of congress as criminals, yet it is already in use by police departments. Palantir’s predictive policing technology was used for 6 years in New Orleans, with city council members not even knowing about the program, much less having any oversight. The newsfeed/timeline/recommendation algorithms of all the major platforms tend to reward incendiary content, prioritizing it for users.\nIn early 2018, the UN ruled that Facebook had played a “determining role” in the ongoing genocide in Myanmar. “I’m afraid that Facebook has now turned into a beast,” said the UN investigator. This result was not a surprise to anyone who had been following the situation in Myanmar. People warned Facebook executives about how the platform was being used to spread dehumanizing hate speech and incite violence against an ethnic minority as early as 2013, and again in 2014 and 2015. As early as 2014, news outlets such as Al Jazeera were covering Facebook’s role in inciting ethnic violence in Myanmar.\nOne person close to the case said, “That’s not 20/20 hindsight. The scale of this problem was significant and it was already apparent.” Facebook execs were warned in 2015 that Facebook could play the same role in Myanmar that radio broadcasts had played during the 1994 Rwandan genocide. As of 2015, Facebook only employed 4 contractors who spoke Burmese (the primary language in Myanmar).\nContrast Facebook’s inaction in Myanmar with their swift action in Germany after the passage of a new law, which could have resulted in penalties of up to 50 million euros. Facebook hired 1,200 German contractors in under a year. In 2018, five years after Facebook was first warned about how they were being used to incite violence in Myanmar, they hired “dozens” of Burmese contractors, a fraction of their response in Germany. The credible threat of a large financial penalty may be the only thing Facebook responds to.\nWhile it can be easy to focus on regulations that are misguided or ineffective, we often take for granted safety standards and regulations that have largely worked well. One major success story comes from automobile safety. Early cars had sharp metal knobs on dashboard that lodged in people’s skulls during crashes, plate glass windows that shattered dangerously, and non-collapsible steering columns that would frequently impale drivers. Beyond that, there was a widespread belief that the only issue with cars was the people driving them, and car manufactures did not want data on car safety to be collected. It took consumer safety advocates decades to push the conversation to how cars could be designed with greater safety, and to pass laws regarding seat belts, driver licenses, crash tests, and the collection of car crash data. For more on this topic, Datasheets for Datasets covers cases studies of how standardization came to the electronics, pharmaceutical, and automobile industries, and 99% Invisible has a deep dive on the history of car safety (with parallels and contrasts to the gun industry).\n\nHow We Can Do Better\n\nThe good news: none of the problems listed here are inherent to algorithms! There are ways we can do better:\n\nMake sure there is a meaningful, human appeals process. Plan for how to catch and address mistakes in advance.\nTake responsibility, even when our work is just one part of the system.\nBe on the lookout for bias. Create datasheets for data sets.\nChoose not to just optimize metrics.\nPush for thoughtful regulations and standards for the tech industry.\n\nThe problems we are facing can feel scary and complex. However, it is still very early on in this age of AI and increasing algorithmic automation. Now is a great time to take action: we can change our culture, cultivate a greater sense of responsibility for our work, seek out thoughtful accountability to counterbalance the inordinate power that major tech companies have, and choose to create more humane products and systems. Technology is just a tool, and it can be used for good or bad. Let’s work to use it for good, to improve the lives of many, rather than just generate wealth for a small number of people.\n\nRelated Posts\n\nYou may be interested in these related posts on tech and ethics:\n\nAI Ethics Resources\nWhat HBR Gets Wrong About Algorithms and Bias\nTech’s long hours are discriminatory & counter-productive\nArtificial Intelligence Needs All of Us (TEDx talk)"
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html",
    "href": "posts/2019-02-12-long-hours/index.html",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "",
    "text": "Whether you realize it or not, you are likely interacting with ill or disabled people regularly. According to recent survey data, a high portion of the U.S. workforce reports having a disability (30 percent), even though a much smaller percentage says they’ve self-identified as disabled to their employer (only 3.2 percent). Often, these illnesses and disabilities are impossible for others to observe, so many people choose to keep their conditions a secret from managers and co-workers to avoid discrimination.\nHealth is not binary; it can fluctuate and is subjective. I have experienced a number of health challenges, including having brain surgery twice (once while pregnant) and one life-threatening brain infection (which can take years to recover from). Trust me when I say that you can’t assess someone’s health based on their appearance or mood. And yet, over one-third of people with disabilities say they have experienced negative bias in their current job.\nI work in the tech industry, where there is an overt glorification — and in many cases, a requirement — of working unhealthily long hours. This is in spite of research showing that putting in longer hours doesn’t lead to greater productivity and instead is harmful. And when you’re ill or disabled and working in this field, the long hours can be not just counterproductive but discriminatory.\n\n\n\nA banner from 1856 reads, “8 hours labour, 8 hours recreation, 8 hours rest.” Source: Wikimedia"
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#fewer-hours-in-the-day",
    "href": "posts/2019-02-12-long-hours/index.html#fewer-hours-in-the-day",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "Fewer hours in the day",
    "text": "Fewer hours in the day\nMany people with chronic illnesses or disabilities simply have fewer hours in the day. We may need more sleep than comparatively healthy people — and yet still wake up feeling awful — as well as have to carefully budget limited energy. Conditions often require frequent doctor visits, blood tests, MRIs, physical therapy, and other appointments, plus there’s dealing with the administrative burden of managing scheduling, billing, and insurance claims, all of which frequently involve errors.\nIn an episode of the podcast No End in Sight, which is focused on chronic illness, a front-end software engineer named John pinpointed his experience feeling time-crunched. John has bipolar disorder and Fabry disease, a rare genetic disorder that causes reduced kidney function and chronic pain and requires him to get regular IV infusion treatments. He described being told during a job interview at Microsoft that he needed to spend more of his free time coding:\n\nI really felt looked down on as being lazy. And really, I’m not lazy. I have chronic illness, and I’m trying to do the best — like, I’m not trying to push myself too hard because I don’t want to throw myself into a bipolar tailspin. And I also don’t want to hurt my hands and have it be even worse to type… I was told by this abled person how to go about living assuming that I was abled, and it was just really frustrating. I’ve contributed at least a thousand hours to open source, and I’m supposed to just keep doing more. When does it end?\n\nNatasha Walton, who founded the Tech Disability Project, has fibromyalgia and post-traumatic stress disorder. She noted on Twitter that certain aspects of the day, like sleep and fitness routines, are not optional for her. “They account for the time I spend meeting my body’s basic needs each and every day so that I can participate in the wider world,” she explained.\nThe tech work environment is hostile even to healthy people. The “ideal worker” in tech is in perfect health, child-free, and has no other commitments. I’ve had several jobs in tech that I could do for a time, and even do quite well, but that I knew would be unsustainable for me long-term. The question was not if I would burn out, but when. Numerous co-workers have also seemed on the brink of burnout regardless of whether they had a chronic illness. I even have tech-industry friends who developed permanent chronic illnesses while in toxic work environments.\nThere are companies where people like me would not be welcome based on unreasonable employee demands. Last year, Andrew Ng’s deeplearning.ai posted a controversial job ad that not only specified that employees typically spend 70–90 hours per week working and studying (later changed to 70+ hours), but that doing so is the natural consequence of believing you can change the world. Many companies operate on this assumption, even if most are not quite so frank about it.\nElon Musk posted a declaration that to change the world, people need to work 80 hours per week, peaking above 100 at times. Uber formerly had an explicit company value to “work harder, longer, and smarter” and served dinner at 8:15 p.m. “Working seven days a week, sometimes until 1 or 2 a.m., was considered normal,” said one former employee. A New York Times article about Amazon described “marathon conference calls on Easter Sunday and Thanksgiving, criticism from bosses for spotty Internet access on vacation, and hours spent working at home most nights or weekends,” as well as employees being given low-performance ratings directly after cancer treatment, major surgeries, or giving birth to a stillborn child."
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#the-research-on-productivity",
    "href": "posts/2019-02-12-long-hours/index.html#the-research-on-productivity",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "The research on productivity",
    "text": "The research on productivity\nAs much as possible, we need to get away from the shallow idea that the quantity of time worked is what matters. The tech industry’s obsession with ridiculously long hours is not only inaccessible to many disabled people and harmful to everyone’s health and relationships, but as Olivia Goldhill pointed out for Quartz at Work, research on productivity suggests it’s just inefficient:\n\nAs countless studies have shown, this simply isn’t true. Productivity dramatically decreases with longer work hours, and completely drops off once people reach 55 hours of work a week, to the point that, on average, someone working 70 hours in a week achieves no more than a colleague working 15 fewer hours.\n\nAlex Soojung-Kim Pang’s book Rest covers the crucial role that leisure time and downtime play in our creativity, health, and productivity. Prolific, talented figures including Charles Darwin, Henri Poincaré, G.H. Hardy, mathematician Paul Halmos, Charles Dickens, and many others were known to engage in only four or five hours of highly concentrated work per day. Pang also highlights an overlooked aspect of the “rule” popularized by Malcolm Gladwell that to become an expert takes 10,000 hours of practice. Gladwell based it on psychologist K. Anders Ericsson’s study of top musical performers, but Pang observes that the top performers also slept more and took afternoon naps:\n\nWe’ve come to believe that world-class performance comes after 10,000 hours of practice. But that’s wrong. It comes after 10,000 hours of deliberate practice, 12,500 hours of deliberate rest, and 30,000 hours of sleep.\n\nMore support for rest-boosted productivity is detailed in a Harvard Business Review roundup titled “The Research Is Clear: Long Hours Backfire for People and for Companies.” It highlights a variety of other study results:\n\nManagers could not tell the difference between employees who worked 80-hour weeks and those who pretended to — though they still penalized employees who were open about working less.\nOverwork is linked to impaired sleep, and sleep deprivation has long been known to lengthen reaction time, interfere with problem-solving, and even induce an impairment equivalent to being drunk.\nDepression, heavy drinking, diabetes, memory problems, heart disease, and poorer judgment calls are all repercussions tied to being overworked.\nPredictable, required time off (like nights and weekends) make teams more productive.\n\n\n\n\nRoman timekeeping: Four clocks show the amount of night and day at times of the year. Image from Wikimedia"
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#the-importance-of-flexible-work-environments",
    "href": "posts/2019-02-12-long-hours/index.html#the-importance-of-flexible-work-environments",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "The importance of flexible work environments",
    "text": "The importance of flexible work environments\nAccommodations, even simple ones, can mean a world of difference to employees with illnesses or disabilities. Brianne Benness, founder of the No End in Sight podcast mentioned above, has written about how a flexible job with remote work helped her stay employed during her at-the-time undiagnosed illness: “When I woke up in a lot of pain, I could tell my boss I was working from home… When the pain in my neck made it too distracting to sit at my desk, I could move to a couch and lie down with my head supported.”\nBut when Brianne switched to a job with a more rigid in-office schedule, both her health and work level plummeted. With nowhere to lie down with her laptop, she would not only get distracted by the pain but put her focus on trying to seem productive. Meanwhile, she lost energy fast during the day, struggled with brain fog, and would go to bed as soon as she got home. She nailed the conundrum in explaining, “I know that when my brain is firing on all cylinders, I can get more done in five hours than I can get done in a full week when my brain is plodding. But I don’t know how to share that value with an employer.”\n\nThe tech industry problems of exclusion, discrimination, and unhealthy work habits run deep, but there is also a widespread appetite for change.\n\nSome business leaders and employers are recognizing the value on their own. A Harvard Business Review article details an experiment by Stanford professor Nicholas Bloom and Ctrip travel website cofounder James Liang in which they let half of Ctrip’s employees work from home for nine months. They found that the group working from home was both more productive and only half as likely to quit as other employees. Bloom said he was blown away by the results, and the benefits of flexible work were much greater than he expected.\nMassive employers like PricewaterhouseCoopers are experimenting, too. PwC is the second largest professional services firm in the world, and last year, it announced a new flexible work program in which potential employees can choose how many hours per week or how many months per year they are available to work. It seems to have created a valuable appeal in recruiting. When one of the company’s leaders, Anne Donovan, shared her advice on switching to a more flexible culture, she asserted that everyone deserves the same degree of flexibility and that culture comes from the top."
  },
  {
    "objectID": "posts/2019-02-12-long-hours/index.html#we-have-a-long-way-to-go",
    "href": "posts/2019-02-12-long-hours/index.html#we-have-a-long-way-to-go",
    "title": "Tech’s Long Hours Are Discriminatory and Counterproductive",
    "section": "We have a long way to go",
    "text": "We have a long way to go\nIn her keynote at GopherCon, Google developer advocate Julia Ferraioli pointed out that, in tech, we often make products accessible but not the processes or the teams we use to build them. True inclusion means having disabled people on your team; the people creating the technology need to be representative of the people using technology, which, increasingly, is everyone.\nIn previous posts, I have shared extensive research on how gender and racial bias manifest in the tech industry, including in retention, promotions, onboarding, and hiring. But in researching bias around disability in the field, I found far less, and this is in part because tech companies aren’t tracking it. When a TechCrunch reporter, who has a severe disability, asked Intel, Apple, Twitter, Facebook, Slack, Google, and Salesforce why none of them included disability in their diversity reports, the companies gave evasive, off-the-record responses. Since the zeroth step to increasing inclusion is to understand the scope and details of the problem, this is an indicator we have a long way to go.\nThe ideas are out there, though. Ted Kennedy Jr., an attorney and state senator who lost his leg as a child due to cancer, recently wrote about common, straightforward themes among companies that are inclusive of people with disabilities:\n\nThey hire people with disabilities.\nThey encourage and advance those employees.\nThey provide accessible tools and technology and have a formal accommodations program.\nThey empower those employees with mentoring and coaching initiatives (Note: Not all mentorship is the same. Research has shown that public endorsement of a mentee’s authority and championing their ideas is far more effective than advice on how the mentee should change and gain self-knowledge.)\n\nI certainly can’t and don’t speak for everyone with chronic illnesses or disabilities, and I encourage you to listen and read the accounts of others. The tech industry problems of exclusion, discrimination, and unhealthy work habits run deep, but there is also a widespread appetite for change. Reconsider the culture at your workplace by hiring and promoting people with disabilities, de-emphasizing hours spent working in favor of quality of work, and allowing a more flexible setup.\nThis post was originally published Feb 12, 2019 on Medium. This is a follow-up to an_ earlier post I wrote on how the tech industry is failing people with disabilities and chronic illnesses.\nThank you to Julia Ferraioli, Jeremy Howard, and Negar Rostamzadeh for giving me valuable feedback on earlier drafts of this post."
  },
  {
    "objectID": "posts/2019-05-13-blogging-advice/index.html",
    "href": "posts/2019-05-13-blogging-advice/index.html",
    "title": "Advice for Better Blog Posts",
    "section": "",
    "text": "A blog is like a resume, only better. I’ve been invited to give keynote talks based on my posts, and I know of people for whom blog posts have led to job offers. I’ve encouraged people to start blogging in several of my previous posts, and I even required students in my computational linear algebra course to write a blog post (although they weren’t required to publish it), since good technical writing skills are useful in the workplace and in interviews. Also, explaining something you’ve learned to someone else is a way to cement your knowledge. I gave a list of tips for getting started with your first blog post previously, and I wanted to offer some more advanced advice here."
  },
  {
    "objectID": "posts/2019-05-13-blogging-advice/index.html#who-is-your-audience",
    "href": "posts/2019-05-13-blogging-advice/index.html#who-is-your-audience",
    "title": "Advice for Better Blog Posts",
    "section": "Who is your audience?",
    "text": "Who is your audience?\nAdvice that my speech coach gave me about preparing talks, which I think also applies to writing, is to choose one particular person that you can think of as your target audience. Be as specific as possible. It’s great if this is a real person (and it is totally fine if they are not actually going to read your post or attend your talk), although it doesn’t have to be (you just need to be extra-thorough in making up details about them if it’s not). Either way, what is their background? What sort of questions or misconceptions might they have about the topic? At various points, the person I’m thinking of has been a friend or colleague, one of my students, or my younger self.\nBeing unclear about your audience can lead to a muddled post: for instance, I’ve seen blog posts that contain both beginner material (e.g. defining what training and test sets are) as well as very advanced material (e.g. describing complex new architectures). Experts would be bored and beginners would get lost."
  },
  {
    "objectID": "posts/2019-05-13-blogging-advice/index.html#dos-and-donts",
    "href": "posts/2019-05-13-blogging-advice/index.html#dos-and-donts",
    "title": "Advice for Better Blog Posts",
    "section": "Dos and Don’ts",
    "text": "Dos and Don’ts\nWhen you read other people’s blog posts, think about what works well. What did you like about it? And when you read blog posts that you don’t enjoy as much, think about why not? What would make the post more engaging for you? Note that not every post will appeal to every person. Part of having a target audience means that there are people who are not in your target audience, which is fine. And sometimes I’m not somebody else’s target audience. As with all advice, this is based on my personal experience and I’m sure that there are exceptions.\n\nThings that often work well:\n\nBring together many useful resources (but don’t include everything! the value is in your curation)\nDo provide motivation and context. If you are going to explain how an algorithm works, first give some examples of real-world applications where it is used, or how it is different from other options.\nPeople are convinced by several different things: stories, statistics, research, and visuals. Try using a blend of these.\nIf you’re using a lot of code, try writing in a Jupyter notebook (which can be converted into a blog post) or a Kaggle Kernel.\n\n\n\nDon’ts\n\nDon’t reinvent the wheel. If you know of a great explanation of something elsewhere, link to it! Include a quote or one sentence summary about the resource you’re linking to.\nDon’t try to build everything up from first principles. For example, if you want to explain the transformer architecture, don’t begin by defining machine learning. Who is your target audience? People already familiar with machine learning will lose interest, and those who are brand new to machine learning are probably not seeking out posts on the transformer architecture. You can assume that your reader already has a certain background (sometimes it is helpful to make this explicit).\nDon’t be afraid to have an opinion. For example, TensorFlow (circa 2016, before eager execution) made me feel unintelligent, even though everyone else seemed to be saying how awesome it was. I was pretty nervous writing a blog post that said this, but a lot of people responded positively.\nDon’t be too dull or dry. If people lose interest, they will stop reading, so you want to hook them (and keep them hooked!)\nDon’t plagiarize. Always cite sources, and use quote marks around direct quotes. Do this even as you are first gathering sources and taking notes, so you don’t make a mistake later and forget which material is someone else’s. It is wrong to plagiarize the work of others and ultimately will hurt your reputation. Cite and link to people who have given you ideas.\nDon’t be too general. You don’t have to cover everything on a topic– focus on the part that interests (or frustrates) you most."
  },
  {
    "objectID": "posts/2019-05-13-blogging-advice/index.html#put-the-time-in-to-do-it-well",
    "href": "posts/2019-05-13-blogging-advice/index.html#put-the-time-in-to-do-it-well",
    "title": "Advice for Better Blog Posts",
    "section": "Put the time in to do it well",
    "text": "Put the time in to do it well\nAs DeepMind researcher and University of Oxford PhD student Andrew Trask advised, “The secret to getting into the deep learning community is high quality blogging… Don’t just write something ok, either—take 3 or 4 full days on a post and try to make it as short and simple (yet complete) as possible.” Honestly, I’ve spent far more than 3 or 4 days on many of my most popular posts.\nHowever, this doesn’t mean that you need to be a “naturally gifted” writer. I attended a poor, public high school in a small city in Texas, where I had few writing assignments and didn’t really learn to write a proper essay. An introductory English class my first semester of college highlighted how much I struggled with writing, and after that, I tried to avoid classes that would require much writing (part of the reason I studied math and computer science is that those were the only fields I knew of that involved minimal writing AND didn’t have lab sessions). It wasn’t until I was in my 30s and wanted to start blogging that I began to practice writing. I typically go through many, many drafts, and do lots of revisions. As with most things, skill is not innate; it is something you build through deliberate practice.\nNote: I realize many people may not have time to blog– perhaps you are a parent, dealing with chronic illness, suffering burnout from a toxic job, or prefer to do other things in your free time– that’s alright! You can still have a successful career without blogging, this post is only for those who are interested."
  },
  {
    "objectID": "posts/2019-05-13-blogging-advice/index.html#write-a-blog-version-of-your-academic-paper",
    "href": "posts/2019-05-13-blogging-advice/index.html#write-a-blog-version-of-your-academic-paper",
    "title": "Advice for Better Blog Posts",
    "section": "Write a blog version of your academic paper",
    "text": "Write a blog version of your academic paper\nThe top item on my wish list for AI researchers is that more of them would write blog posts to accompany their papers:\n\n\n\nmy wish list for AI researchers https://t.co/Cel5x32K9O pic.twitter.com/AyYBqwYDFX\n\n— Rachel Thomas (@math_rachel) April 11, 2018\n\n\n\nFar more people may read your blog post than will read an academic paper. This is a chance to get your message to a broader audience, in a more conversational and accessible format. You can and should link to your academic paper from your blog post, so there’s no need to worry about including all the technical details. People will read your paper if they want more detail!\nCheck out these excellent pairs of academic papers and blog posts for inspiration: - Gender Shades (blog post & visualization) and Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification (paper), by Joy Buolamwini & Timnit Gebru - The Problem with “Biased Data” (blog post) and A Framework for Understanding Unintended Consequences of Machine Learning (paper), by Harini Suresh & John Guttag - Introducing state of the art text classification with universal language models (blog post) and Universal Language Model Fine-tuning for Text Classification (paper), by Jeremy Howard & Sebastian Ruder\nI usually advise new bloggers that your target audience could be you-6-months-ago. For grad students, you may need to change this to you-2-years-ago. Assume that unlike your paper reviewers, the reader of your blog post has not read the related research papers. Assume your audience is intelligent, but not in your subfield. What does it take to explain your research to a friend in a different field?"
  },
  {
    "objectID": "posts/2019-05-13-blogging-advice/index.html#getting-started-with-your-first-post",
    "href": "posts/2019-05-13-blogging-advice/index.html#getting-started-with-your-first-post",
    "title": "Advice for Better Blog Posts",
    "section": "Getting Started with your first post",
    "text": "Getting Started with your first post\nHere are some tips I’ve shared previously to help you start your first post:\n\nMake a list of links to other blog posts, articles, or studies that you like, and write brief summaries or highlight what you particularly like about them. Part of my first blog post came from my making just such a list, because I couldn’t believe more people hadn’t read the posts and articles that I thought were awesome.\nSummarize what you learned at a conference you attended, or in a class you are taking.\nAny email you’ve written twice should be a blog post. Now, if I’m asked a question that I think someone else would also be interested in, I try to write it up.\nYou are best positioned to help people one step behind you. The material is still fresh in your mind. Many experts have forgotten what it was like to be a beginner (or an intermediate) and have forgotten why the topic is hard to understand when you first hear it.\nWhat would have helped you a year ago? What would have helped you a week ago?\nIf you’re wondering about the actual logistics, Medium makes it super simple to get started. Another option is to use Jekyll and Github pages. I can personally recommend both, as I have 2 blogs and use one for each (my other blog is here)."
  },
  {
    "objectID": "posts/2019-05-13-blogging-advice/index.html#related-posts",
    "href": "posts/2019-05-13-blogging-advice/index.html#related-posts",
    "title": "Advice for Better Blog Posts",
    "section": "Related Posts",
    "text": "Related Posts\n\nMaking Peace with Personal Branding\nAlternatives to a Degree to Prove Yourself in Deep Learning\nA Discussion about Accessibility in AI at Stanford"
  },
  {
    "objectID": "posts/2019-08-07-surveillance/index.html",
    "href": "posts/2019-08-07-surveillance/index.html",
    "title": "8 Things You Need to Know about Surveillance",
    "section": "",
    "text": "Over 225 police departments have partnered with Amazon to have access to Amazon’s video footage obtained as part of the “smart” doorbell product Ring, and in many cases these partnerships are heavily subsidized with taxpayer money. Police departments are allowing Amazon to stream 911 call information directly in real-time, and Amazon requires police departments to read pre-approved scripts when talking about the program. If a homeowner doesn’t want to share data from their video camera doorbell with police, an officer for the Fresno County Sheriff’s Office said they can just go directly to Amazon to obtain it. This creation of an extensive surveillance network, the murky private-public partnership surrounding it, and a lack of any sort of regulations or oversight is frightening. And this is just one of many examples related to surveillance technology that have recently come to light.\nI frequently talk with people who are not that concerned about surveillance, or who feel that the positives outweigh the risks. Here, I want to share some important truths about surveillance:\n\n\nSurveillance can facilitate human rights abuses and even genocide\n\n\nData is often used for different purposes than why it was collected\n\n\nData often contains errors\n\n\nSurveillance typically operates with no accountability\n\n\nSurveillance changes our behavior\n\n\nSurveillance disproportionately impacts the marginalized\n\n\nData privacy is a public good\n\n\nWe don’t have to accept invasive surveillance\n\n\nWhile I was writing this post, a number of investigative articles came out with disturbing new developments related to surveillance. I decided that rather than attempt to include everything in one post (which would make it too long and too dense), I would go ahead and share the above facts about surveillance, as they are just a relevant as ever.\n\n\n\nThe last 24 hours:- NYC police using facial recognition on 11 year old kids- Cops are giving Amazon real-time 911 caller data- California Facial Recognition Interconnect- contd facial rec on protesters in Hong Kong- Palantir founder Peter Thiel gets op-ed in NYTimes\n\n— Rachel Thomas (@math_rachel) August 2, 2019\n\n\n\n\n\nSurveillance can facilitate human rights abuses and even genocide\n\n\nThere is a long history of data about sensitive attributes being misused, including the use of the 1940 USA Census to intern Japanese Americans, a system of identity cards introduced by the Belgian colonial government that were later used during the 1994 Rwandan genocide (in which nearly a million people were murdered), and the role of IBM in helping Nazi Germany use punchcard computers to identify and track the mass killing of millions of Jewish people. More recently, the mass internment of over one million people who are part of an ethnic minority in Western China was facilitated through the use of a surveillance network of cameras, biometric data (including images of people’s faces, audio of their voices, and blood samples), and phone monitoring.\n\n\n\nAdolf Hitler meeting with IBM CEO Tom Watson Sr. in 1937. Source: https://www.computerhistory.org/revolution/punched-cards/2/15/109\n\n\nPictured above is Adolf Hitler (far left) meeting with IBM CEO Tom Watson Sr. (2nd from left), shortly before Hitler awarded Watson a special “Service to the Reich” medal in 1937 (for a timeline of the Holocaust, see here). Watson returned the medal in 1940, although IBM continued to do business with the Nazis. IBM technology helped the Nazis conduct detailed censuses in countries they occupied, to thoroughly identify anyone of Jewish descent. Nazi concentration camps used IBM’s punchcard machines to tabulate prisoners, recording whether they were Jewish, gay, or Gypsies, and whether they died of “natural causes,” execution, suicide, or via “special treatment” in gas chambers. It is not the case that IBM sold the machines and then was done with it. Rather, IBM and its subsidiaries provided regular training and maintenance on-site at the concentration camps: printing off cards, configuring machines, and repairing them as they broke frequently.\n\n\nData is often used for different purposes than why it was collected\n\n\nIn the above examples, the data collection began before genocide was committed. IBM began selling to Nazi Germany well before the Holocaust (although continued for far too long), including helping with Germany’s 1933 census conducted by Adolf Hitler, which was effective at identifying far more Jewish people than had previously been recognized in Germany.\nIt is important to recognize how data and images gathered through surveillance can be weaponized later. Columbia professor Tim Wu wrote that “One [hard truth] is that data and surveillance networks created for one purpose can and will be used for others. You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.”\nPlenty of data collection is not involved with such extreme abuse as genocide; however, in a time of global resurgence of white supremacist, ethno-nationalist, and authoritarian movements, it would be deeply irresponsible to not consider how data & surveillance can and will be weaponized against already vulnerable groups.\n\n\nData often has errors (and no mechanism for correcting them)\n\n\nA database of suspected gang members maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). Even worse, there was no process in place for correcting mistakes or removing people once they’ve been added.\nAn NPR reporter recounts his experience of trying to rent an apartment and discovering that TransUnion, one of the 3 major credit bureaus, incorrectly reported him as having two felony firearms convictions. TransUnion only removed the mistakes after a dozen phone calls and notification that the story would be reported on. This is not an unusual story: the FTC’s large-scale study of credit reports in 2012 found 26% of consumers had at least one mistake in their files and 5% had errors that could be devastating. An even more opaque, unregulated “4th bureau” exists: a collection of companies buying and selling personal information about people on the margins of the banking system (such as immigrants, students, and people with low incomes), with no standards on what types of data are included, no way to opt out, and no system for identifying or correcting mistakes.\n\n\nSurveillance typically operates with no accountability\n\n\nWhat makes the examples in the previous section disturbing is not just that errors occurred, but that there was no way to identify or correct them, and no accountability for those profiting off the error-laden data. Often, even the existence of the systems being used is not publicly known (much less details of how these systems work), unless discovered by journalists or revealed by whistleblowers. The Detroit Police Dept used facial recognition technology for nearly two years without public input and in violation of a requirement that a policy be approved by the city’s Board of Police Commissioners, until a study from Georgetown Law’s Center for Privacy & Technology drew attention to the issue. Palantir, the defense startup founded by billionaire Peter Thiel, ran a program with New Orleans Police Department for 6 years which city council members did not even know about, much less have any oversight.\nAfter two studies found that Amazon’s facial recognition software produced inaccurate and racially biased results, Amazon countered that the researchers should have changed the default parameters. However, it turned out that Amazon was not instructing police departments that use its software to do this either. Surveillance programs are operating with few regulations, no oversight, no accountability around accuracy or mistakes, and in many cases, no public knowledge of what is going on.\n\n\nSurveillance changes our behavior\n\n\nHundreds of thousands of people in Hong Kong are protesting an unpopular new bill which would allow extradition to China. Typically, Hong Kong locals use their rechargeable smart cards to ride the subway. However, during the protests, long lines of people waited to use cash to buy paper tickets (usually something that only tourists do) concerned that they would be tracked for having attended the protests. Would fewer people protest if this was not an option?\nIn the United States, in 2015 the Baltimore Police Department used facial recognition technology to surveil people protesting the death of Freddie Grey, a young Black man who was killed in police custody, and arrested protesters with outstanding warrants. Mass surveillance could have a chilling impact on our rights to move about freely, to express ourselves, and to protest. “We act differently when we know we are ‘on the record.’ Mass privacy is the freedom to act without being watched and thus in a sense, to be who we really are,” Columbia professor Tim Wu wrote in the New York Times.\n\n\n\nFlyer from the company Geofeedia. Source: https://www.aclunc.org/docs/20161011_geofeedia_baltimore_case_study.pdf\n\n\n\n\nSurveillance disproportionately impacts those who are already marginalized\n\n\nSurveillance is applied unevenly, causing the greatest harm to people who are already marginalized, including immigrants, people of color, and people living in poverty. These groups are more heavily policed and surveilled. The Perpetual Line-Up from the Georgetown Law Center on Privacy and Technology studied the unregulated use of facial recognition by police, with half of all Americans appearing in law enforcement databases, and the risks of errors, racial bias, misuses, and threats to civil liberties. The researchers pointed out that African Americans are more likely to appear in these databases (many of which are drawn from mug shots) since they are disproportionately likely to be stopped, interrogated, or arrested. For another example, consider the contrast between how easily people over 65 can apply for Medicare benefits by filling out an online form, with the invasive personal questions asked of a low-income mother on Medicaid about her lovers, hygiene, parental shortcomings, and personal habits.\nIn an article titled Trading privacy for survival is another tax on the poor, Ciara Byrne wrote, “Current public benefits programs ask applicants extremely detailed and personal questions and sometimes mandate home visits, drug tests, fingerprinting, and collection of biometric information… Employers of low-income workers listen to phone calls, conduct drug tests, monitor closed-circuit television, and require psychometric tests as conditions of employment. Prisoners in some states have to consent to be voiceprinted in order to make phone calls.”\n\n\nData privacy is a public good, like air quality or safe drinking water\n\n\nData is more revealing in aggregate. It can be nearly impossible to know what your individual data could reveal when combined with the data of others or with data from other sources, or when machine learning inference is performed on it. For instance, as Zeynep Tufekci wrote in the New York Times, individual Strava users could not have predicted how in aggregate their data could be used to identify the locations of US military bases. “Data privacy is not like a consumer good, where you click ‘I accept’ and all is well. Data privacy is more like air quality or safe drinking water, a public good that cannot be effectively regulated by trusting in the wisdom of millions of individual choices. A more collective response is needed.”\nUnfortunately, this also means that you can’t fully safeguard your privacy on your own. You may choose not to purchase Amazon’s ring doorbell, yet you can still show up in the video footage collected by others. You might strengthen your online privacy practices, yet conclusions will still be inferred about you based on the behavior of others. As Professor Tufekci wrote, we need a collective response.\n\n\nWe don’t have to accept invasive surveillance\n\n\nMany people are uncomfortable with surveillance, but feel like they have no say in the matter. While the threats surveillance poses are large, it is not too late to act. We are seeing success: in response to community organizing and an audit, Los Angeles Police Department scrapped a controversial program to predict who is most likely to commit violent crimes. Citizens, researchers, and activists in Detroit have been effective at drawing attention to the Detroit Police Department’s unregulated use of facial recognition and a bill calling for a 5-year moratorium has been introduced to the state legislature. Local governments in San Francisco, Oakland, and Somerville have banned the use of facial recognition by police.\nFor further resources, please check out: - Georgetown Law Center on Privacy and Technology - Digital Defense Playbook"
  },
  {
    "objectID": "posts/2019-09-24-metrics/index.html",
    "href": "posts/2019-09-24-metrics/index.html",
    "title": "The problem with metrics is a big problem for AI",
    "section": "",
    "text": "Update: This post was expanded into a paper, Reliance on metrics is a fundamental challenge for AI, by Rachel Thomas and David Uminsky, which was accepted to the Ethics of Data Science Conference 2020 and to Cell Patterns. The paper version includes more grounding in previous academic work and a framework towards mitigating these harms.\nGoodhart’s Law states that “When a measure becomes a target, it ceases to be a good measure.” At their heart, what most current AI approaches do is to optimize metrics. The practice of optimizing metrics is not new nor unique to AI, yet AI can be particularly efficient (even too efficient!) at doing so.\nThis is important to understand, because any risks of optimizing metrics are heightened by AI. While metrics can be useful in their proper place, there are harms when they are unthinkingly applied. Some of the scariest instances of algorithms run amok (such as Google’s algorithm contributing to radicalizing people into white supremacy, teachers being fired by an algorithm, or essay grading software that rewards sophisticated garbage) all result from over-emphasizing metrics. We have to understand this dynamic in order to understand the urgent risks we are facing due to misuse of AI.\n\n\n\nHeadlines from HBR, Washington Post, and Vice on some of the outcomes of over-optimizing metrics: rewarding gibberish essays, promoting propaganda, massive fraud at Wells Fargo, and firing good teachers\n\n\nThe following principles will be illustrated through a series of case studies:\n\n\nAny metric is just a proxy for what you really care about\n\n\nMetrics can, and will, be gamed\n\n\nMetrics tend to overemphasize short-term concerns\n\n\nMany online metrics are gathered in highly addictive environments\n\n\nMetrics are most likely to be useful when they are treated as one piece of a bigger picture\n\n\n\nWe can’t measure the things that matter most\n\nMetrics are typically just a proxy for what we really care about. The paper Does Machine Learning Automate Moral Hazard and Error? covers an interesting example: the researchers investigate which factors in someone’s electronic medical record are most predictive of a future stroke. However, the researchers found that several of the most predictive factors (such as accidental injury, a benign breast lump, or colonoscopy) don’t make sense as risk factors for stroke. So, just what is going on? It turned out that the model was just identifying people who utilize health care a lot. They didn’t actually have data of who had a stroke (a physiological event in which regions of the brain are denied new oxygen); they had data about who had access to medical care, chose to go to a doctor, were given the needed tests, and had this billing code added to their chart. But a number of factors influence this process: who has health insurance or can afford their co-pay, who can take time off of work or find childcare, gender and racial biases that impact who gets accurate diagnoses, cultural factors, and more. As a result, the model was largely picking out people who utilized healthcare versus who did not.\nThis an example of the common phenomenon of having to use proxies: You want to know what content users like, so you measure what they click on. You want to know which teachers are most effective, so you measure their students test scores. You want to know about crime, so you measure arrests. These things are not the same. Many things we do care about can not be measured. Metrics can be helpful, but we can’t forget that they are just proxies.\nAs another example, Google used hours spent watching YouTube as a proxy for how happy users were with the content, writing on the Google blog that “If viewers are watching more YouTube, it signals to us that they’re happier with the content they’ve found.” Guillaume Chaslot, an AI engineer who formerly worked at Google/YouTube, shares how this had the side effect of incentivizing conspiracy theories, since convincing users that the rest of the media is lying kept them watching more YouTube.\n\nMetrics can, and will, be gamed\n\nIt is almost inevitable that metrics will be gamed, particularly when they are given too much power. One week this spring, Chaslot collected 84,695 videos from YouTube and analyzed the number of views and the number of channels from which they were recommended. This is what he found (also covered in the Washington Post):\n\n\n\nChart showing Russia Today’s video on the Mueller Report as being an outlier in how many YouTube channels recommended it.\n\n\nThe state-owned media outlet Russia Today was an extreme outlier in how much YouTube’s algorithm had selected it to be recommended by a wide-variety of other YouTube channels. Such algorithmic selections, which begin autoplaying as soon as your current video is done, account for 70% of the time that users spend on YouTube. This chart strongly suggests that Russia Today has in some way gamed YouTube’s algorithm. Platforms are rife with attempts to game their algorithms, to show up higher in search results or recommended content, through fake clicks, fake reviews, fake followers, and more.\nAutomatic essay grading software focuses primarily on metrics like sentence length, vocabulary, spelling, and subject-verb agreement, but is unable to evaluate aspects of writing that are hard to quantify, such as creativity. As a result, gibberish essays randomly generated by computer programs to contain lots of sophisticated words score well. Essays from students in mainland China, which do well on essay length and sophisticated word choice, received higher scores from the algorithms than from expert human graders, suggesting that these students may be using chunks of pre-memorized text.\nAs USA education policy began over-emphasizing student test scores as the primary way to evaluate teachers, there have been widespread scandals of teachers and principals cheating by altering students scores, in Georgia, Indiana, Massachusetts, Nevada, Virginia, Texas, and elsewhere. One consequence of this is that teachers who don’t cheat may be penalized or even fired (when it appears student test scores have dropped to more average levels under their instruction). When metrics are given undue importance, attempts to game those metrics become common.\n\nMetrics tend to overemphasize short-term concerns\n\nIt is much easier to measure short-term quantities: click through rates, month-over-month churn, quarterly earnings. Many long-term trends have a complex mix of factors and are tougher to quantify. What is the long-term impact on user trust of having your brand associated with promoting pedophilia, white supremacy, and flat-earth theories? What is the long-term impact on hiring to be the subject of years worth of privacy scandals, political manipulation, and facilitating genocide?\nSimply measuring what users click on is a short-term concern, and does not take into account factors like the potential long-term impact of a long-form investigative article which may have taken months to research and which could help shape a reader’s understanding of a complex issue and even lead to significant societal changes.\nA recent Harvard Business Review article looked at Wells Fargo as a case study of how letting metrics replace strategy can harm a business. After identifying cross-selling as a measure of long-term customer relationships, Wells Fargo went overboard emphasizing the cross-selling metric: intense pressure on employees combined with an unethical sales culture led to 3.5 million fraudulent deposit and credit card accounts being opened without customers’ consent. The metric of cross-selling is a much more short-term concern compared to the loftier goal of nurturing long-term customer relationships. Overemphasizing metrics removes our focus from long-term concerns such as our values, trust and reputation, and our impact on society and the environment, and myopically focuses on the short-term.\n\nMany metrics gather data of what we do in highly addictive environments\n\nIt matters which metrics we gather and in what environment we do so. Metrics such as what users click on, how much time they spend on sites, and “engagement” are heavily relied on by tech companies as proxies for user preference, and are used to drive important business decisions. Unfortunately, these metrics are gathered in environments engineered to be highly addictive, laden with dark patterns, and where financial and design decisions have already greatly circumscribed the range of options.\n\n\n\nOur online environment is a buffet of junk food\n\n\nZeynep Tufekci, a professor at UNC and regular contributor to the New York Times, compares recommendation algorithms (such as YouTube choosing which videos to auto-play for you and Facebook deciding what to put at the top of your newsfeed) to a cafeteria shoving junk food into children’s faces. “This is a bit like an autopilot cafeteria in a school that has figured out children have sweet teeth, and also like fatty and salty foods. So you make a line offering such food, automatically loading the next plate as soon as the bag of chips or candy in front of the young person has been consumed.” As those selections get normalized, the output becomes ever more extreme: “So the food gets higher and higher in sugar, fat and salt – natural human cravings – while the videos recommended and auto-played by YouTube get more and more bizarre or hateful.” Too many of our online environments are like this, with metrics capturing that we love sugar, fat, and salt, not taking into account that we are in the digital equivalent of a food desert and that companies haven’t been required to put nutrition labels on what they are offering. Such metrics are not indicative of what we would prefer in a healthier or more empowering environment.\n\nWhen Metrics are Useful\n\nAll this is not to say that we should throw metrics out altogether. Data can be valuable in helping us understand the world, test hypotheses, and move beyond gut instincts or hunches. Metrics can be useful when they are in their proper context and place. One way to keep metrics in their place is to consider a slate of many metrics for a fuller picture (and resist the temptation to try to boil these down to a single score). For instance, knowing the rates at which tech companies hire people from under-indexed groups is a very limited data point. For evaluating diversity and inclusion at tech companies, we need to know comparative promotion rates, cap table ownership, retention rates (many tech companies are revolving doors driving people from under-indexed groups away with their toxic cultures), number of harassment victims silenced by NDAs, rates of under-leveling, and more. Even then, all this data should still be combined with listening to first-person experiences of those working at these companies.\nColumbia professor and New York Times Chief Data Scientist Chris Wiggins wrote that quantitative measures should always be combined with qualitative information, “Since we can not know in advance every phenomenon users will experience, we can not know in advance what metrics will quantify these phenomena. To that end, data scientists and machine learning engineers must partner with or learn the skills of user experience research, giving users a voice.”\nAnother key to keeping metrics in their proper place is to keep domain experts and those who will be most impacted closely involved in their development and use. Surely most teachers could have foreseen that evaluating teachers primarily on the standardized test scores of their students would lead to a host of negative consequences.\nI am not opposed to metrics; I am alarmed about the harms caused when metrics are overemphasized, a phenomenon that we see frequently with AI, and which is having a negative, real-world impact. AI running unchecked to optimize metrics has led to Google/YouTube’s heavy promotion of white supremacist material, essay grading software that rewards garbage, and more. By keeping the risks of metrics in mind, we can try to prevent these harms."
  },
  {
    "objectID": "posts/2020-01-21-responsible-govt/index.html",
    "href": "posts/2020-01-21-responsible-govt/index.html",
    "title": "4 Principles for Responsible Government Use of Technology",
    "section": "",
    "text": "As governments consider new uses of technology, whether that be sensors on taxi cabs, police body cameras, or gunshot detectors in public places, this raises issues around surveillance of vulnerable populations, unintended consequences, and potential misuse. There are several principles to keep in mind in how these decisions can be made in a healthier and more responsible manner. It can be tempting to reduce debates about government adoption of technology into binary for/against narratives, but that fails to capture many crucial and nuanced aspects of these decisions.\nWe recently hosted the Tech Policy Workshop at the USF Center for Applied Data Ethics. One of the themes was how governments can promote the responsible use of technology. Here I will share some key recommendations that came out of these discussions."
  },
  {
    "objectID": "posts/2020-01-21-responsible-govt/index.html#listen-to-local-communities",
    "href": "posts/2020-01-21-responsible-govt/index.html#listen-to-local-communities",
    "title": "4 Principles for Responsible Government Use of Technology",
    "section": "Listen to local communities",
    "text": "Listen to local communities\nThere aren’t universal ethical answers that will make sense in every country and culture. Therefore, decisions on technology use should be made in close consultation with local communities. In 2013, Oakland announced plans for a new Domain Awareness Center (DAC), which would implement over 700 cameras throughout schools and public housing, facial recognition software, automated license plate readers (ALPRs), storage capacity for 300 terabytes of data, and a centralized facility with live monitroy. Brian Hofer was an Oakland resident who had never set foot in City Hall prior to this, but he was alarmed by the plans, particularly in light of Edward Snowden’s revelations, which had been released the same month. Together with other citizens and privacy advocates, Hofer was concerned about the intrusiveness of the plans and began attending city council meetings. There were a number of reasons for their concerns, including the discovery that city staff had been discussing using DAC to surveil protests and demonstrations. Through the advocacy of local citizens, the plans were dramatically scaled back and the Oakland Privacy Commission was formed, which continues to provide valuable insight into potential government decisions and purchases.\nSadly, the concerns of local communities are often overridden, in part due to corporate interests and racist stereotypes. For instance, in Detroit, a city that is 79% Black, citizens protested against police use of facial recognition. Yet the city council ended up voting to okay its use, in violation of the police department’s own policy. In contrast, the demographics of cities that have been successful at banning facial recognition are quite different: San Francisco is only 5% Black and Oakland is 25% Black (credit to Tawana Petty for highlighting these statistics). The racial composition of cities is a significant factor in where and how technology is deployed and used. In another sobering example of the significance of race, Baltimore Police Department used facial recognition to identify people protesting the death of Freddie Gray, a Black man killed in police custody."
  },
  {
    "objectID": "posts/2020-01-21-responsible-govt/index.html#beware-how-ndas-obscure-public-sector-process-and-law",
    "href": "posts/2020-01-21-responsible-govt/index.html#beware-how-ndas-obscure-public-sector-process-and-law",
    "title": "4 Principles for Responsible Government Use of Technology",
    "section": "Beware how NDAs obscure public sector process and law",
    "text": "Beware how NDAs obscure public sector process and law\nIn order for citizens to have a voice in the use of technology by their local governments, the first step is that they need to know what technology is being used. Unfortunately, many local governments are shrouded in secrecy on this topic, and they often sign overly strict non-disclosure agreements (NDAs), hiding even the existence of the technology they use. In 2017 New York City passed a measure appointing a task force on Automated Decision Systems to investigate the fairness of software being used by the city and make policy recommendations. However, members of the task force were repeatedly denied their requests for even a basic list of automated systems already in use, with the city claiming that this is proprietary information. When the city released the final report from the commission, many members dissented with it and released their own shadow report in tandem. Meredith Whittaker, a member of the task force and founder of AI Now Institute, described the city’s failure to share relevant information in what could have been a groundbreaking project, “It’s a waste, really. This is a sad precedent.”\nThe law typically develops through lots of cases over time, explained Elizabeth Joh. However, NDAs often prevent citizens from finding out that a particular technology even exists, much less how it is being used in their city. For instance, cell-site simulators (often referred to as sting-rays), which help police locate a person’s cell phone, were protected by particularly strong NDAs, in which police had to agree that it was better to drop a case than to reveal that a cell-site simulator had been used in apprehending the suspect. How can our law develop when such important details remain hidden? The traditional process of developing and refining our legal system breaks down. “Typically we think we have oversight into what police can do,” Joh has said previously. “Now we have third-party intermediary, they have a kind of privacy shield, they’re not subject to state public record laws, and they have departments sign contracts that they are going to keep this secret.”"
  },
  {
    "objectID": "posts/2020-01-21-responsible-govt/index.html#security-is-not-the-same-as-safety",
    "href": "posts/2020-01-21-responsible-govt/index.html#security-is-not-the-same-as-safety",
    "title": "4 Principles for Responsible Government Use of Technology",
    "section": "Security is not the same as safety",
    "text": "Security is not the same as safety\nProject Green Light is a public-private partnership in Detroit in which high-definition surveillance cameras outside business stream live data to police and are prioritized by police over non-participants. Over 500 businesses are a part of it. This is the largest experiment of facial recognition on a concentrated group of Black people (700,000) to date. Black people are disproportionately likely to be stopped by police (even though when police search Black, Latino and Native American people, they are less likely to find drugs, weapons or other contraband compared to when they search white people), disproportionately likely to be written up on minor infractions, and thus disproportionately likely to have their faces appear in police face databases (which are unregulated and not audited for mistakes). This is particularly concerning when combined with knowledge of America’s long history of surveilling and abusing Black communities. While the aims of the program are ostensibly to make Detroit safer, we have to ask, “Safer FOR who? And safer FROM whom?”\n\n\n\nGraphic about Detroit’s Project Greenlight, originally from data.detroitmi.gov and shared in Detroit Riverwise Magazine”\n\n\nTawana Petty is a poet and social justice organizer who was born and raised in Detroit. She serves as Director of Data Justice Programming for the Detroit Community Technology Project and co-leads the Our Data Bodies Project. At the CADE Tech Policy Workshop she shared how Project Green Light makes her feel less safe, and gave a more hopeful example of how to increase safety: give people chairs to sit on their front porches and encourage them to spend more time outside talking with their neighbors. Myrtle Thompson-Curtis wrote about the origins of the idea: in 1980 in Milwaukee “a group of young African Americans remembered how elders would sit on the front porch and keep an eye on them when they were small. These watchful eyes gave them a sense of safety, of being cared for and looked out for by the community. When these youth grew into adulthood, they noticed that no one sat on their porches anymore. Instead people were putting bars on their doors and windows, fearing one another.” Young people went door to door and offered free chairs to neighbors if they would agree to sit on their front porches while children walked to and from school. This program has since been replicated in St. Clair Shores, Michigan, to help defuse racial tensions, and now in Detroit, to illustrate an alternative to the city’s invasive Green Light Surveillance program. “Security is not safety,” Tawana stated, contrasting surveillance with true safety.\nRumman Chowdhury, the leader of the Responsible AI group at Accenture, pointed out that surveillance is often part of a stealth increase in militarization. While on the surface, militarization is sold as improving security, it can often have the opposite effect. Low-trust societies tend to be very militarized, and militarized societies tend to be low-trust. As Zeynep Tufekci wrote in Wired, sociologists distinguish between high-trust societies (in which people can expect most interactions to work and to have access to due process) and low-trust societies (in which people expect to be cheated and that there is no recourse when you are wronged). In low trust societies, it is harder to make business deals, to find or receive credit, or to forge professional relationships. People in low-trust societies may also be more vulnerable to authoritarian rulers, who promise to impose order. We are already seeing a shift of the internet having gone from a high-trust environment to a low-trust environment, and the use of surveillance may be accelerating this shift in the physical world."
  },
  {
    "objectID": "posts/2020-01-21-responsible-govt/index.html#policy-decisions-should-not-be-outsourced-as-design-decisions",
    "href": "posts/2020-01-21-responsible-govt/index.html#policy-decisions-should-not-be-outsourced-as-design-decisions",
    "title": "4 Principles for Responsible Government Use of Technology",
    "section": "Policy decisions should not be outsourced as design decisions",
    "text": "Policy decisions should not be outsourced as design decisions\nWhen considering police body cameras, there are a number of significant decisions: should the officer be able to turn them on and off at any time? Should the camera have a blinking red light to let people know it is recording? Where should the videos be stored and who should have access to them? Even though these decisions will have a profound impact on the public, they are currently decided by private tech companies. This is just one of the examples Elizabeth Joh shared in illustrating how what should be policy decisions often end up being determined by corporations as design decisions. In the case of police body cameras, this lack of choice/control is worsened by the fact that Axon (previously known as Taser) has a monopoly on police-body cameras: since they have a relationship with 17,000 of the 18,000 police departments in the USA, cities may not even have much choice. Vendor-customer relationships influence how police do their jobs and how we can hold them accountable.\nHeather Patterson, a privacy researcher at Intel and a member of Oakland’s Privacy Commission, spoke about how tech companies often neglect cities, failing to build products that fit with their needs and requirements, and treating them as an afterthought. In many cases, cities may want to have fewer options or collect less data, which goes against the prevailing tech approach which Mozilla Head of Policy Chris Riley described as “collect now, monetize later, store forever just in case”.\n\n\n\nSome of the many great speakers from our Tech Policy Workshop, who spoke on a variety of topics\n\n\nThese principles can guide us towards a more responsible use of technology by local governments. Technology can be used for good when it is developed and deployed responsibly, with input from a diverse group of relevant stakeholders, and embedded with the appropriate transparency and accountability.\nMore responsible government use of technology was just one of the themes discussed at the Tech Policy Workshop. Stay tuned for more resources and insights from the workshop!"
  },
  {
    "objectID": "posts/2020-08-06-ergonomics/index.html",
    "href": "posts/2020-08-06-ergonomics/index.html",
    "title": "Essential Work-From-Home Advice: Cheap and Easy Ergonomic Setups",
    "section": "",
    "text": "You weren’t expecting to spend 2020 working from home. You can’t afford a fancy standing desk. You don’t have a home office, or even much spare space, in your apartment. Your neck is getting a permanent crick from hunching over your laptop on the couch. While those of us who are able to work from home are privileged to have this option, we still don’t want to permanently damage our backs, necks, or arms from a bad ergonomic setup.\nThis is not a post for ergonomic aficionados (the setups I share could all be further optimized). This is a post for folks who don’t know where to get started, have a limited budget, and are willing to try simple, scrappy approaches. Key takeway: for 34 dollars (21 for a good mouse, and 13 for a cheap keyboard), as well as some household items, you can create an ergonomic setup like the one below. I will show many other options throughout the post, for both sitting and standing, as well as approaches you can easily assemble/disassemble (if you are using the family dinner table and need to clear it off each evening).\n\n\n\nWhile visiting family, I created an ergonomic setup on a counter\n\n\n\nYou can permanently damage your body with bad ergonomics\n\nYou can permanently damage your back, neck, and wrists from working without an ergonomic setup. Almost two decades ago, my partner Jeremy suffered from repetitive stress injury due to working without an ergonomic setup. At the time, his arms were paralyzed and he had to take months off from work. Even now and after years filled with good ergonomics and yoga, this still impacts his life, severely limiting how much time he can spend in cars or on planes, and creating painful flare-ups. Please take this issue seriously.\n\nKey advice: Have a separate keyboard and mouse\n\nThe most important thing to know is that you want your screen approximately at eye height, and your elbows at approximately right angles to your torso as they type and use the mouse. This is the case whether you are sitting or standing. If you are using a laptop, this will be impossible with the built-in keyboard and trackpad (no matter how nice they are). It is essential to have a separate keyboard and mouse. If you only do one thing to address ergonomics, obtain a separate keyboard and mouse.\nIf you can’t afford an external monitor, no worries, you can just elevate your laptop. Over the years, I have used cardboard boxes, drinking glasses, bottles of soda, board games, and stacks of books to elevate my laptop. I will recommend some keyboards and mice that I like below, but anything is better than using the ones built into your laptop (since that forces you to keep your screen at the wrong height). For example, the picture in the intro is of a set-up I created while visiting a family member’s apartment in 2014, using books and a cardboard box to elevate my keyboard, mouse, and laptop to the appropriate heights.\n\n\n\nFor the deep learning study group, I routinely used a brown cardboard box. Bonus: I could store everything in the box when we had the clear out of that room each night.\n\n\nAbove is a picture from the deep learning study group, which meets 5 days a week, for 7 weeks, every time we run the deep learning course. I use a brown cardboard box to elevate my keyboard. We have to clear out of that conference room each evening, and it is simple for me to put my items in the box. This sort of solution could work if you don’t have a dedicated office space in your home, and need to be able to set up/take down your workstation regularly.\nI rarely worked in coffee shops pre-pandemic (and never do now), but when I had to I would still try to create an ergonomic setup (and go to a coffeeshop where there was enough space!). Here, I’ve stacked my laptop on top of my rolled-up backpack. Ideally, my screen would be higher, but this is still better than having it at table level. Don’t let the perfect be the enemy of the good. Every step you take towards a more ergonomic setup is helpful.\n\n\n\nWhen working at a coffee shop (pre-pandemic), I brought an external keyboard and mouse, and used my rolled-up backpack to raise the height of my laptop screen\n\n\n\nAbout standing desks\n\nIf you have a regular desk (or even just a table) at home and want a standing desk, one option is to convert it using the $22 standing desk approach, which involves an Ikea side table and shelf. I had a previous job in which this was quite popular. Here is a photo of my work desk from that time.\n\n\n\nIn a previous job, many of us set up $22 standing desks using Ikea side tables\n\n\nStanding on a hard floor can be difficult for your back. I have a GelPro mat, which I love. If you can’t afford a GelPro mat, standing on a folded-up yoga mat works great too.\nNote that standing desks are not a cure-all. I’ve often seen people with expensive standing-desk converters (also known as desktop risers) that still have their monitor way too low. Even if you have an external monitor and desktop riser, makes sure your monitor is at an appropriate height. It is likely you will still need to stack it on top of something. If you don’t like the aesthetics of using books or other household items, you can buy a monitor stand, such as this one.\nUsing a standing desk with poor posture is not very ergonomic, so be cognizant of when you start feeling fatigued. I prefer to switch between standing and sitting throughout the day, as my energy fluctuates.\n\nBudget Recommendations\n\nMy “budget recommendation” would be to get an Anker vertical mouse for $21 and literally any keyboard. If you have to choose, I’ve found that having a good mouse is way more important than a good keyboard. It is important that you get some keyboard though, so that you can elevate your laptop screen. In the setup below, I’m using a lightweight travel keyboard that isn’t particularly ergonomic, but it works fine.\n\n\n\nThe barista at this coffee shop kindly let me use 2 plastic tubs to prop up my laptop (pre-pandemic).\n\n\nI realize that at a time when many Americans do not have enough to eat, that you may not have 34 dollars to spare (21 dollars for a mouse and 13 dollars for a cheap keyboard). However, if this is an option for you, it is well worth the cost. If you permanently damage your back, neck, or arms, no amount of money may be enough to heal them later.\n\nOther products I like\n\nMy favorite mouse is the Logitech wireless trackball mouse. I have also used and liked the Anker vertical mouse. For keyboards, I like Goldtouch (I use an older version of this one) or the Microsoft Ergonomic Keyboard. And if you are looking for a compact, lightweight travel keyboard, I like the iClever foldup keyboard.\nAs mentioned above, GelPro mats are great if you are going to be standing, and a folded-up yoga mat is a cheaper alternative.\nI have a Roost portable, lightweight laptop stand, which is great, although I can’t use it since I switched from a Macbook Air to a Microsoft Surface Pro. None of the links in this post are affiliate links; I’m just recommending what I’ve personally used and like.\nFor more about home office set-ups, Jeremy recently posted a twitter thread about his preferred computer set-up (which includes some pricier options). It’s also worth noting that his desk has a small footprint, and fits in the corner of our living room.\n\n\n\nI couldn't be happier with my little standing desk setup. I have tried far to many products over the years, and here's what I highly recommend:1/ pic.twitter.com/lMagQPLys1\n\n— Jeremy Howard (@jeremyphoward) July 22, 2020"
  },
  {
    "objectID": "posts/2021-08-17-eleven-ethics-videos/index.html",
    "href": "posts/2021-08-17-eleven-ethics-videos/index.html",
    "title": "11 Short Videos About AI Ethics",
    "section": "",
    "text": "I made a playlist of 11 short videos (most are 6-13 mins long) on Ethics in Machine Learning. This is from my ethics lecture in Practical Deep Learning for Coders v4. I thought these short videos would be easier to watch, share, or skip around.\n\n\n\n\nWhat are Ethics and Why do they Matter? Machine Learning Edition: Through 3 key case studies, I cover how people can be harmed by machine learning gone wrong, why we as machine learning practitioners should care, and what tech ethics are.\nAll machine learning systems need ways to identify & address mistakes. It is crucial that all machine learning systems are implemented with ways to correctly surface and correct mistakes, and to provide recourse to those harmed.\nThe Problem with Metrics, Feedback Loops, and Hypergrowth: Overreliance on metrics is a core problem both in the field of machine learning and in the tech industry more broadly. As Goodhart’s Law tells us, when a measure becomes the target, it ceases to be a good measure, yet the incentives of venture capital push companies in this direction. We see out-of-control feedback loops, widespread gaming of metrics, and people being harmed as a result.\nNot all types of bias are fixed by diversifying your dataset. The idea of bias is often too general to be useful. There are several different types of bias, and different types require different interventions to try to address them. Through a series of cases studies, we will go deeper into some of the various causes of bias.\n\n\n\nPart of the Ethics Videos Playlist\n\n\nHumans are biased too, so why does machine learning bias matter? A common objection to concerns about bias in machine learning models is to point out that humans are really biased too. This is correct, yet machine learning bias differs from human bias in several key ways that we need to understand and which can heighten the impact.\n7 Questions to Ask About Your Machine Learning Project\nWhat You Need to Know about Disinformation: With a particular focus on how machine learning advances can contribute to disinformation, this covers some of the fundamental things to understand.\nFoundations of Ethics: We consider different lenses through which to evaluate ethics, and what sort of questions to ask.\nTech Ethics Practices to Implement at your Workplace: Practical tech ethics practices you can implement at your workplace.\nHow to Address the Machine Learning Diversity Crisis: Only 12% of machine learning researchers are women. Based on research studies, I outline some evidence-based steps to take towards addressing this diversity crisis.\nAdvanced Technology is not a Substitute for Good Policy: We will look at some examples of what incentives cause companies to change their behavior or not (e.g. being warned for years of your role in an escalating genocide vs. threat of a hefty fine), how many AI ethics concerns are actually about human rights, and case studies of what happened when regulation & safety standards came to other industries.\nYou can find the playlist of 11 short videos here. And here is a longer, full-length free fast.ai course on practical data ethics."
  },
  {
    "objectID": "posts/2021-10-12-medicine-political/index.html",
    "href": "posts/2021-10-12-medicine-political/index.html",
    "title": "Medicine is Political",
    "section": "",
    "text": "Experts warn that we are not prepared for the surge in disability due to long covid, an illness that afflicts between one-fourth and one-third of people who get covid, including mild cases, for months afterwards. Some early covid cases have been sick for 18 months, with no end in sight. The physiological damage that covid causes can include cognitive dysfunction and deficits, brain activity scans similar to those seen in Alzheimer’s patients, GI immune system damage, cornea damage, immune dysfunction, increased risk of kidney outcomes, dysfunction in T cell memory generation, pancreas damage, and ovarian failure. Children are at risk too.\nAs the evidence continues to mount of alarming long term physiological impacts of covid, and tens of millions are unable to return to work, we might expect leaders to take covid more seriously. Yet we are seeing concerted efforts to downplay the long-term health effects of covid using strategies straight out of the climate denial playbook, such as funding contrarian scientists, misleading petitions, social media bots, and disingenuous debate tactics that make the science seem murkier than it is. In many cases, these minimization efforts are being funded by the same billionaires and institutions that fund climate change denialism. Dealing with many millions of newly disabled people will be very expensive for governments, social service programs, private insurance companies, and others. Thus, many have a significant financial interest in distorting the science around long term effects of covid to minimize the perceived impact.\nIn topics ranging from covid-19 to HIV research to the long history of wrongly assuming women’s illnesses are psychosomatic, we have seen again and again that medicine, like all science, is political. This shows up in myriad ways, such as: who provides funding, who receives that funding, which questions get asked, how questions are framed, what data is recorded, what data is left out, what categories included, and whose suffering is counted.\nScientists often like to think of their work as perfectly objective, perfectly rational, free from any bias or influence. Yet by failing to acknowledge the reality that there is no “view from nowhere”, they miss their own blindspots and make themselves vulnerable to bad-faith attacks. As one climate scientist recounted of the last 3 decades, “We spent a long time thinking we were engaged in an argument about data and reason, but now we realize it’s a fight over money and power… They [climate change deniers] focused their lasers on the science and like cats we followed their pointer and their lead.”\nThe American Institute for Economic Research (AIER), a libertarian think tank funded by right wing billionaire Charles Koch which invests in fossil fuels, energy utilities, and tobacco, is best known for its research denying the climate crisis. In October 2020, a document called the Great Barrington Declaration (GBD) was developed at a private AIER retreat, calling for a “herd immunity” approach to covid, arguing against lockdowns, and suggesting that young, healthy people have little to worry about. The three scientists who authored the GBD have prestigious pedigrees and are politically well-connected, speaking to White House Officials and having found favor in the British government. One of them, Sunetra Gupta of Oxford, had released a wildly inaccurate paper in March 2020 claiming that up to 68% of the UK population had been exposed to covid, and that there were already significant levels of herd immunity to coronavirus in both the UK and Italy (again, this was in March 2020). Gupta received funding from billionaire conservative donors, Georg and Emily von Opel. Another one of the authors, Jay Bhattacharya of Stanford, co-authored a widely criticized pre-print in April 2020 that relied on a biased sampling method to “show” that 85 times more people in Santa Clara County California had already had covid compared to other estimates, and thus suggested that the fatality rate for covid was much lower than it truly is.\nHalf of the social media accounts advocating for herd immunity seem to be bots, characterized as engaging in abnormally high levels of retweets & low content diversity. An article in the BMJ recently advised that it is “critical for physicians, scientists, and public health officials to realize that they are not dealing with an orthodox scientific debate, but a well-funded sophisticated science denialist campaign based on ideological and corporate interests.”\nThis myth of perfect scientific objectivity positions modern medicine as completely distinct from a history where women were diagnosed with “hysteria” (roaming uterus) for a variety of symptoms, where Black men were denied syphilis treatment for decades as part of a “scientific study”, and multiple sclerosis was “called hysterical paralysis right up to the day they invented a CAT scan machine” and demyelination could be seen on brain scans.\nHowever, there is not some sort of clean break where bias was eliminated and all unknowns were solved. Black patients, including children, still receive less pain medication than white patients for the same symptoms. Women are still more likely to have their physical symptoms dismissed as psychogenic. Nearly half of women with autoimmune disorders report being labeled as “chronic complainers” by their doctors in the 5 years (on average) they spend seeking a diagnosis. All this impacts what data is recorded in their charts, what symptoms are counted.\nMedical data are not objective truths. Like all data, the context is critical. It can be missing, biased, and incorrect. It is filtered through the opinions of doctors. Even blood tests and imaging scans are filtered through the decisions of what tests to order, what types of scans to take, what accepted guidelines recommend, what technology currently exists. And the technology that exists depends on research and funding decisions stretching back decades, influenced by politics and cultural context.\nOne may hope that in 10 years we will have clearer diagnostic tests for some illnesses which remain contested now, just as the ability to identify multiple sclerosis improved with better imaging. In the meantime, we should listen to patients and trust in their ability to explain their own experiences, even if science can’t fully understand them yet.\nScience does not just progress inevitably, independent of funding and politics and framing and biases. A self-fulfilling prophecy often occurs in which doctors:\n\nlabel a new, poorly understood, multi-system disease as psychogenic,\nuse this as justification to not invest much funding into researching physiological origins,\nand then point to the lack of evidence as a reason why the illness must be psychogenic.\n\nThis is largely the experience of ME/CFS patients over the last several decades. Myalgic encephalomyelitis (ME/CFS), involves dysfunction of the immune system, autonomic systems, and energy metabolism (including mitochondrial dysfunction, hypoacetylation, reduced oxygen uptake, and impaired oxygen delivery). ME/CFS is more debilitating than many chronic diseases, including chronic renal failure, lung cancer, stroke, and type-2 diabetes. It is estimated 25–29% of patients are homebound or bedbound. ME/CFS is often triggered by viral infections, so it is not surprising that we are seeing some overlap between ME/CFS and long covid. ME/CFS disproportionately impacts women, and a now discredited 1970 paper identified a major outbreak in 1958 amongst nurses at a British hospital as “epidemic hysteria”. This early narrative of ME/CFS as psychogenic has been difficult to shake. Even as evidence continues to accumulate of immune, metabolic, and autonomous system dysfunction, some doctors persist in believing that ME/CFS must be psychogenic. It has remained woefully underfunded: from 2013-2017, NIH funding was only at 7.3% relative commensurate to its disease burden. Note that the below graph is on a log scale: ME/CFS is at 7%, Depression and asthma are at 100% and diseases like cancer and HIV are closer to 1000%.\n\n\n\nGraph of NIH funding on log scale, from above paper by Mirin, Dimmock, Leonard\n\n\nPortraying patients as unscientific and irrational is the other side of the same coin for the myth that medicine is perfectly rational. Patients that disagree with having symptoms they know are physiological dismissed as psychogenic, that reject treatments from flawed studies, or who distrust medical institutions based on their experiences of racism, sexism, and mis-diagnosis, are labeled as “militant” or “irrational”, and placed in the same category with conspiracy theorists and those peddling disinformation.\nOn an individual level, receiving a psychological misdiagnosis lengthens the time it will take to get the right diagnosis, since many doctors will stop looking for physiological explanations. A study of 12,000 rare disease patients covered by the BBC found that “while being misdiagnosed with the wrong physical disease doubled the time it took to get to the right diagnosis, getting a psychological misdiagnosis extended it even more – by 2.5 up to 14 times, depending on the disease.” This dynamic holds true at the disease level as well: once a disease is mis-labeled as psychogenic, many doctors will stop looking for physiological origins.\nWe are seeing increasing efforts to dismiss long covid as psychogenic in high profile platforms such as the WSJ and New Yorker. The New Yorker’s first feature article on long covid, published last month, neglected to interview any clinicians who treat long covid patients nor to cite the abundant research on how covid causes damage to many organ systems, yet interviewed several doctors in unrelated fields who claim long covid is psychogenic. In response to a patient’s assertion that covid impacts the brain, the author spent an entire paragraph detailing how there is currently no evidence that covid crosses the blood-brain barrier, but didn’t mention the research on covid patients finding cognitive dysfunction and deficits, PET scans similar to those seen in Alzheimer’s patients, neurological damage, and shrinking grey matter. This leaves a general audience with the mistaken impression that it is unproven whether covid impacts the brain, and is a familiar tactic from bad-faith science debates.\nThe New Yorker article set up a strict dichotomy between long covid patients and doctors, suggesting that patients harbor a “disregard for expertise”; are less “concerned about what is and isn’t supported by evidence”; and are overly “impatient.” In contrast, doctors appreciate the “careful study design, methodical data analysis, and the skeptical interpretation of results” that medicine requires. Of course, this is a false dichotomy: many patients are more knowledgeable about the latest research than their doctors, some patients are publishing in peer-reviewed journals, and there are many medical doctors that are also patients. And on the other hand, doctors are just as prone as the rest of us to biases, blind spots, and institutional errors.\n\n\n\nAP Photo/J. Scott Applewhit\n\n\nIn 1987, 40,000 Americans had already died of AIDS, yet the government and pharmaceutical companies were doing little to address this health crisis. AIDS was heavily stigmatized, federal spending was minimal, and pharmaceutical companies lacked urgency. The activists of ACT UP used a two pronged approach: creative and confrontational acts of protest, and informed scientific proposals. When the FDA refused to even discuss giving AIDS patients access to experimental drugs, ACT UP protested at their headquarters, blocking entrances and lying down in front of the building with tombstones saying “Killed by the FDA”. This opened up discussions, and ACT UP offered viable scientific proposals, such as switching from the current approach of conducting drug trials on a small group of people over a long time, and instead testing a large group of people over a short time, radically speeding up the pace at which progress occurred. ACT UP used similar tactics to protest the NIH and pharmaceutical companies, demanding research on how to treat the opportunistic infections that killed AIDS patients, not solely research for a cure. The huge progress that has happened in HIV/AIDS research and treatment would not have happened without the efforts of ACT UP.\nAcross the world, we are at a pivotal time in determining how societies and governments will deal with the masses of newly disabled people due to long covid. Narratives that take hold early often have disproportionate staying power. Will we inaccurately label long covid as psychogenic, primarily invest in psychiatric research that can’t address the well-documented physiological damage caused by covid, and financially abandon the patients who are now unable to work? Or will we take the chance to transform medicine to better recognize the lived experiences and knowledge of patients, to center patient partnerships in biomedical research for complex and multi-system diseases, and strengthen inadequate disability support and services to improve life for all people with disabilities? The decisions we collectively make now on these questions will have reverberations for decades to come."
  },
  {
    "objectID": "posts/2021-11-04-data-disasters/index.html",
    "href": "posts/2021-11-04-data-disasters/index.html",
    "title": "Avoiding Data Disasters",
    "section": "",
    "text": "Things can go disastrously wrong in data science and machine learning projects when we undervalue data work, use data in contexts that it wasn’t gathered for, or ignore the crucial role that humans play in the data science pipeline. A new multi-university centre focused on Information Resilience, funded by the Australian government’s top scientific funding body (ARC), has recently launched. Information Resilience is the capacity to detect and respond to failures and risks across the information chain in which data is sourced, shared, transformed, analysed, and consumed. I’m honored to be a member of the strategy board, and I have been thinking about what information resilience means with respect to data practices. Through a series of case studies and relevant research papers, I will highlight these risks and point towards more resilient practices."
  },
  {
    "objectID": "posts/2021-11-04-data-disasters/index.html#case-study-uk-covid-tracking-app",
    "href": "posts/2021-11-04-data-disasters/index.html#case-study-uk-covid-tracking-app",
    "title": "Avoiding Data Disasters",
    "section": "Case study: UK covid tracking app",
    "text": "Case study: UK covid tracking app\nData from a covid-symptom tracking app was used in a research paper to draw wildly inaccurate conclusions about the prevalence of Long Covid, the often debilitating neurological, vascular, and immune disease that can last for months or longer (some patients have been sick for 20 months and counting). The app suggested that only 1.5% of patients still experience symptoms after 3 months, an order of magnitude smaller than estimates of 10-35% being found by other studies.\nHow could this research project have gone so wrong? Well, the app had been designed for a completely different purpose (tracking 1-2 week long respiratory infections), didn’t include the most common Long Covid symptoms (such as neurological dysfunction), had a frustrating user-interface that led many patients to quit using it, and made the erroneous assumption that those who stopped logging must be fully recovered. The results from this faulty research paper were widely shared, including in a BBC article, offering false reassurance than Long Covid prevalence is much rarer than it is. Patients had been voicing their frustrations with the app all along, and if researchers had listened sooner, they could have collected a much higher quality and more accurate data set.\nThis research failure illustrates a few common issues in data projects:\n\nThe context of the data was not taken into account. The user-interface, the categories listed, the included features– these were all designed to record data about a short-term mild respiratory infection. However, when it was used for a different purpose (long covid patients suffering for months with vascular and neurological symptoms), it did a poor job, and led to missing and incomplete data. This happens all too often, in which data gathered for one context is used for another\nThe people most impacted (long covid patients) were ignored. They had the most accurate expertise on what long covid actually entailed, yet were not listened to. Ignoring this expertise led to lower quality data and erroneous research conclusions. Patients have crucial domain expertise, which is distinct from that of doctors, and must be included in medical data science projects. From the start of the pandemic, patients who had suffered from other debilitating post-viral illnesses warned that we should be on the lookout for long-term illness, even in initially “mild” cases."
  },
  {
    "objectID": "posts/2021-11-04-data-disasters/index.html#data-is-crucial",
    "href": "posts/2021-11-04-data-disasters/index.html#data-is-crucial",
    "title": "Avoiding Data Disasters",
    "section": "Data is Crucial",
    "text": "Data is Crucial\nCollecting data about covid and its long-term effects directly from patients was a good idea, but poorly executed in this case. Due to privacy and surveillance risks, I frequently remind people not to record data that they don’t need. However, the pandemic has been a good reminder of how much data we really do need, and how tough it is when it’s missing.\nAt the start of the pandemic in the United States, we had very little data about what was happening– the government was not tabulating information on cases, testing, or hospitalization. How could we know how to react when we didn’t understand how many cases there were, what death rates were, how transmissible the disease was, and other crucial information? How could we make policy decisions in the absence of a basic understanding of the facts.\nIn early March 2020, two journalists and a data scientist from a medication-discovery platform began pulling covid data together into a spreadsheet to understand the situation in the USA. This launched into a 15-month long project in which 500 volunteers compiled and published data on COVID-19 testing, cases, hospitalizations, and deaths in the USA. During those 15 months, the Covid Tracking Project was the most comprehensive source of covid data in the USA, even more comprehensive than what the CDC had, and it was used by the CDC, numerous government agencies, and both the Trump and Biden Administrations. It was cited in academic studies and in thousands of news articles.\n\n\n\nA reflection on the covid tracking project from a core data infrastructure engineer\n\n\nA data infrastructure engineer and contributor for the project later recounted, “It quickly became apparent that daily, close contact with the data was necessary to understand what states were reporting. States frequently changed how, what, and where they reported data. Had we set up a fully automated data capture system in March 2020, it would have failed within days.” The project used automation as a way to support and supplement manual work, not to replace it. At numerous points, errors in state reporting mechanisms were caught by eagle-eyed data scientists notifying discrepancies.\nThis vision of using automation to support human work resonates with our interest at fast.ai in “augmentedML”, not “autoML”. I have written previously and gave an AutoML workshop keynote on how too often automation ignores the important role of human input. Rather than try to automate everything (which often fails), we should focus on how humans and machines can best work together to take advantage of their different strengths.\n\n\n\nSpeaking about AugmentedML vs. AutoML at ICML 2019"
  },
  {
    "objectID": "posts/2021-11-04-data-disasters/index.html#data-work-is-undervalued",
    "href": "posts/2021-11-04-data-disasters/index.html#data-work-is-undervalued",
    "title": "Avoiding Data Disasters",
    "section": "Data Work is Undervalued",
    "text": "Data Work is Undervalued\nInterviews of 53 AI practitioners across 6 countries on 3 continents found a pattern that is very familiar to many of us (including me) who work in machine learning: “Everyone wants to do the model work, not the data work.” Missing meta-data leads to faulty assumptions. Data collection practices often conflict with the workflows of on-the-ground partners, such as nurses or farmers, who are usually not compensated for this extraneous effort. Too often data work is arduous, invisible, and taken for granted. Undervaluing of data work leads to poor practices and often results in negative, downstream events, including dangerously inaccurate models and months of lost work.\nThroughout the pandemic, data about covid (both initial cases and long covid) has often been lacking. Many countries have experienced testing shortages, leading to undercounts of how many people have covid. The CDC decision not to track breakthrough cases unless they resulted in hospitalization made it harder to understand prevalence of break-throughs (a particularly concerning decision since break-throughs can still lead to long covid). In September, it was revealed that British Columbia, Canada was not including covid patients in their ICU counts once the patients were no longer infectious, a secretive decision that obscured how full ICUs were. Some studies of Long Covid have failed to include common symptoms, such as neurological ones, making it harder to understand the prevalence or nature."
  },
  {
    "objectID": "posts/2021-11-04-data-disasters/index.html#data-has-context",
    "href": "posts/2021-11-04-data-disasters/index.html#data-has-context",
    "title": "Avoiding Data Disasters",
    "section": "Data has Context",
    "text": "Data has Context\nCovid is giving us a first-hand view of how data, which we may sometimes want to think of as “objective”, are shaped by countless human decisions and factors. In the example of the symptom tracking app, decisions about which symptoms were included had a significant impact on the prevalence rate calculated. Design decisions that influenced the ease of use impacted how much data was gathered. Lack of understanding of how the app was being used (and why people quit using it) led to erroneous decisions about which cases should be considered “recovered”. These are all examples of the context for data. Here, the data gathered was reasonably appropriate for understanding initial covid infections (a week or two of respiratory symptoms), but not for patients experiencing months of neurological and vascular symptoms. Numbers can not stand alone, we need to understand how they were measured, who was included and excluded, relevant design decisions, under what situations a dataset is appropriate to use vs. not.\nAs another example, consider covid testing counts: Who has access to testing (this involves health inequities, due to race or urban vs. rural), who is encouraged to get tested (at various times, people without symptoms, children, or other groups have been discouraged from doing so), varying accuracies (e.g. PCR tests are less accurate on children, missing almost half of cases that later go on to seroconvert), and making decisions about what counts as a “case” (I know multiple people who had alternating test results: positive, negative, positive, or the reverse– what counts as a positive case?)\n\n\n\nDatasheet for an electrical component. Image from ‘Datasheets for Datasets’\n\n\nOne proposal for capturing this context is Datasheets for Datasets. Prior to doing her PhD at Stanford in computer vision and then co-leading Google’s AI ethics team, Dr. Timnit Gebru worked at Apple in circuit design and electrical engineering. In electronics, each component (such as a circuit or transistor) comes with a datasheet that lists when and where it was manufactured, under what conditions it is safe to use, and other specifications. Dr. Gebru drew on this background to propose a similar idea for datasets: listing the context of when and how it was created, what data was included/excluded, recommended uses, potential biases and ethical risks, work needed to maintain it, and so on. This is a valuable proposal towards making the context of data more explicit."
  },
  {
    "objectID": "posts/2021-11-04-data-disasters/index.html#the-people-most-impacted",
    "href": "posts/2021-11-04-data-disasters/index.html#the-people-most-impacted",
    "title": "Avoiding Data Disasters",
    "section": "The People Most Impacted",
    "text": "The People Most Impacted\nThe inaccurate research and incomplete data from the covid tracking app could have been avoided by drawing on the expertise of patients. Higher quality data could have been collected sooner and more thoroughly, if patients were consulted in the app design and in the related research studies. Participatory approaches to machine learning is an exciting and growing area of research. In any domain, the people who would be most impacted by errors or mistakes need to be included as partners in the design of the project.\n\n\n\nThe Diverse Voices project from University of Washington Tech Policy Lab involves academic papers and practical how-to guides.\n\n\nOften, our approaches to addressing fairness or other ethics issues, further centralize the power of system designers and operators. The organizers of an ICML workshop on the topic called for more cooperative, democratic, and participatory approaches instead. We need to think not just about explainability, but about giving people actionable recourse. As Professor Berk Ustun highlights, when someone asks why their loan was denied, usually what they want is not just an explanation but to know what they could change in order to get a loan. We need to design systems with contestability in mind, to include from the start the idea that people should be able to challenge system outputs. We need to include expert panels of perspectives that are often overlooked, depending on the application, this could mean formerly or currently incarcerated people, people who don’t drive, people with very low incomes, disabled people, and many others. The Diverse Voices project from University of Washington Tech Lab provides guidance on how to do this. And it is crucial that this not just be tokenistic participation-washing, but a meaningful, appropriately compensated, and ongoing role in their design and operation."
  },
  {
    "objectID": "posts/2021-11-04-data-disasters/index.html#towards-greater-data-resilience",
    "href": "posts/2021-11-04-data-disasters/index.html#towards-greater-data-resilience",
    "title": "Avoiding Data Disasters",
    "section": "Towards Greater Data Resilience",
    "text": "Towards Greater Data Resilience\nI hope that we can improve data resilience through: - Valuing data work - Documenting context of data - Close contact with the data - Meaningful, ongoing, and compensated involvement of the people impacted\nAnd I hope that when our data represents people we can remember the human side. As AI researcher Inioluwa Deborah Raji wrote, “Data are not bricks to be stacked, oil to be drilled, gold to be mined, opportunities to be harvested. Data are humans to be seen, maybe loved, hopefully taken care of.”\n\n\n\nQuote from AI researcher Inioluwa Deborah Raji"
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html",
    "href": "posts/2022-03-15-math-person/index.html",
    "title": "There’s no such thing as not a math person",
    "section": "",
    "text": "On the surface, I may seem into math: I have a math PhD, taught a graduate computational linear algebra course, co-founded AI research lab fast.ai, and even go by the twitter handle @math_rachel.\nYet many of my experiences of academic math culture have been toxic, sexist, and deeply alienating. At my lowest points, I felt like there was no place for me in math academia or math-heavy tech culture.\nIt is not just mathematicians or math majors who are impacted by this: Western culture is awash in negative feelings and experiences regarding math, which permate from many sources and impact students of all ages. In this post, I will explore the cultural factors, misconceptions, stereotypes, and relevant studies on obstacles that turn people off to math. If you (or your child) doesn’t like math or feels anxious about your own capabilities, you’re not alone, and this isn’t just a personal challenge. The below essay is based on part of a talk I recently gave."
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#myth-of-innate-ability-myth-of-the-lone-genius",
    "href": "posts/2022-03-15-math-person/index.html#myth-of-innate-ability-myth-of-the-lone-genius",
    "title": "There’s no such thing as not a math person",
    "section": "Myth of Innate Ability, Myth of the Lone Genius",
    "text": "Myth of Innate Ability, Myth of the Lone Genius\nOne common myth is the idea that certain people’s brains aren’t “wired” the right way to do math, tech, or AI, that your brain either “works that way” or not. None of the evidence supports this viewpoint, yet when people believe this, it can become a self-fulfilling prophecy. Dr. Omoju Miller, who earned her PhD at UC Berkeley and was a senior machine learning engineer and technical advisor to the CEO at Github, shares some of the research debunking the myth of innate ability in this essay and in her TEDx talk. In reality, there is no such thing as “not a math person.”\nDr. Cathy O’Neil, a Harvard Math PhD and author of Weapons of Math Destruction, wrote about the myth of the lone genius mathematician, “You don’t have to be a genius to become a mathematician. If you find this statement at all surprising, you’re an example of what’s wrong with the way our society identifies, encourages and rewards talent… For each certified genius, there are at least a hundred great people who helped achieve such outstanding results.”\n\n\n\nDr. Miller debunking the myth of innate ability, and Dr. O’Neil debunking the myth of the lone genius mathematician"
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#music-without-singing-or-instruments",
    "href": "posts/2022-03-15-math-person/index.html#music-without-singing-or-instruments",
    "title": "There’s no such thing as not a math person",
    "section": "Music without singing or instruments",
    "text": "Music without singing or instruments\nImagine a world where children are not allowed to sing songs or play instruments until they reach adulthood, after spending a decade or two transcribing sheet music by hand. This scenario is absurd and nightmarish, yet it is analogous to how math is often taught, with the most creative and interesting parts saved until almost everyone has dropped out. Dr. Paul Lockhart eloquently describes this metaphor in his essay, A Mathematician’s Lament, on “how school cheats us out of our most fascinating and imaginative art form.” Dr. Lockhart left his role as a university math professor to teach K-12 math, as he felt that so much reform was needed in how math is taught.\nDr. David Perkins uses the analogy of how children can play baseball wthout knowing all the technical details, without having a full team or playing a full 9 innings, yet still gain a sense of the “whole game.” Math is usually taught with an overemphasis on dry, technical details, without giving students a concept of the “whole game.” It can take years and years before enough technical details are accumulated to build something interesting. There is an overemphasis on techniques rather than meaning.\n\n\n\nWhat if math was taught more like how music or sports are taught?\n\n\nMath curriculums are usually arranged in a vertical manner, with each year building tightly on the previous, such that one bad year can ruin everything that comes after. Many people I talk to can pinpoint the year that math went bad for them: “I used to like math until 6th grade, when I had a bad teacher/was dealing with peer pressure/my undiagnosed ADHD was out of control. After that, I was never able to succeed in future years.” This is less true in other subjects, where one bad history teacher/one bad year doesn’t mean that you can’t succeed at history the following year."
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#gender-race-and-stereotypes",
    "href": "posts/2022-03-15-math-person/index.html#gender-race-and-stereotypes",
    "title": "There’s no such thing as not a math person",
    "section": "Gender, race, and stereotypes",
    "text": "Gender, race, and stereotypes\nFemale teachers’ math anxiety affects girls’ math achievement: In the USA, over 90% of primary school teachers are female, and research has found “the more anxious teachers were about math, the more likely girls (but not boys) were to endorse the commonly held stereotype that ‘boys are good at math, and girls are good at reading’ and the lower these girls’ math achievement… People’s fear and anxiety about doing math—over and above actual math ability—can be an impediment to their math achievement.”\nResearch across a number of universities has found that more women go into engineering when courses focus on problems with positive social impact.\nStructural racism also impacts what messages teachers impart to students. An Atlantic article How Does Race Affect a Student’s Math Education? covered the research paper A Framework for Understanding Whiteness in Mathematics Education, noting that “Constantly reading and hearing about underperforming Black, Latino, and Indigenous students begins to embed itself into how math teachers view these students, attributing achievement differences to their innate ability to succeed in math… teachers start to expect worse performance from certain students, start to teach lower content, and start to use lower-level math instructional practices. By contrast, white and Asian students are given the benefit of the doubt and automatically afforded the opportunity to do more sophisticated and substantive mathematics.”"
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#the-mathematics-community-is-an-absolute-mess-which-actively-pushes-out-the-sort-of-people-who-might-make-it-better",
    "href": "posts/2022-03-15-math-person/index.html#the-mathematics-community-is-an-absolute-mess-which-actively-pushes-out-the-sort-of-people-who-might-make-it-better",
    "title": "There’s no such thing as not a math person",
    "section": "The mathematics community is “an absolute mess which actively pushes out the sort of people who might make it better”",
    "text": "The mathematics community is “an absolute mess which actively pushes out the sort of people who might make it better”\n\n\n\nDr. Harron’s website, and some of the coverage of her number theory thesis, including on the Scientific American blog\n\n\nDr. Piper Harron made waves with her Princeton PhD thesis, utilizing humor, analogies, sarcasm, and genuine efforts to be accessible as she described advanced concepts in a ground-breaking way, very atypical for a mathematics PhD thesis. Dr. Harron wrote openly in the prologue of her thesis on how alienating the culture of mathematics is, “As any good grad student would do, I tried to fit in, mathematically. I absorbed the atmosphere and took attitudes to heart. I was miserable, and on the verge of failure. The problem was not individuals, but a system of self-preservation that, from the outside, feels like a long string of betrayals, some big, some small, perpetrated by your only support system.” At her blog, the Liberated Mathematician, she writes, “My view of mathematics is that it is an absolute mess which actively pushes out the sort of people who might make it better.”\nThese descriptions resonate with my own experiences obtaining a math PhD (as well as the experiences of many friends, at a variety of universities). The toxicity of academic math departments is self-perpetuating, pushing out the people who could make them better."
  },
  {
    "objectID": "posts/2022-03-15-math-person/index.html#the-full-talk",
    "href": "posts/2022-03-15-math-person/index.html#the-full-talk",
    "title": "There’s no such thing as not a math person",
    "section": "The full talk",
    "text": "The full talk\nThis post is based on the first part of the talk I gave in the below video, which includes more detail and a Q&A. The talk also includes recommendations about math apps and resources, as well as a framework for how to consider screentime. Stay tuned for a future fast.ai blog post covering math apps and screentime."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html",
    "href": "posts/2022-05-17-societal-harms/index.html",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "",
    "text": "When the USA government switched to facial identification service ID.me for unemployment benefits, the software failed to recognize Bill Baine’s face. While the app said that he could have a virtual appointment to be verified instead, he was unable to get through. The screen had a wait time of 2 hours and 47 minutes that never updated, even over the course of weeks. He tried calling various offices, his daughter drove in from out of town to spend a day helping him, and yet he was never able to get a useful human answer on what he was supposed to do, as he went for months without unemployment benefits. In Baine’s case, it was eventually resolved when a journalist hypothesized that the issue was a spotty internet connection, and that Baine would be better off traveling to another town to use a public library computer and internet. Even then, it still took hours for Baine to get his approval.\n\n\n\nJournalist Andrew Kenney of Colorado Public Radio has covered the issues with ID.me\n\n\nBaine was not alone. The number of people receiving unemployment benefits plummeted by 40% in the 3 weeks after ID.me was introduced. Some of these were presumed to be fraudsters, but it is unclear how many genuine people in need of benefits were wrongly harmed by this. These are individual harms, but there are broader, societal harms as well: the cumulative costs of the public having to spend ever more time on hold, trying to navigate user-hostile automated bureaucracies where they can’t get the answers they need. There is the societal cost of greater inequality and greater desperation, as more people are plunged into poverty through erroneous denial of benefits. And there is the undermining of trust in public services, which can be difficult to restore.\nPotential for algorithmic harm takes many forms: loss of opportunity (employment or housing discrimination), economic cost (credit discrimination, narrowed choices), social detriment (stereotype confirmation, dignitary harms), and loss of liberty (increased surveillance, disproportionate incarceration). And each of these four categories manifests in both individual and societal harms.\nIt should come as no surprise that algorithmic systems can give rise to societal harm. These systems are sociotechnical: they are designed by humans and teams that bring their values to the design process, and algorithmic systems continually draw information from, and inevitably bear the marks of, fundamentally unequal, unjust societies. In the context of COVID-19, for example, policy experts warned that historical healthcare inequities risked making their way into the datasets and models being used to predict and respond to the pandemic. And while it’s intuitively appealing to think of large-scale systems as creating the greatest risk of societal harm, algorithmic systems can create societal harm because of the dynamics set off by their interconnection with other systems/ players, like advertisers, or commercially-driven media, and the ways in which they touch on sectors or spaces of public importance.\nStill, in the west, our ideas of harm are often anchored to an individual being harmed by a particular action at a discrete moment in time. As law scholar Natalie Smuha has powerfully argued, legislation (both proposed and passed) in Western countries to address algorithmic risks and harms often focuses on individual rights: regarding how an individual’s data is collected or stored, to not be discriminated against, or to know when AI is being used. Even metrics used to evaluate the fairness of algorithms are often aggregating across individual impacts, but unable to capture longer-term, more complex, or second- and third-order societal impacts."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#case-study-privacy-and-surveillance",
    "href": "posts/2022-05-17-societal-harms/index.html#case-study-privacy-and-surveillance",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Case Study: Privacy and surveillance",
    "text": "Case Study: Privacy and surveillance\nConsider the over-reliance on individual harms in discussing privacy: so often focused on whether individual users have the ability to opt in or out of sharing their data, notions of individual consent, or proposals that individuals be paid for their personal data. Yet widespread surveillance fundamentally changes society: people may begin to self-censor and to be less willing (or able) to advocate for justice or social change. Professor Alvaro Bedoya, director of the Center on Privacy and Technology at the Georgetown University Law Center, traces a history of how surveillance has been used by the state to try to shut down movements for progress– targeting religious minorities, poor people, people of color, immigrants, sex workers and those considered “other”. As Maciej Ceglowski writes, “Ambient privacy is not a property of people, or of their data, but of the world around us… Because our laws frame privacy as an individual right, we don’t have a mechanism for deciding whether we want to live in a surveillance society.”\nDrawing on interviews with African data experts, Birhane et al write that even when data is anonymized and aggregated, it “can reveal information on the community as a whole. While notions of privacy often focus on the individual, there is growing awareness that collective identity is also important within many African communities, and that sharing aggregate information about communities can also be regarded as a privacy violation.” Recent US-based scholarship has also highlighted the importance of thinking about group level privacy (whether that group is made up of individuals who identify as members of that group, or whether it’s a ‘group’ that is algorithmically determined - like individuals with similar shopping habits on Amazon). Because even aggregated anonymised data can reveal important group-level information (e.g., the location of military personnel training via exercise tracking apps) “managing privacy”, these authors argue “is often not intrapersonal but interpersonal.” And yet legal and tech design privacy solutions are often better geared towards assuring individual-level privacy than negotiating group privacy."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#case-study-disinformation-and-erosion-of-trust",
    "href": "posts/2022-05-17-societal-harms/index.html#case-study-disinformation-and-erosion-of-trust",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Case Study: Disinformation and erosion of trust",
    "text": "Case Study: Disinformation and erosion of trust\nAnother example of a collective societal harm comes from how technology platforms such as Facebook have played a significant role in elections ranging from the Philippines to Brazil, yet it can be difficult (and not necessarily possible or useful) to quantify exactly how much: something as complex as a country’s political system and participation involves many interlinking factors. But when ‘deep fakes’ make it “possible to create audio and video of real people saying and doing things they never said or did” or when motivated actors successfully game search engines to amplify disinformation, the (potential) harm that is generated is societal, not just individual. Disinformation and the undermining of trust in institutions and fellow citizens have broad impacts, including on individuals who never use social media.\n\n\n\nReports and Events on Regulatory Approaches to Disinformation\n\n\nEfforts by national governments to deal with the problem through regulation have not gone down well with everyone. ‘Disinformation’ has repeatedly been highlighted as one of the tech-enabled ‘societal harms’ that the UK’s Online Safety Bill or the EU’s Digital Services Act should address, and a range of governments are taking aim at the problem by proposing or passing a slew of (in certain cases ill-advised) ‘anti-misinformation’ laws. But there’s widespread unease around handing power to governments to set standards for what counts as ‘disinformation’. Does reifying ‘disinformation’ as a societal harm become a legitimizing tool for governments looking to silence political dissent or undermine their weaker opponents? It’s a fair and important concern - and yet simply leaving that power in the hands of mostly US-based, unaccountable tech companies is hardly a solution. What are the legitimacy implications if a US company like Twitter were to ban democratically elected Brazilian President Jair Bolsonaro for spreading disinformation, for example? How do we ensure that tech companies are investing sufficiently in governance efforts across the globe, rather than responding in an ad hoc manner to proximal (i.e. mostly US-based) concerns about disinformation? Taking a hands off approach to platform regulation doesn’t make platforms’ efforts to deal with disinformation any less politically fraught."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#individual-harms-individual-solutions",
    "href": "posts/2022-05-17-societal-harms/index.html#individual-harms-individual-solutions",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Individual Harms, Individual Solutions",
    "text": "Individual Harms, Individual Solutions\nIf we consider individual solutions our only option (in terms of policy, law, or behavior), we often limit the scope of the harms we can recognize or the nature of the problems we face. To take an example not related to AI: Oxford professor Trish Greenhalgh et al analyzed the slow reluctance of leaders in the West to accept that covid is airborne (e.g. it can linger and float in the air, similar to cigarette smoke, requiring masks and ventilation to address), rather than droplet dogma (e.g. washing your hands is a key precaution). One reason they highlight is the Western framing of individual responsibility as the solution to most problems. Hand-washing is a solution that fits the idea of individual responsibility, whereas collective responsibility for the quality of shared indoor air does not. The allowable set of solutions helps shape what we identify as a problem. Additionally, the fact that recent research suggests that “the level of interpersonal trust in a society” was a strong predictor of which countries managed COVID-19 most successfully should give us pause. Individualistic framings can limit our imagination about the problems we face and which solutions are likely to be most impactful."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#parallels-with-environmental-harms",
    "href": "posts/2022-05-17-societal-harms/index.html#parallels-with-environmental-harms",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Parallels with Environmental Harms",
    "text": "Parallels with Environmental Harms\nBefore the passage of environmental laws, many existing legal frameworks were not well-suited to address environmental harms. Perhaps a chemical plant releases waste emissions into the air once per week. Many people in surrounding areas may not be aware that they are breathing polluted air, or may not be able to directly link air pollution to a new medical condition, such as asthma, (which could be related to a variety of environmental and genetic factors).\n\n\n\nThere are parallels between air polllution and algorithmic harms\n\n\nThere are many parallels between environmental issues and AI ethics. Environmental harms include individual harms for people who develop discrete health issues from drinking contaminated water or breathing polluted air. Yet, environmental harms are also societal: the societal costs of contaminated water and polluted air can reverberate in subtle, surprising, and far-reaching ways. As law professor Nathalie Smuha writes, environmental harms are often accumulative and build over time. Perhaps each individual release of waste chemicals from a refinery has little impact on its own, but adds up to be significant. In the EU, environmental law allows for mechanisms to show societal harm, as it would be difficult to challenge many environmental harms on the basis of individual rights. Smuha argues that there are many similarities with AI ethics: for opaque AI systems, spanning over time, it can be difficult to prove a direct causal relationship to societal harm."
  },
  {
    "objectID": "posts/2022-05-17-societal-harms/index.html#directions-forward",
    "href": "posts/2022-05-17-societal-harms/index.html#directions-forward",
    "title": "AI Harms are Societal, Not Just Individual",
    "section": "Directions Forward",
    "text": "Directions Forward\nTo a large extent our message is to tech companies and policymakers. It’s not enough to focus on the potential individual harms generated by tech and AI: the broader societal costs of tech and AI matter.\nBut those of us outside tech policy circles have a crucial role to play. One way in which we can guard against the risks of the ‘societal harm’ discourse being co-opted by those with political power to legitimise undue interference and further entrench their power is by claiming the language of ‘societal harm’ as the democratic and democratising tool it can be. We all lose when we pretend societal harms don’t exist, or when we acknowledge they exist but throw our hands up. And those with the least power, like Bill Baine, are likely to suffer a disproportionate loss.\nIn his newsletter on Tech and Society, L.M. Sacasas encourages people to ask themselves 41 questions before using a particular technology. They’re all worth reading and thinking about - but we’re listing a few especially relevant ones to get you started. Next time you sit down to log onto social media, order food online, swipe right on a dating app or consider buying a VR headset, ask yourself:\n\nHow does this technology empower me? At whose expense? (Q16)\nWhat feelings does the use of this technology generate in me toward others? (Q17)\nWhat limits does my use of this technology impose upon others? (Q28)\nWhat would the world be like if everyone used this technology exactly as I use it? (Q37)\nDoes my use of this technology make it easier to live as if I had no responsibilities toward my neighbor? (Q40)\nCan I be held responsible for the actions which this technology empowers? Would I feel better if I couldn’t? (Q41)\n\nIt’s on all of us to sensitise ourselves to the societal implications of the tech we use."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html",
    "href": "posts/2022-06-01-qualitative/index.html",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "",
    "text": "“All research is qualitative; some is also quantitative” – Harvard Social Scientist and Statistician Gary King\nSuppose you wanted to find out whether a machine learning system being adopted - to recruit candidates, lend money, or predict future criminality - exhibited racial bias. You might calculate model performance across groups with different races. But how was race categorised– through a census record, a police officer’s guess, or by an annotator? Each possible answer raises another set of questions. Following the thread of any seemingly quantitative issue around AI ethics quickly leads to a host of qualitative questions. Throughout AI, qualitative decisions are made about what metrics to optimise for, which categories to use, how to define their bounds, who applies the labels. Similarly, qualitative research is necessary to understand AI systems operating in society: evaluating system performance beyond what can be captured in short term metrics, understanding what is missed by large-scale studies (which can elide details and overlook outliers), and shedding light on the circumstances in which data is produced (often by crowd-sourced or poorly paid workers).\nUnfortunately, there is often a large divide between computer scientists and social scientists, with over-simplified assumptions and fundamental misunderstandings of one another. Even when cross-disciplinary partnerships occur, they often fall into “normal disciplinary divisions of labour: social scientists observe, data scientists make; social scientists do ethics, data scientists do science; social scientists do the incalculable, data scientists do the calculable.” The solution is not for computer scientists to absorb a shallow understanding of the social sciences, but for deeper collaborations. In a paper on exclusionary practices in AI ethics, an interdisciplinary team wrote of the “indifference, devaluation, and lack of mutual support between CS and humanistic social science (HSS), [which elevates] the myth of technologists as ‘ethical unicorns’ that can do it all, though their disciplinary tools are ultimately limited.”\nThis is further reflected in an increasing number of job ads for AI ethicists that list a computer science degree as a requirement, “prioritising technical computer science infrastructure over the social science skills that can evaluate AI’s social impact. In doing so, we are building the field of AI Ethics to replicate the very flaws this field is trying to fix.” Interviews with 26 responsible AI practitioners working in industry highlighted a number of challenges, including that qualitative work was not prioritised. Not only is it impossible to fully understand ethics issues solely through quantitative metrics, inappropriate and misleading quantitative metrics are used to evaluate the responsible AI practitioners themselves. Interviewees reported that their fairness work was evaluated on metrics related to generating revenue, in a stark misalignment of goals."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#qualitative-research-helps-us-evaluate-ai-systems-beyond-short-term-metrics",
    "href": "posts/2022-06-01-qualitative/index.html#qualitative-research-helps-us-evaluate-ai-systems-beyond-short-term-metrics",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "Qualitative research helps us evaluate AI systems beyond short term metrics",
    "text": "Qualitative research helps us evaluate AI systems beyond short term metrics\nWhen companies like Google and YouTube want to test whether the recommendations they are making (in the form of search engine results or YouTube videos, for example) are “good” - they will often focus quite heavily on “engagement” or “dwell time” - the time a user spent looking at or watching the item recommended to them. But it turns out, unsurprisingly, that a focus on engagement and dwell time, narrowly understood, raises all sorts of problems. Demographics can impact dwell time (e.g. older users may spend longer on websites than younger users, just as part of the way they use the internet). A system that ‘learns’ from a user’s behavioural cues (rather than their ‘stated preferences’) might lock them into a limiting feedback loop, appealing to that user’s short term interests rather than those of their ‘Better Selves.’ Scholars have called for more qualitative research to understand user experience and build this into the development of metrics.\nThis is the part where people will point out, rightly, that companies like Google and YouTube rely on a complex range of metrics and signals in their machine learning systems - and that where a website ranks on Google, or how a YouTube video performs in recommendation does not boil down to simple popularity metrics, like engagement. Google employs an extensive process to determine “relevance” and “usefulness” for search results. In its 172-page manual for search result ‘Quality’ evaluation, for example, the company explains how evaluators should assess a website’s ‘Expertise/ Authoritativeness/ Trustworthiness’ or ‘E-A-T’; and what types of content, by virtue of its harmful nature (e.g., to protected groups), should be given a ‘low’ ranking. YouTube has identified specific categories of content (such as news, scientific subjects, and historical information) for which ‘authoritativeness’ should be considered especially important. It has also determined that dubious-but-not-quite-rule-breaking information (what it calls ‘borderline content’) should not be recommended, regardless of the video’s engagement levels.\nIrrespective of how successful we consider the existing approaches of Google Search and YouTube to be (and partly, the issue is that evaluating their implementation from the outside is frustratingly difficult), the point here is that there are constant qualitative judgments being made, about what makes a search result or recommendation “good” and of how to define and quantify expertise, authoritativeness, trustworthiness, borderline content, and other values. This is true of all machine learning evaluation, even when it isn’t explicit. In a paper guiding companies about how to carry out internal audits of their AI systems, Inioluwa Deborah Raji and colleagues emphasise the importance of interviews with management and engineering teams to “capture and pay attention to what falls outside the measurements and metrics, and to render explicit the assumptions and values the metrics apprehend.” (p.40).\nThe importance of thoughtful humanities research is heightened if we are serious about grappling with the potential broader social effects of machine learning systems (both good and bad), which are often delayed, distributed and cumulative."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#small-scale-qualitative-studies-tell-an-important-story",
    "href": "posts/2022-06-01-qualitative/index.html#small-scale-qualitative-studies-tell-an-important-story",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "Small-scale qualitative studies tell an important story",
    "text": "Small-scale qualitative studies tell an important story\nHypothetically, let’s say you wanted to find out whether the use of AI technologies by doctors during a medical appointment would make doctors less attentive to patients - what do you think the best way of doing it would be? You could find some criteria and method for measuring ‘attentiveness’, say tracking the amount of eye contact between the doctor and patient, and analyse this across a representative sample of medical appointments where AI technologies were being used, compared to a control group of medical appointments where AI technologies weren’t being used. Or would you interview doctors about their experiences using the technology during appointments? Or talk to patients about how they felt the technology did, or didn’t, impact their experience?\nIn research circles, we describe these as ‘epistemological’ choices - your judgement of what constitutes the ‘best’ approach is inextricably linked to your judgement about how we can claim to ‘know’ something. These are all valid methods for approaching the question, but you can imagine how they might result in different, even conflicting, insights. For example, you might end up with the following results: - The eye contact tracking experiment suggests that overall, there is no significant difference in doctors’ attentiveness to the patient when the AI tech is introduced. - The interviews with doctors and patients reveal that some doctors and patients feel that the AI technology reduces doctors’ attentiveness to patients, and others feel that it makes no difference or even increases doctors’ attention to the patient.\nEven if people are not negatively impacted by something ‘on average’ (e.g., in our hypothetical eye contact tracking experiment above), there will remain groups of people who will experience negative impacts, perhaps acutely so. “Many of people’s most pressing questions are about effects that vary for different people,” write Matias, Pennington and Chan in a recent paper on the idea of N-of-one trials. To tell people that their experiences aren’t real or valid because they don’t meet some threshold for statistical significance across a large population doesn’t help us account for the breadth and nature of AI’s impacts on the world.\nExamples of this tension between competing claims to knowledge about AI systems’ impacts abound. Influencers who believe they are being systematically downranked (‘shadowbanned’) by Instagram’s algorithmic systems are told by Instagram that this simply isn’t true. Given the inscrutability of these proprietary algorithmic systems, it is impossible for influencers to convincingly dispute Instagram’s claims. Kelley Cotter refers to this as a form of “black box gaslighting”: platforms can “leverage perceptions of their epistemic authority on their algorithms to undermine users’ confidence in what they know about algorithms and destabilise credible criticism.” Her interviews with influencers give voice to stakeholder concerns and perspectives that are elided in Instagram’s official narrative about its systems. The mismatch between different stakeholders’ accounts of ‘reality’ is instructive. For example, a widely-cited paper by Netflix employees claims that Netflix recommendation “influences choice for about 80% of hours streamed at Netflix.” But this claim stands in stark contrast to Mattias Frey’s mixed-methods research (representative survey plus small sample for interviews) run with UK and US adults, in which less than 1 in 5 adults said they primarily relied on Netflix recommendations when deciding what films to watch. Even if this is because users underestimate their reliance on recommender systems, that’s a critically important finding - particularly when we’re trying to regulate recommendation and so many are advocating providing better user-level controls as a check on platform power. Are people really going to go to the trouble of changing their settings if they don’t think they rely on algorithmic suggestions that much anyway?"
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#qualitative-research-sheds-light-on-the-context-of-data-annotation",
    "href": "posts/2022-06-01-qualitative/index.html#qualitative-research-sheds-light-on-the-context-of-data-annotation",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "Qualitative research sheds light on the context of data annotation",
    "text": "Qualitative research sheds light on the context of data annotation\nMachine learning systems rely on vast amounts of data. In many cases, for that data to be useful, it needs to be labelled/ annotated. For example, a hate speech classifier (an AI-enabled tool used to identify and flag potential cases of hate speech on a website) relies on huge datasets of text labelled as ‘hate speech’ or ‘not hate speech’ to ‘learn’ how to spot hate speech. But it turns out that who is doing the annotating and in what context they’re doing it, matters. AI-powered content moderation is often held up as the solution to harmful content online. What has continued to be underplayed is the extent to which those automated systems are and most likely will remain dependent on the manual work of human content moderators sifting through some of the worst and most traumatic online material to power the machine learning datasets on which automated content moderation depends. Emily Denton and her colleagues highlight the significance of annotators’ social identity (e.g., race, gender) and their expertise when it comes to annotation tasks, and they point out the risks associated with overlooking these factors and simply ‘aggregating’ results as ‘ground truth’ rather than properly exploring disagreements between annotators and the important insights that this kind of disagreement might offer.\nHuman commercial content moderators (such as the people that identify and remove violent and traumatic imagery on Facebook) often labour in terrible conditions, lacking psychological support or appropriate financial compensation. The interview-based research of Sarah T. Roberts has been pioneering in highlighting these conditions. Most demand for crowdsourced digital labour comes from the Global North, yet the majority of these workers are based in the Global South and receive low wages. Semi-structured interviews reveal the extent to which workers feel unable to bargain effectively for better pay in the current regulatory environment. As Mark Graham and his colleagues point out, these findings are hugely important in a context where several governments and supranational development organisations like the World Bank are holding up digital work as a promising tool to fight poverty.\nThe decision of how to measure ‘race’ in machine learning systems is highly consequential, especially in the context of existing efforts to evaluate these systems for their “fairness.” Alex Hanna, Emily Denton, Andrew Smart and Jamila Smith-Loud have done crucial work highlighting the limitation of machine learning systems that rely on official records of race or their proxies (e.g. census records), noting that the racial categories provided by such records are “unstable, contingent, and rooted in racial inequality.” The authors emphasise the importance of conducting research in ways that prioritise the perspectives of the marginalised racial communities that fairness metrics are supposed to protect. Qualitative research is ideally placed to contribute to a consideration of “race” in machine learning systems that is grounded in the lived experiences and needs of the racially subjugated."
  },
  {
    "objectID": "posts/2022-06-01-qualitative/index.html#what-next",
    "href": "posts/2022-06-01-qualitative/index.html#what-next",
    "title": "Qualitative humanities research is crucial to AI",
    "section": "What next?",
    "text": "What next?\nCollaborations between quantitative and qualitative researchers are valuable in understanding AI ethics from all angles.\nConsider reading more broadly, outside your particular area. Perhaps using the links and researchers listed here as starting points. They’re just a sliver of the wealth that’s out there. You could also check out the Social Media Collective’s Critical Algorithm Studies reading list, the reading list provided by the LSE Digital Ethnography Collective, and Catherine Yeo’s suggestions.\nStrike up conversations with researchers in other fields, and consider the possibility of collaborations. Find a researcher slightly outside your field but whose work you broadly understand and like, and follow them on Twitter. With any luck, they will share more of their work and help you identify other researchers to follow. Collaboration can be an incremental process: Consider inviting the researcher to form part of a discussion panel, reach out to say what you liked and appreciated about their work and why, and share your own work with them if you think it’s aligned with their interests.\nWithin your university or company, is there anything you could do to better reward or facilitate interdisciplinary work? As Humanities Computing Professor Willard McCarty notes, somewhat discouragingly, “professional reward for genuinely interdisciplinary research is rare.” To be sure, individual researchers and practitioners have to be prepared to put themselves out there, compromise and challenge themselves - but carefully tailored institutional incentives and enablers matter."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html",
    "href": "posts/2022-09-06-homeschooling/index.html",
    "title": "My family’s unlikely homeschooling journey",
    "section": "",
    "text": "My husband Jeremy and I never intended to homeschool, and yet we have now, unexpectedly, committed to homeschooling long-term. Prior to the pandemic, we both worked full-time in careers that we loved and found meaningful, and we sent our daughter to a full-day Montessori school. Although I struggled with significant health issues, I felt unbelievably lucky and fulfilled in both my family life and my professional life. The pandemic upended my careful balance. Every family is different, with different needs, circumstances, and constraints, and what works for one may not work for others. My intention here is primarily to share the journey of my own (very privileged) family."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#our-unplanned-introduction-to-homeschooling",
    "href": "posts/2022-09-06-homeschooling/index.html#our-unplanned-introduction-to-homeschooling",
    "title": "My family’s unlikely homeschooling journey",
    "section": "Our unplanned introduction to homeschooling",
    "text": "Our unplanned introduction to homeschooling\nFor the first year of the pandemic, most schools in California, where we lived at the time, were closed. Like countless other families, we were unexpectedly thrust into the world of virtual-school and home-school. We ended up participating in an innovative online program that did NOT try to replicate in-person school. A few key differences: - Each child could work at their own pace, largely through playing educational games and apps that adapted to where they were. There was no particular endpoint that the kids needed to get to at the end of the semester. - Group video calls were limited in size to no more than 6 kids (and often smaller), so kids got lots of personal interaction with their tutors and each other. Even as an adult, I find video calls larger than 6 people overwhelming. - Regular movement breaks, where the kids had jumping jack competitions, did Cosmic Kids yoga videos, held dance parties, and ran around the house for scavenger hunts. - Took advantage of existing materials: the program did not reinvent the wheel, but instead made use of excellent, existing online videos and educational apps.\nFrom August 2020 - March 2021, our daughter was with a small group online, where daily she would spend 1 hour on socio-emotional development (including games, getting to know each other, and discussing feelings), 1 hour on reading, and 1 hour on math. For reading and math, the children each worked at their own pace through engaging games, and could ask the teacher and each other questions whenever they needed help. At the end of these 8 months, our daughter, along with several other kids in her small group, were several years beyond their age levels in both math and reading. It had never been our goal for her to end up accelerated; Jeremy and I were mostly trying to keep her happy and busy for a few hours so we could get some of our own work done. She also had fun and made close friends, who she continues to have video calls and Minecraft virtual playdates with regularly."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#our-unconventional-views",
    "href": "posts/2022-09-06-homeschooling/index.html#our-unconventional-views",
    "title": "My family’s unlikely homeschooling journey",
    "section": "Our unconventional views",
    "text": "Our unconventional views\nAlthough there are plenty of ways to homeschool that don’t involve any screens or technology, Jeremy and I have made use of online tutors, long-distance friendships, educational apps, videos, and web-based games, as key parts of our approach. One thing that helped us going into the pandemic is that we have never treated online/long-distance relationships as inferior to in-person relationships. We both have meaningful friendships that occur primarily, or even entirely, through phone calls, video chats, texts, and online interactions. I have made several big moves since I graduated from high school (moving from Texas to Pennsylvania to North Carolina back to Pennsylvania again and then to California) and I was used to family and close friends being long distance. We live far from our families, and our daughter was already accustomed to chatting with her grandparents on both sides via video calls. My daughter’s best friend is now a child she has never met in person, but has been skyping with almost daily for the last 2 years.\nAnother thing that made this transition easier is that Jeremy and I have never been anti-screen time. In fact, we don’t consider “screen time” a useful category, since a child passively watching a movie alone is different than skyping with their grandparent is different than playing an educational game interactively with their parent beside them. While we almost never let our daughter do things passively and alone with screens, we enjoy relational and educational screen time. Furthermore, we focus on including other positive life elements (e.g. making sure she is getting enough time outside, being physically active, reading, getting enough sleep, etc) rather than emphasising limits.\n{% include image w=“600” url=“mathperson/venndiagram.jpg” caption=“A Venn Diagram showing how I think about screentime. We avoid the outside (white) region and mostly stick to the intersections.” %}"
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#a-return-to-in-person-school",
    "href": "posts/2022-09-06-homeschooling/index.html#a-return-to-in-person-school",
    "title": "My family’s unlikely homeschooling journey",
    "section": "A return to in-person school",
    "text": "A return to in-person school\nIn 2021, our family immigrated from the USA to my husband’s home country of Australia, and we enrolled our daughter at an in-person school, which she attended from late April - early Dec 2021. Our state had closed borders and almost no cases of covid transmission during this time. By all measures, the school she attended is great: friendly families, kind staff, and a fun performing arts program. While our daughter adjusted quickly to the new environment and made friends, she was quite bored. She lost her previous curiosity and enthusiasm, became more passive, and started to spend a lot of time zoning out. The school tried to accommodate her, letting her join an older grade level for math each day. While the material was initially new, she still found the pace too slow. She started to get very upset at home practising piano or playing chess (activities she previously loved, but where mistakes are inevitable), because she had grown accustomed to getting everything right without trying. At one point, all schools in our region closed during an 8-day snap lockdown. Our daughter was disappointed when the lockdown ended and she had to return to school."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#when-homeschooling-works-well-and-when-it-doesnt",
    "href": "posts/2022-09-06-homeschooling/index.html#when-homeschooling-works-well-and-when-it-doesnt",
    "title": "My family’s unlikely homeschooling journey",
    "section": "When homeschooling works well (and when it doesn’t)",
    "text": "When homeschooling works well (and when it doesn’t)\nOver the summer holidays (Dec 2021-Jan 2022), our state pivoted from zero covid to promoting mass infection as “necessary”. We pulled our daughter out of school, initially intending that it would just be a temporary measure until her age group could be fully vaccinated (vaccine rollout was later in Australia than in the USA). However, we immediately saw positive changes, with her regaining her old curiosity, enthusiasm, and proactive nature, all of which she had lost being in school. Her perfectionism disappeared and she began to enjoy challenges again. We supplemented her online classes with in-person playdates, extracurriculars, and sports (due to covid risks, we wear masks and stay outdoors for all of these). We are fortunate to live in a beautiful part of the world, where we can spend most of the year outside. We enjoy visiting the beaches, forests, and parks in our region. Our daughter is happy: playing Minecraft with friends online, learning tennis with other local children, riding bikes as a family, spending hours absorbed in books of her own choosing, enjoying piano and chess again, running around in nature, and learning at her own pace.\nHomeschooled kids typically score 15 to 30 percentile points above public-school students on standardised academic achievement tests, and 87% of studies on social development “showed clearly positive outcomes for the homeschooled compared to those in conventional schools”. However, it is understandable that many children had negative experiences with virtual learning in the past 2 years, given that programs were often hastily thrown together with inadequate resources and inappropriately structured to try to mimic in-person school, against the stressful backdrop of a global pandemic. Many parents faced the impossible task of simultaneously needing to work full-time and help their children full-time (and many other parents did not even have the option to stay home). Every family is different, and virtual learning or homeschooling will not suit everyone. There are children who need in-person services only offered within schools; parents whose work constraints don’t allow for it; and kids who thrive being with tons of other kids.\nDespite the difficulty of the pandemic, there are a variety of families who found that virtual or homeschooling was better for their particular kids. Some parents have shared about children with ADHD who found in-person school too distracting; children who were facing bullying or violence at school; kids who couldn’t get enough sleep on a traditional school schedule; Black and Latino families whose cultural heritages were not being reflected in curriculums. I enjoyed these article featuring a few such families: - What if Some Kids Are Better Off at Home? | The New York Times For parents like me, the pandemic has come with a revelation: For our children, school was torture. - They Rage-Quit the School System—and They’re Not Going Back | WIRED The pandemic created a new, more diverse, more connected crop of homeschoolers. They could help shape what learning looks like for everyone."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#covid-risks",
    "href": "posts/2022-09-06-homeschooling/index.html#covid-risks",
    "title": "My family’s unlikely homeschooling journey",
    "section": "Covid Risks",
    "text": "Covid Risks\nI have had brain surgery twice, was hospitalised in the ICU with a life-threatening brain infection, and have a number of chronic health issues. I am both at higher risk for negative outcomes from covid AND acutely aware of how losing your health can destroy your life. It is lonely and difficult being high-risk in a society that has given up on protecting others. While I am nervous about the long-term impact that homeschooling will have on my career (on top of how my existing health issues already hinder it), acquiring additional disabilities would be far, far worse.\nI have been disturbed to follow the ever-accumulating research on cardiovascular, neurological, and immune system harms that can be caused by covid, even in previously healthy people, even in the vaccinated, and even in children. While vaccines significantly reduce risk of death, unfortunately they provide only a limited reduction in Long Covid risk. Immunity wanes, and people face cumulative risks with each new covid infection (so even if you’ve had covid once or twice, it is best to try to avoid reinfections). I am alarmed that leaders are encouraging mass, repeated infections of a generation of children.\nGiven all this, I am relieved that our decision to continue homeschooling was relatively clear. It much better suits our daughter’s needs AND drastically reduces our family’s covid risk. We can nurture her innate curiosity, protect her intrinsic motivation, and provide in-person social options that are entirely outdoors and safer than being indoors all day at school. Most families are not so fortunate and many face difficult choices, with no good options."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#the-broader-picture",
    "href": "posts/2022-09-06-homeschooling/index.html#the-broader-picture",
    "title": "My family’s unlikely homeschooling journey",
    "section": "The Broader Picture",
    "text": "The Broader Picture\nI believe that high-quality, equitable, and safe public education is important for a healthy democracy, and I worry about the various ongoing ways in which education is being undermined and attacked. Furthermore, due to a lack of covid protections in communities, high-risk children and children with high-risk families are being shut out of in-person school options in the USA, Australia, and many other places. While the workplaces of politicians and a handful of schools in ultra-wealthy areas installed expensive ventilation upgrades, the majority of schools in the USA and Australia have not had any ventilation upgrades, nor received air purifiers. All children deserve access to an education that is safe, fits their needs, and will allow them to thrive. Even when homeschooling does work, it is often still just an individual solution to systemic problems."
  },
  {
    "objectID": "posts/2022-09-06-homeschooling/index.html#related-posts",
    "href": "posts/2022-09-06-homeschooling/index.html#related-posts",
    "title": "My family’s unlikely homeschooling journey",
    "section": "Related Posts",
    "text": "Related Posts\nA few other posts that you may be interested in, related to my views on education and teaching: - There’s No Such Thing as Not A Math Person: based on a webinar I gave to parents addressing cultural myths about math and how to support your kids in their math education, even if you don’t see yourself as a “math person” - The Qualities of a Good Education: common pitfalls in traditional technical education and ideas towards doing better - What You Need to Know Before Considering a PhD: includes a reflection on how my own success in traditional academic environments was actually a weakness, because I’d learned how to solve problems I was given, but not how to how to find and scope interesting problems on my own"
  },
  {
    "objectID": "posts/2023-02-07-school-immunology/index.html",
    "href": "posts/2023-02-07-school-immunology/index.html",
    "title": "I was an AI researcher. Now, I am an immunology student.",
    "section": "",
    "text": "I like complexity, and I like challenges. When a new topic fascinates me, I want to devote all of my time to it. In 2012, I was working as a quant in energy trading when I became so captivated by the topic of machine learning that I abruptly moved cross-country to San Francisco and spent a decade learning as much as I could about machine learning, AI, data ethics, and algorithmic harms. In 2022, I became fascinated by a new topic: immunology. I completed 7 online courses last year, am currently taking 4 more courses, and have created over 2,000 immunology-related flashcards for myself, which I spend time on daily. (I will write more about how I use Anki flashcards in a future post.)\nI found immunology to be both overwhelming and fascinating. The field is full of jargon, and there is a steep curve just to learn the language: IL-2, IL-4, IL-5, IL-12, IL-13, IL-18, CD-3, CD-22, CD-34, CD-47, CD-155, C3, C5, C8, and so on (lots of letter/number combinations, abbreviations, and acronyms! But underneath them, the mechanisms are fascinating). The more I studied, the more I wanted to learn. It became clear that I needed a formal program to go deeper and to provide appropriate context, so I started applying to graduate school."
  },
  {
    "objectID": "posts/2023-02-07-school-immunology/index.html#back-to-school",
    "href": "posts/2023-02-07-school-immunology/index.html#back-to-school",
    "title": "I was an AI researcher. Now, I am an immunology student.",
    "section": "Back to School",
    "text": "Back to School\nI was delighted to be accepted to a Masters in Immunology graduate program, and after eager anticipation, last month I officially began my formal degree. While my ultimate goal is to apply my machine learning and data ethics expertise to the field, I want to make sure I fully understand the relevant immunology first. Too often machine learning practitioners unthinkingly grasp for a nail to use their hammer on, without first having the necessary in-depth knowledge of the underlying domain, its data, its context, and its actual challenges.\nThe more I learn about immunology, the more I realise how complex, vast, and full of open questions and not-yet-understood phenomena the field is. It was only in 2021 that researchers proved Epstein-Barr virus causes multiple sclerosis. Researchers are making new discoveries about links between viral infections and neurodegenerative diseases, such as Alzheimer’s. A study in late 2022 found a possible mechanism to explain the fact that varicella zoster virus significantly increases risk of stroke. Unusually severe outbreaks of RSV and Group A Strep (a bacteria that can often follow as a secondary infection after a virus) made headlines in the past few months. A variety of viruses have long been known to sometimes trigger autoimmune diseases or cancer, yet there is still much to discover about these relationships."
  },
  {
    "objectID": "posts/2023-02-07-school-immunology/index.html#living-in-the-pandemicene",
    "href": "posts/2023-02-07-school-immunology/index.html#living-in-the-pandemicene",
    "title": "I was an AI researcher. Now, I am an immunology student.",
    "section": "Living in the Pandemicene",
    "text": "Living in the Pandemicene\nEven as the ongoing covid pandemic continues to cause death and disability, science journalist Ed Yong warned that we are now living in the pandemicene, a period with increasingly likely pandemics. Climate change is crowding species into new habitats, raising the risks of viral spillover from the estimated 40,000 viruses that inhabit mammals. Immunology, virology, and microbiology will become even more important in the coming decades.\n\n\n\nEd Yong’s article in The Atlantic"
  },
  {
    "objectID": "posts/2023-02-07-school-immunology/index.html#mathematical-biology-and-ai-in-medicine",
    "href": "posts/2023-02-07-school-immunology/index.html#mathematical-biology-and-ai-in-medicine",
    "title": "I was an AI researcher. Now, I am an immunology student.",
    "section": "Mathematical Biology and AI in Medicine",
    "text": "Mathematical Biology and AI in Medicine\nFor over 20 years, my focus has been on mathematics, computer science, and data ethics. I studied mathematics, computer science, and linguistics as an undergraduate; earned a PhD in mathematics; and then spent 12 years working in a mix of industry and academia as a data scientist, teacher, and researcher. I am best known for my work as cofounder of fast.ai, creator of the most popular deep learning courses in the world, and for previously serving as the founding director of the University of San Francisco Center for Applied Data Ethics. Over the years, I have had a recurring interest in medicine, doing mathematical modelling of cell processes as part of a Howard Hughes Medical Institute fellowship while I was in graduate school, publishing about machine learning in medicine for The Boston Review, and being an invited keynote speaker for Stanford’s AI in Medicine symposium."
  },
  {
    "objectID": "posts/2023-02-07-school-immunology/index.html#the-value-of-domain-expertise",
    "href": "posts/2023-02-07-school-immunology/index.html#the-value-of-domain-expertise",
    "title": "I was an AI researcher. Now, I am an immunology student.",
    "section": "The value of domain expertise",
    "text": "The value of domain expertise\nCore to the mission of fast.ai is the idea that domain expertise is crucial. In our very first post announcing the launch of fast.ai in 2016, my cofounder Jeremy Howard wrote, “Only domain experts: fully understand and appreciate what are the most important problems in their field; have access to the data necessary to solve those problems; and understand the opportunities and constraints to implementing data driven solutions.” It is dangerous for machine learning practitioners to apply machine learning to fields in which they have only superficial knowledge (unless working closely with domain experts from end-to-end). Collaborative, interdisciplinary, and career changing work has always fascinated me. I have written about how necessary qualitative humanities research is to the field of AI. I previously taught software engineering to adult women changing careers, and long-believed that career changers have something special to offer. I am now taking my own advice, and delving into immunology, with the long-term goal of integrating this new knowledge with my data ethics and machine learning skills.\n\n\n\nExcited to be back in school\n\n\nAfter having been in a more “established” place in my career for a while, it is intimidating to publicly start off on a new branch like this. However, it’s also exciting, and I hope to share some of my journey as an immunology student along the way through blog posts and essays, just as I’ve always encouraged fast.ai students to do."
  },
  {
    "objectID": "posts/2023-02-21-anki/index.html",
    "href": "posts/2023-02-21-anki/index.html",
    "title": "How to Remember Anything",
    "section": "",
    "text": "I have been using Anki flashcards to help me study. Anki is a free, open-source system based on the research of spaced repetition learning. I have been using this technique to study immunology, but it can be used for any subject. You make your own cards and they can include anything you like. Anki is an app available on Android, iPhone, and desktop, and it is easy to sync between your phone and computer (it is free on Android and desktop, although costs money for the iPhone). I want to explain the basics of the approach. This post is intended for people new to Anki– I will not cover power user features.\n\n\n\nThe front and back of one of my flashcards (the graphs are screenshots I took from one of my professor’s lectures)\n\n\nOver 6 months ago, I began studying immunology every day. Every immunology professor I’ve watched lectures from has said that learning immunology is like learning a foreign language and that the amount of specialised jargon is intense. At first, I pulled out a notebook to write down what I wanted to remember. However, I soon decided to try out Anki flashcards instead."
  },
  {
    "objectID": "posts/2023-02-21-anki/index.html#spaced-repetition-learning",
    "href": "posts/2023-02-21-anki/index.html#spaced-repetition-learning",
    "title": "How to Remember Anything",
    "section": "Spaced Repetition Learning",
    "text": "Spaced Repetition Learning\nIn order to retain information, research shows that the best time to be quizzed about something you are learning is right before you otherwise would have forgotten it. For new information, this will be relatively soon. However, as you study a piece of information more times, you can go longer and longer before you need to be prompted about it. The other key factor in when you need to see a card again is how difficult a question is for you. If you find it easy, you can wait longer before you need to see it again, but if it’s hard, you should review it sooner. And if you got it wrong or didn’t know the answer, you should study it again in the same session. Anki handles the spacing of cards for you, keeping track of when you need to see each card again.\n\n\n\nMy practice using Anki for the last 28 weeks (darker blue means more cards studied)\n\n\nYou create the cards. You can write with text, include images, and/or use a fill-in-the-blank format (called cloze). I usually make flashcards as I watch video lectures, pausing the video to screenshot images I want to use or to transcribe key points I want to remember. I make cards on my laptop as I watch videos and then study on my phone."
  },
  {
    "objectID": "posts/2023-02-21-anki/index.html#again-hard-good-easy",
    "href": "posts/2023-02-21-anki/index.html#again-hard-good-easy",
    "title": "How to Remember Anything",
    "section": "Again, Hard, Good, Easy",
    "text": "Again, Hard, Good, Easy\nOn the back of each card, at the bottom, Anki offers you 4 choices to rate whether you need to see the card again (choose this if you got it wrong) or whether you found it hard, good, or easy. For a new card, all the choices will lead to reviewing again soon, with “easy” corresponding to 4 days. However, over time, the choices for all options get longer as you answer the card correctly more times (except for “again”, which indicates you forgot and need to restart that card). This shows the options Anki gives on 3 different cards: a brand new one, one I’ve seen a few times, and one I’ve gotten correct many times (and likely won’t need to see again for many months).\n\n\n\nThe lengths of time vary based on your history with the card\n\n\nIf you get a card wrong/need to see it again, Anki will reset to showing you the card with higher frequency again. Here is an old card that I missed. After I select “Again” (because I forgot an answer), these were the options the next time I see that card:\n\n\n\nResetting"
  },
  {
    "objectID": "posts/2023-02-21-anki/index.html#sustainable-over-time",
    "href": "posts/2023-02-21-anki/index.html#sustainable-over-time",
    "title": "How to Remember Anything",
    "section": "Sustainable over time",
    "text": "Sustainable over time\nSince you will see older cards less and less often (assuming you are getting them mostly correct), the number of Anki cards you need to do each day will decline with time if you don’t add more cards. This makes Anki feel very sustainable to pick up, and I have found a rhythm around how many new cards I can add each day, while keeping the time I spend on Anki relatively constant. It is reassuring to know that as long as I keep up on my daily cards, I will be able to retain the information I am learning.\nFor instance, I can look at my stats on Anki and see that today I did 114 cards, but for each day going forward I will have less and less (this graph doesn’t take into account that I plan to continue adding new cards each day).\n\n\n\nHow many cards I will have each day going forward if I stop adding new cards"
  },
  {
    "objectID": "posts/2023-02-21-anki/index.html#for-all-ages-and-all-subjects",
    "href": "posts/2023-02-21-anki/index.html#for-all-ages-and-all-subjects",
    "title": "How to Remember Anything",
    "section": "For all ages and all subjects",
    "text": "For all ages and all subjects\nMy daughter is currently taking Biochemistry Literacy for Kids, a biochem course aimed at elementary aged children. The teacher suggested that the kids should memorise the 20 basic amino acids. While my daughter was initially unsure if she’d be able to do it, she had fun making up silly stories about each one (based on their structure and name), only spends a few minutes on Anki each day, and now has a strong sense of pride that she knows the amino acids (and her parents don’t).\nMy partner Jeremy used Anki to study Chinese characters. After 6 months of study, when he went to Beijing to attend an in-person Chinese school, he tested into a class where the other students had years of practice."
  },
  {
    "objectID": "posts/2023-02-21-anki/index.html#combining-with-other-techniques",
    "href": "posts/2023-02-21-anki/index.html#combining-with-other-techniques",
    "title": "How to Remember Anything",
    "section": "Combining with other techniques",
    "text": "Combining with other techniques\nAnki can (and should) be combined with other memory techniques, such as creating short stories that are funny, surprising, sexy, violent, unexpected, or related to family, friends, and celebrities (all these are attributes proven to make things more memorable)."
  },
  {
    "objectID": "posts/2023-02-21-anki/index.html#other-resources-on-effective-learning-and-memory",
    "href": "posts/2023-02-21-anki/index.html#other-resources-on-effective-learning-and-memory",
    "title": "How to Remember Anything",
    "section": "Other Resources on Effective Learning and Memory",
    "text": "Other Resources on Effective Learning and Memory\nMany people incorrectly believe that memory is purely a fixed talent that you either have or you don’t, but there are a number of techniques proven to help make both memory and learning more effective. For more on how to learn effectively, here are several books, videos, and courses:\n\nMoonwalking with Einstein: The Art and Science of Remembering Everything, by Joshua Foer\nRemember, Remember, by world memory champion Ed Cooke\nA Mind For Numbers: How to Excel at Math and Science (Even If You Flunked Algebra), by Barbara Oakley\nJeremy Howard’s talk on techniques for learning Chinese\nCoursera class on Learning How to Learn, taught by Barbara Oakley. This is one of the most popular Coursera courses of all time.\nThere’s no such thing as not a math person: my post about learning math and myths of innate ability"
  },
  {
    "objectID": "posts/2023-03-07-viruses1/index.html",
    "href": "posts/2023-03-07-viruses1/index.html",
    "title": "Viruses are weirder, worse, & more preventable than you realise",
    "section": "",
    "text": "When I was 6, I caught chickenpox (this was before the chickenpox vaccine was widely available). I found the blisters quite itchy and painful, and I insisted that my parents take a picture of me so I could have a record of how many blisters I had. I stayed home from school for 2 weeks and was quite disappointed to miss being in the Amelia Bedelia school play (Amelia Bedelia was the protagonist of my favourite book series at the time, about a maid who takes everything literally). The only upside was getting to watch a VHS tape of the Pippi Longstocking movie on repeat. Once I recovered, I thought of chickenpox as annoying and unpleasant, but not a big deal in the scheme of things."
  },
  {
    "objectID": "posts/2023-03-07-viruses1/index.html#long-term-impacts-of-the-virus-behind-chickenpox",
    "href": "posts/2023-03-07-viruses1/index.html#long-term-impacts-of-the-virus-behind-chickenpox",
    "title": "Viruses are weirder, worse, & more preventable than you realise",
    "section": "Long-term impacts of the virus behind chickenpox",
    "text": "Long-term impacts of the virus behind chickenpox\nOnly much later did I learn that chickenpox is caused by the varicella zoster virus (VZV), which remains latent in our neurons after infection, and for 30% of people, unless vaccinated, will reactivate as shingles decades later. People who have shingles are at an 80% higher risk of stroke for up to a year afterwards, long after the acute symptoms of shingles have gone away. The stroke risk is particularly high for people who get shingles under the age of 40 (and this younger group is not eligible for the shingles vaccine).\nThe mechanisms by which VZV leads to this increased stroke risk are still being actively researched. In 2022, researchers discovered that VZV causes the formation of small sacs full of proteins which promote blood clots. (These small sacs are called exosomes and are used to transport various materials through our bloodstream.) These blood-clot-encouraging exosomes are still being produced 3 months after a shingles infection, and likely for longer (the study ended at 3 months).\n\n\n\nFrom an article covering the research on Varicella Zoster Virus, strokes, and exosomes\n\n\nVZV is not just linked to strokes, but also linked to developing multiple sclerosis or vascular dementia, as was found in a recent study analysing hundreds of thousands of medical records for associations between common viruses and diseases impacting the brain and nervous system. In the case of VZV, it is possible that the links to dementia and MS may occur through the reactivation of other viruses. That is another disturbing fact about viruses: they can quietly lie latent for years and then be reactivated by other viruses."
  },
  {
    "objectID": "posts/2023-03-07-viruses1/index.html#but-arent-infections-inevitable",
    "href": "posts/2023-03-07-viruses1/index.html#but-arent-infections-inevitable",
    "title": "Viruses are weirder, worse, & more preventable than you realise",
    "section": "But aren’t infections inevitable?",
    "text": "But aren’t infections inevitable?\nI never gave much thought to how my brother and I had caught chicken pox, because for most of my life I assumed that viruses were a temporary discomfort, and that they were generally inevitable, just something that happens. However, in recent years, medical understanding of how pathogens spread has been upended, which in turn has profound implications for what we can do in response.\nThe flu, long believed to be spread through droplets from sneezes or through touching contaminated surfaces, has been shown to be released in small aerosols exhaled through ordinary breathing. These aerosols can remain suspended in the air, and others risk infection when they breathe them in. In work published in the prestigious PNAS, researchers found that neither sneezing nor coughing was necessary to spread flu, just ordinary breathing. This shift in understanding is important: it means that we have been relying on insufficient tools to stop viruses (e.g. hand washing and poorly fitting surgical masks alone will never be enough against airborne flu). Chickenpox, too, is spread through the air. As Professor Jonathan Gershoni of Tel Aviv University says in his course on viruses, “The most common direct route [of viral transmission] is when viruses become airborne and are subsequently inhaled. When people cough, sneeze, or simply exhale, they produce an aerosol of minute fluid droplets that are dispersed in the air around them.”\n\n\n\nFrom a course on viruses taught by Dr. Jonathan Gershoni\n\n\nFortunately, there are ways we can clean the air with improved ventilation (drawing in more outdoor air), filtration (removing harmful particles), and well-fitting masks in crowded public indoor spaces. For years, excellent ventilation and filtration systems have been used in pig farms, to avoid the spread of airborne porcine diseases, with some farms even employing a worker whose sole duty is the daily inspection and maintenance of the ventilation/filtration systems. Numerous studies document the effectiveness of hog farm filtration systems at reducing illness (see this fascinating thread for links), as well as the harms of poor ventilation for humans. Methods to clean the air have been implemented by wealthy elites at places including the World Economic Forum’s Davos Summit (attended by politicians and billionaires), the UK Parliament, Westminster Palace, and an Australian state parliament. However, unlike politicians and pigs, very few children or working class people have access to clean air in their schools or workplaces.\nRice University biochemistry and cell biology professor Alma Moon Novotny opened her series of in-depth immunology courses by singing the praises of not immunologists nor virologists, but of… engineers! Prof Novotny said, “I want to emphasize that an ounce of prevention is worth a pound of cure, and I will be praising engineers, particularly civil engineers because what they provide for us is clean water and proper sewage disposal.” She suggested that sanitation is the greatest medical advance of the last 150 years and went on to list other important engineering improvements that improve health: mosquito nets, houses that don’t let in as many insects (which can often carry pathogens), and an infrastructure system to transport needed medical supplies. She also spoke of how expensive our immune systems are in terms of ATP (our bodies’ energy currency). Despite the amazing intricacies of the human immune system, it is better to avoid getting sick.\n\n\n\nHeadline from The Guardian and a lecture slide from Rice University Prof Alma Novotny\n\n\nExperts have been highlighting the extensive infrastructure overhaul London used to tackle cholera in the 19th century, similar to how cleaning indoor air could be part of a “plan to stop every respiratory virus at once”. Clean air could be the next clean water, revolutionising global health."
  },
  {
    "objectID": "posts/2023-03-07-viruses1/index.html#a-new-way-of-thinking",
    "href": "posts/2023-03-07-viruses1/index.html#a-new-way-of-thinking",
    "title": "Viruses are weirder, worse, & more preventable than you realise",
    "section": "A New Way of Thinking",
    "text": "A New Way of Thinking\nThe idea that a common childhood virus can quietly hang out unnoticed in our nervous system for decades, reactivate to cause the blisters and nerve pain of shingles, and then months after the shingles blisters clear up cause blood clotting and strokes is mind-boggling to me, and stands in stark contrast to how I thought about viruses when I was younger.\nThe trajectory of my growing understanding of VZV is similar to my understanding of many viruses: I have moved from thinking of common viruses as both inevitable and discretely contained events (e.g. you are sick for a week or two, but then it’s over) to recognizing that:\n\neven common viruses can have long-reaching, surprising, and devastating consequences\nthere is much more we can do to stop viral transmission than I previously realised\n\nVZV is just one of many viruses that have surprised me. Research has now shown that several common viruses raise the risk of dementia and Alzheimer’s disease, and the virus behind mono / glandular fever (Epstein-Barr virus) causes Multiple Sclerosis. The fact that it is possible to reduce viral transmission makes it all the more urgent to understand the long-reaching consequences of viruses, since we have the power to act on what we learn.\nEach risk on its own may not be that likely on the individual level: I am not suggesting that you should feel alarmed that you in particular will suffer a stroke after shingles, or that your elderly parent will catch the flu and then develop dementia, or that mono/glandular fever will lead to multiple sclerosis for you. However, it is important to think about this across the population, and across all these different virus-disease interactions. When you look at this research in aggregate, it suggests that many, many people are having their lives drastically impacted by diseases in which viruses play a role. Given what we know about both the risks of viruses and ways to reduce transmission, it is worthwhile to take effective measures like increasing ventilation, using air purifiers, and wearing N95/KF94/P2 masks in indoor public places to make our schools, workplaces, and healthcare settings safer.\nThis post is part 1 in a 3-part series. Please stay tuned for future posts!"
  },
  {
    "objectID": "posts/2023-03-22-viruses2/index.html",
    "href": "posts/2023-03-22-viruses2/index.html",
    "title": "Viruses: The Silent Triggers of Autoimmune and Neurodegenerative Diseases",
    "section": "",
    "text": "This is Part 2 in a series. Read Part 1 here.\nYour previously healthy 1-year old develops Type 1 diabetes, a condition they will have for the rest of their life and that will require constant monitoring. Your partner has a stroke. Your parent shows early symptoms of Alzheimer’s disease. What do these seemingly disparate health events have in common? All can be triggered by viruses where the immediate infection seems mild."
  },
  {
    "objectID": "posts/2023-03-22-viruses2/index.html#a-life-changing-diagnosis",
    "href": "posts/2023-03-22-viruses2/index.html#a-life-changing-diagnosis",
    "title": "Viruses: The Silent Triggers of Autoimmune and Neurodegenerative Diseases",
    "section": "A Life-Changing Diagnosis",
    "text": "A Life-Changing Diagnosis\nWhen Maria’s 1-year old daughter came down with a cold, it initially seemed like no big deal. However, her cough lingered, and in the coming months she was diagnosed with type 1 diabetes, a condition which can not be cured and which her daughter will have for the rest of her life. Maria was thrust into a stressful world of needing to constantly monitor her baby’s blood sugar, where either too much or too little insulin could kill her baby.\nIn the wake of her cold, the baby’s immune system had mistakenly turned on her pancreas, permanently destroying the cells needed to generate insulin. Type 1 diabetes is not related to diet or lifestyle, and is far less predictable than the more famous type 2 diabetes. As Maria writes, “We can do the same thing every day… and all our efforts simply do not work. It’s a never-ending battle. Diabetes never takes a break and needs to be managed 24/7/365 days a year… If we give too much insulin, she can go into a coma, and if we do not give her sufficient insulin, she can go into DKA [a life-threatening condition].”\n\n\n\n\nIn Type 1 Diabetes, the pancreas is incapable of producing sufficient insulin. Image: Scientific Animations, Creative Commons License\n\n\nType 1 diabetes is often triggered by a viral infection. One study tracked viruses in stool samples from young children, month after month, and discovered that when kids have certain common childhood viruses for consecutive months, they are at much higher risk of having their immune systems develop autoimmunity against their own pancreas (a precursor to Type 1 diabetes)."
  },
  {
    "objectID": "posts/2023-03-22-viruses2/index.html#what-are-autoimmune-diseases-and-how-do-they-relate-to-viruses",
    "href": "posts/2023-03-22-viruses2/index.html#what-are-autoimmune-diseases-and-how-do-they-relate-to-viruses",
    "title": "Viruses: The Silent Triggers of Autoimmune and Neurodegenerative Diseases",
    "section": "What are autoimmune diseases, and how do they relate to viruses?",
    "text": "What are autoimmune diseases, and how do they relate to viruses?\nType 1 diabetes is just one of many, many diseases caused by our own immune systems gone awry. These are known as autoimmune diseases. The Greek prefix auto means self. A key role of the immune system is to be able to distinguish between what is foreign and what is self, and it is harmful when the immune system erroneously learns to attack self. At its best, the immune system is a beautiful and complex orchestra working together for our good. However, the immune system is a double-edged sword that can cause great harm when it goes awry.\nDespite impacting a wide range of body systems, all of the following examples are united in being autoimmune diseases:\n\nCrohn’s disease occurs when the immune system attacks the colon\nRheumatoid arthritis occurs when the immune system attacks its own antibodies\nMultiple sclerosis occurs when the immune system attacks the protective coverings of neurons\nLupus occurs when the immune system attacks our own DNA\nHashimoto’s disease occurs when the immune system attacks the thyroid\nPsoriasis occurs when the immune system attacks skin cells\n\n\n\n\n\nAutoimmune disease can impact the whole body. Image: Glover, Mishra and Singh; Creative Commons License\n\n\nThere is still much that is unknown about the onset of autoimmune diseases, although in many cases, a viral infection can be the trigger (there are likely other factors involved too, including genetic predisposition). During an infection, our adaptive immune systems learn to recognize small patterns of molecules that indicate a pathogen or infected cell. Unfortunately, if these small patterns appear similar to elements of our own bodies, the body may continue attacking itself even once the acute infection is over."
  },
  {
    "objectID": "posts/2023-03-22-viruses2/index.html#obstacles-in-research-and-treatment",
    "href": "posts/2023-03-22-viruses2/index.html#obstacles-in-research-and-treatment",
    "title": "Viruses: The Silent Triggers of Autoimmune and Neurodegenerative Diseases",
    "section": "Obstacles in research and treatment",
    "text": "Obstacles in research and treatment\nWomen are more likely to develop autoimmune diseases, and there are numerous differences in women’s immune systems compared to men. Like most diseases disproportionately impacting women or people of colour, autoimmune diseases have too often been neglected when it comes to funding and research, leaving a rudimentary understanding, inadequate treatments, and lots of open questions.\nMoreover, autoimmune diseases have often been treated in different silos, based on which body system they impact. Crohn’s disease is studied by gastroenterologists, psoriasis by dermatologists, and Hashimoto’s by endocrinologists. While there are reasons for this approach, it can also hinder research on deeper patterns amongst autoimmune disorders. Once someone develops one autoimmune disease, they are significantly more likely to develop additional ones, which is another reason why studying common underlying causes and mechanisms would be helpful.\nAutoimmune disorders are often treated with medications that suppress the immune system to keep it from attacking the patient’s own body. While these medications can be crucial in addressing symptoms, they come with the significant downside of inhibiting how the immune system responds to foreign invaders, making the patient vulnerable to infection."
  },
  {
    "objectID": "posts/2023-03-22-viruses2/index.html#viruses-can-cause-neurodegenerative-diseases-too",
    "href": "posts/2023-03-22-viruses2/index.html#viruses-can-cause-neurodegenerative-diseases-too",
    "title": "Viruses: The Silent Triggers of Autoimmune and Neurodegenerative Diseases",
    "section": "Viruses can cause neurodegenerative diseases too",
    "text": "Viruses can cause neurodegenerative diseases too\nDiseases that cause cells in the brain or nervous system to lose function and eventually die are referred to as neurodegenerative diseases. Some are also autoimmune diseases. Multiple sclerosis is perhaps the best known disease that is firmly established as both neurodegenerative and autoimmune. In this disease, the immune system attacks the protective sheaths coating nervous cells in the spinal cord and brain. Evidence is growing that ALS (also known as Lou Gerig’s disease), Parkinson’s disease have autoimmune causes, and some researchers are asking if Alzheimer’s is an autoimmune disease too.\nCommon viruses increase the risk of developing a neurodegenerative disease, for up to 15 years after the viral infection. A study drawing on datasets of 800,000 people across Finland and the UK looked at how various viruses could increase likelihood of neurodegenerative diseases such as multiple sclerosis, ALS, Parkinson’s, Alzheimer’s and dementia, and found many significant links. For instance, six different groups of viruses (including flu and viral pneumonia) were linked to an increased risk of developing dementia. Viral encephalitis (a virus which in many cases causes only mild, flu-like symptoms) raised the risk of developing Alzheimer’s disease. These were just a few of the associations found.\n\n\n\nMRIs of a brain with Alzheimer’s, with dementia, and a normal control, from Wikimedia\n\n\nFor decades, research on Alzheimer’s Disease was dominated by a hypothesis that mostly turned out to be a dead-end. Funding and support was funnelled toward projects that failed (and in some cases, were based on fraudulent data), while other researchers (whose ideas are now being proven correct) faced hostility and extreme difficulty obtaining grants. For decades, the idea of viruses being a cause of Alzheimer’s was considered absurd, with only a few lone researchers pursuing it in the face of pushback and ostracism, yet now ever more research is accumulating to support this idea.\nA core idea in Alzheimer’s research has turned out to be backwards: what was considered to be an intrinsically abnormal and useless protein (amyloid-beta) causing Alzheimer’s has turned out to be a part of our innate immune response, intended to protect the brain against infection. Amyloid-beta may be inadvertently causing harm, but it likely builds up as a protective reaction against a more fundamental root cause. Amyloid-beta has positive, anti-microbial properties for fighting off invaders. A large study from Taiwan found that HSV-1 infections increased the risk of developing dementia, but that treating these infections with antiviral medication lowered that risk.\n\n\n\nAn article by Dr. Ruth Itzhaki, Oxford University’s Institute of Population Ageing"
  },
  {
    "objectID": "posts/2023-03-22-viruses2/index.html#science-is-political",
    "href": "posts/2023-03-22-viruses2/index.html#science-is-political",
    "title": "Viruses: The Silent Triggers of Autoimmune and Neurodegenerative Diseases",
    "section": "Science is political",
    "text": "Science is political\nThere have been many hurdles to pursuing the role of infection in contributing to Alzheimer’s: the siloed nature of science (virology and Alzheimer’s were seen as two very disparate areas); the failure of many to understand asymptomatic infection; and the political nature of medicine.\nScience does not just progress inevitably. Instead, politics, biases, and even trends all impact which ideas receive the opportunities, funding, and support for progress to be made. A reluctance over decades to adequately fund post-viral research, to explore links between viruses and neurodegenerative diseases, or to take autoimmune diseases (which disproportionately impact women) seriously, has left us knowing far less than we could. Treating autoimmune diseases all in separate silos has limited progress, as has treating neurodegenerative diseases siloed away from virology.\nIn a previous essay, I wrote of the political nature of medicine, including several key case studies:\n\nSerious political battles and protests were needed to get AIDS taken seriously.\nAn ongoing lack of funding for ME/CFS research, spanning decades, even though ME/CFS patients have the lowest quality of life of pretty much any illness. Commensurate to disease burden, NIH funding for ME/CFS is at only 7%, depression and asthma are at 100%, and diseases like cancer and HIV are closer to 1000%.\n\nOur understanding of the wide-ranging, surprising, and sometimes devastating impacts of viruses is growing everyday, with new scientific discoveries and connections being made. If we can accelerate support for such research, as well as work to reduce transmission of viruses, we could significantly improve human well-being.\nThis is Part 2 in a 3-part series. Read Part 1 here and stay tuned for Part 3."
  }
]