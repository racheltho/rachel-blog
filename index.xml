<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Rachel Thomas, PhD</title>
<link>https://rachel.fast.ai/index.html</link>
<atom:link href="https://rachel.fast.ai/index.xml" rel="self" type="application/rss+xml"/>
<description>a blog about science, data, ethics, &amp; education</description>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Mon, 05 Sep 2022 14:00:00 GMT</lastBuildDate>
<item>
  <title>My family’s unlikely homeschooling journey</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2022-09-06-homeschooling/index.html</link>
  <description><![CDATA[ 




<p>My husband Jeremy and I never intended to homeschool, and yet we have now, unexpectedly, committed to homeschooling long-term. Prior to the pandemic, we both worked full-time in careers that we loved and found meaningful, and we sent our daughter to a full-day Montessori school. Although I struggled with significant health issues, I felt unbelievably lucky and fulfilled in both my family life and my professional life. The pandemic upended my careful balance. Every family is different, with different needs, circumstances, and constraints, and what works for one may not work for others. My intention here is primarily to share the journey of my own (very privileged) family.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2022-09-06-homeschooling/homeschool-headlines.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">We were not alone in discovering that homeschooling worked better for our child</figcaption><p></p>
</figure>
</div>
<section id="our-unplanned-introduction-to-homeschooling" class="level2">
<h2 class="anchored" data-anchor-id="our-unplanned-introduction-to-homeschooling">Our unplanned introduction to homeschooling</h2>
<p>For the first year of the pandemic, most schools in California, where we lived at the time, were closed. Like countless other families, we were unexpectedly thrust into the world of virtual-school and home-school. We ended up participating in an <a href="https://www.modulo.app/">innovative online program</a> that did NOT try to replicate in-person school. A few key differences: - Each child could <strong>work at their own pace</strong>, largely through playing educational games and apps that adapted to where they were. There was no particular endpoint that the kids needed to get to at the end of the semester. - Group video calls were <strong>limited in size</strong> to no more than 6 kids (and often smaller), so kids got lots of personal interaction with their tutors and each other. Even as an adult, I find video calls larger than 6 people overwhelming. - <strong>Regular movement breaks</strong>, where the kids had jumping jack competitions, did <a href="https://www.youtube.com/c/CosmicKidsYoga">Cosmic Kids yoga videos</a>, held dance parties, and ran around the house for scavenger hunts. - <strong>Took advantage of existing materials</strong>: the program did not reinvent the wheel, but instead made use of excellent, existing online videos and educational apps.</p>
<p>From August 2020 - March 2021, our daughter was with a small group online, where daily she would spend 1 hour on socio-emotional development (including games, getting to know each other, and discussing feelings), 1 hour on reading, and 1 hour on math. For reading and math, the children each worked at their own pace through engaging games, and could ask the teacher and each other questions whenever they needed help. At the end of these 8 months, our daughter, along with several other kids in her small group, were several years beyond their age levels in both math and reading. It had never been our goal for her to end up accelerated; Jeremy and I were mostly trying to keep her happy and busy for a few hours so we could get some of our own work done. She also had fun and made close friends, who she continues to have video calls and Minecraft virtual playdates with regularly.</p>
</section>
<section id="our-unconventional-views" class="level2">
<h2 class="anchored" data-anchor-id="our-unconventional-views">Our unconventional views</h2>
<p>Although there are plenty of ways to homeschool that don’t involve any screens or technology, Jeremy and I have made use of online tutors, long-distance friendships, educational apps, videos, and web-based games, as key parts of our approach. One thing that helped us going into the pandemic is that we have never treated online/long-distance relationships as inferior to in-person relationships. We both have meaningful friendships that occur primarily, or even entirely, through phone calls, video chats, texts, and online interactions. I have made several big moves since I graduated from high school (moving from Texas to Pennsylvania to North Carolina back to Pennsylvania again and then to California) and I was used to family and close friends being long distance. We live far from our families, and our daughter was already accustomed to chatting with her grandparents on both sides via video calls. My daughter’s best friend is now a child she has never met in person, but has been skyping with almost daily for the last 2 years.</p>
<p>Another thing that made this transition easier is that Jeremy and I have never been anti-screen time. In fact, we don’t consider “screen time” a useful category, since a child passively watching a movie alone is different than skyping with their grandparent is different than playing an educational game interactively with their parent beside them. While we almost never let our daughter do things passively and alone with screens, we enjoy relational and educational screen time. Furthermore, we focus on including other positive life elements (e.g.&nbsp;making sure she is getting enough time outside, being physically active, reading, getting enough sleep, etc) rather than emphasising limits.</p>
<p>{% include image w=“600” url=“mathperson/venndiagram.jpg” caption=“A Venn Diagram showing how I think about screentime. We avoid the outside (white) region and mostly stick to the intersections.” %}</p>
</section>
<section id="a-return-to-in-person-school" class="level2">
<h2 class="anchored" data-anchor-id="a-return-to-in-person-school">A return to in-person school</h2>
<p>In 2021, our family immigrated from the USA to my husband’s home country of Australia, and we enrolled our daughter at an in-person school, which she attended from late April - early Dec 2021. Our state had closed borders and almost no cases of covid transmission during this time. By all measures, the school she attended is great: friendly families, kind staff, and a fun performing arts program. While our daughter adjusted quickly to the new environment and made friends, she was quite bored. She lost her previous curiosity and enthusiasm, became more passive, and started to spend a lot of time zoning out. The school tried to accommodate her, letting her join an older grade level for math each day. While the material was initially new, she still found the pace too slow. She started to get very upset at home practising piano or playing chess (activities she previously loved, but where mistakes are inevitable), because she had grown accustomed to getting everything right without trying. At one point, all schools in our region closed <a href="https://www.nytimes.com/live/2021/07/31/world/covid-delta-variant-vaccine/brisbane-australias-third-largest-city-joins-sydney-in-lockdown">during an 8-day snap lockdown.</a> Our daughter was disappointed when the lockdown ended and she had to return to school.</p>
</section>
<section id="when-homeschooling-works-well-and-when-it-doesnt" class="level2">
<h2 class="anchored" data-anchor-id="when-homeschooling-works-well-and-when-it-doesnt">When homeschooling works well (and when it doesn’t)</h2>
<p>Over the summer holidays (Dec 2021-Jan 2022), our state pivoted from zero covid to <a href="https://medium.com/@ColinKinner/lethal-stupidity-how-the-palaszczuk-governments-covid-response-has-thrown-queenslanders-under-52a127d67724">promoting mass infection as “necessary”</a>. We pulled our daughter out of school, initially intending that it would just be a temporary measure until her age group could be fully vaccinated (vaccine rollout was later in Australia than in the USA). However, we immediately saw positive changes, with her regaining her old curiosity, enthusiasm, and proactive nature, all of which she had lost being in school. Her perfectionism disappeared and she began to enjoy challenges again. We supplemented her online classes with in-person playdates, extracurriculars, and sports (due to covid risks, we wear masks and stay outdoors for all of these). We are fortunate to live in a beautiful part of the world, where we can spend most of the year outside. We enjoy visiting the beaches, forests, and parks in our region. Our daughter is happy: playing Minecraft with friends online, learning tennis with other local children, riding bikes as a family, spending hours absorbed in books of her own choosing, enjoying piano and chess again, running around in nature, and learning at her own pace.</p>
<p>Homeschooled kids typically score <a href="https://www.nheri.org/research-facts-on-homeschooling/">15 to 30 percentile points</a> above public-school students on standardised academic achievement tests, and <a href="https://www.nheri.org/research-facts-on-homeschooling/">87% of studies on social development</a> “showed clearly positive outcomes for the homeschooled compared to those in conventional schools”. However, it is understandable that many children had negative experiences with virtual learning in the past 2 years, given that programs were often hastily thrown together with inadequate resources and inappropriately structured to try to mimic in-person school, against the stressful backdrop of a global pandemic. Many parents faced the impossible task of simultaneously needing to work full-time and help their children full-time (and many other parents did not even have the option to stay home). Every family is different, and virtual learning or homeschooling will not suit everyone. There are children who need in-person services only offered within schools; parents whose work constraints don’t allow for it; and kids who thrive being with tons of other kids.</p>
<p>Despite the difficulty of the pandemic, there are a variety of families who found that virtual or homeschooling was better for their particular kids. Some parents have shared about children with <a href="https://www.nytimes.com/2020/08/10/opinion/coronavirus-school-closures.html">ADHD who found in-person school too distracting; children who were facing bullying or violence at school; kids who couldn’t get enough sleep on a traditional school schedule</a>; <a href="https://www.wired.com/story/pandemic-homeschoolers-who-are-not-going-back/">Black and Latino families whose cultural heritages were not being reflected in curriculums</a>. I enjoyed these article featuring a few such families: - <a href="https://www.nytimes.com/2020/08/10/opinion/coronavirus-school-closures.html">What if Some Kids Are Better Off at Home? | The New York Times</a> <em>For parents like me, the pandemic has come with a revelation: For our children, school was torture.</em> - <a href="https://www.wired.com/story/pandemic-homeschoolers-who-are-not-going-back/">They Rage-Quit the School System—and They’re Not Going Back | WIRED</a> <em>The pandemic created a new, more diverse, more connected crop of homeschoolers. They could help shape what learning looks like for everyone.</em></p>
</section>
<section id="covid-risks" class="level2">
<h2 class="anchored" data-anchor-id="covid-risks">Covid Risks</h2>
<p>I have had brain surgery twice, was hospitalised in the ICU with a life-threatening brain infection, and have a number of chronic health issues. I am both at higher risk for negative outcomes from covid AND acutely aware of how losing your health can destroy your life. It is lonely and difficult being high-risk in a society that has given up on protecting others. While I am nervous about the long-term impact that homeschooling will have on my career (on top of how <a href="https://medium.com/@racheltho/techs-long-hours-are-discriminatory-and-counterproductive-3676e649a601">my existing health issues already hinder it</a>), acquiring additional disabilities would be far, far worse.</p>
<p>I have been disturbed to follow the ever-accumulating research on <a href="https://www.nature.com/articles/d41586-022-00403-0">cardiovascular</a>, <a href="https://www1.racgp.org.au/newsgp/clinical/research-suggests-covid-affects-brain-even-in-mild">neurological</a>, and <a href="https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-021-02228-6">immune system</a> harms that can be caused by covid, even in <a href="https://missoulian.com/news/local/missed-risk-long-covid-threat-extends-far-beyond-pandemic/article_70311453-1f78-5068-b1ea-02886b41741a.html#tracking-source=most-popular-homepage">previously healthy people</a>, even in <a href="https://www.nature.com/articles/d41586-022-01453-0">the vaccinated</a>, and <a href="https://fortune.com/2022/08/04/covid-creates-higher-risk-kids-children-pediatric-blood-clots-kidney-failure-heart-problems-type-1-diabetes/">even in children</a>. While vaccines significantly reduce risk of death, unfortunately they provide <a href="https://www.nature.com/articles/d41586-022-01453-0">only a limited reduction in Long Covid risk</a>. Immunity wanes, and people face <a href="https://www.abc.net.au/news/2022-06-24/reinfection-covid-more-severe-new-variants/101178246">cumulative risks with each new covid infection</a> (so even if you’ve had covid once or twice, it is best to try to avoid reinfections). I am alarmed that <a href="https://twitter.com/ColinKinner/status/1557257474235076608?s=20&amp;t=hjlgmHIe24111OVNRCDk_Q">leaders are encouraging</a> mass, repeated infections of a generation of children.</p>
<p>Given all this, I am relieved that our decision to continue homeschooling was relatively clear. It much better suits our daughter’s needs AND drastically reduces our family’s covid risk. We can nurture her innate curiosity, protect her intrinsic motivation, and provide in-person social options that are entirely outdoors and safer than being indoors all day at school. Most families are not so fortunate and many face difficult choices, with no good options.</p>
</section>
<section id="the-broader-picture" class="level2">
<h2 class="anchored" data-anchor-id="the-broader-picture">The Broader Picture</h2>
<p>I believe that high-quality, equitable, and safe public education is important for a healthy democracy, and I worry about the various ongoing ways in which education is being undermined and attacked. Furthermore, due to a lack of covid protections in communities, high-risk children and children with high-risk families are being shut out of in-person school options in the USA, Australia, and many other places. While the <a href="https://twitter.com/math_rachel/status/1464827214298443780?s=20&amp;t=sRWCS-xGf3G57R4LVWcDdQ">workplaces of politicians</a> and a handful of <a href="https://twitter.com/myrabatchelder/status/1485870339368366083?s=20&amp;t=sRWCS-xGf3G57R4LVWcDdQ">schools in ultra-wealthy areas</a> installed expensive ventilation upgrades, the majority of schools in the USA and Australia have not had any ventilation upgrades, nor received air purifiers. All children deserve access to an education that is safe, fits their needs, and will allow them to thrive. Even when homeschooling does work, it is often still just an individual solution to systemic problems.</p>
</section>
<section id="related-posts" class="level2">
<h2 class="anchored" data-anchor-id="related-posts">Related Posts</h2>
<p>A few other posts that you may be interested in, related to my views on education and teaching: - <a href="https://www.fast.ai/2022/03/15/math-person/">There’s No Such Thing as Not A Math Person</a>: based on a webinar I gave to parents addressing cultural myths about math and how to support your kids in their math education, even if you don’t see yourself as a “math person” - <a href="https://www.fast.ai/2016/10/08/teaching-philosophy/">The Qualities of a Good Education</a>: common pitfalls in traditional technical education and ideas towards doing better - <a href="https://www.fast.ai/2018/08/27/grad-school/">What You Need to Know Before Considering a PhD</a>: includes a reflection on how my own success in traditional academic environments was actually a weakness, because I’d learned how to solve problems I was given, but not how to how to find and scope interesting problems on my own</p>


</section>

 ]]></description>
  <category>education</category>
  <guid>https://rachel.fast.ai/posts/2022-09-06-homeschooling/index.html</guid>
  <pubDate>Mon, 05 Sep 2022 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2022-09-06-homeschooling/homeschool-headlines.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Qualitative humanities research is crucial to AI</title>
  <dc:creator>Louisa Bartolow and Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2022-06-01-qualitative/index.html</link>
  <description><![CDATA[ 




<p><em>“All research is qualitative; some is also quantitative”</em> Harvard Social Scientist and Statistician Gary King</p>
<p>Suppose you wanted to find out whether a machine learning system being adopted - to <a href="https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias">recruit candidates</a>, <a href="https://mitibmwatsonailab.mit.edu/research/blog/black-loans-matter-fighting-bias-for-ai-fairness-in-lending/">lend money</a>, or <a href="https://www.propublica.org/podcast/how-we-decided-to-test-racial-bias-in-algorithms">predict future criminality</a> - exhibited racial bias. You might calculate model performance across groups with different races. But <a href="https://dl.acm.org/doi/10.1145/3351095.3372826">how was race categorised</a>– through a census record, a <a href="https://www.youtube.com/watch?v=s24vksJRJeE&amp;list=PLgdVmcMc8gy4sVLlgM0GUnnVZ1Fu0et05&amp;index=6">police officer’s guess</a>, or by an annotator? Each possible answer raises another set of questions. Following the thread of any seemingly quantitative issue around AI ethics quickly leads to a host of qualitative questions. Throughout AI, qualitative decisions are made about what metrics to optimise for, which categories to use, how to define their bounds, who applies the labels. Similarly, qualitative research is necessary to understand AI systems operating in society: evaluating system performance beyond what can be captured in short term metrics, understanding what is missed by large-scale studies (which can elide details and overlook outliers), and shedding light on the circumstances in which data is produced (often by crowd-sourced or poorly paid workers).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2022-06-01-qualitative/biasheadlines.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Attempting to measure racial bias leads to qualitative questions</figcaption><p></p>
</figure>
</div>
<p>Unfortunately, there is often a <a href="https://journals.sagepub.com/doi/full/10.1177/2053951719833404">large divide between</a> computer scientists and social scientists, with over-simplified assumptions and fundamental misunderstandings of one another. Even when cross-disciplinary partnerships occur, <a href="https://journals.sagepub.com/doi/full/10.1177/2053951719833404">they often fall</a> into “normal disciplinary divisions of labour: social scientists observe, data scientists make; social scientists do ethics, data scientists do science; social scientists do the incalculable, data scientists do the calculable.” The solution is not for computer scientists to absorb a shallow understanding of the social sciences, but for deeper collaborations. In a paper on <a href="https://dl.acm.org/doi/10.1145/3442188.3445914">exclusionary practices in AI ethics</a>, an interdisciplinary team wrote of the “indifference, devaluation, and lack of mutual support between CS and humanistic social science (HSS), [which elevates] the myth of technologists as ‘ethical unicorns’ that can do it all, though their disciplinary tools are ultimately limited.”</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2022-06-01-qualitative/Abstract-art-britto-qb140.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">There are challenges when mixing computer scientsits with social scientists. (Gallery Britto image under Creative Commons Attribution-Share Alike 4.0 International)</figcaption><p></p>
</figure>
</div>
<p>This is further reflected in an increasing number of <a href="https://twitter.com/cfiesler/status/1427341587613376514?s=20&amp;t=_5Ud6jmEELMulK5LSha9gA">job ads</a> for AI ethicists that list a <a href="https://www.businessinsider.com/ai-ethics-jobs-catherine-hicks-algorithms">computer science degree as a requirement</a>, “prioritising technical computer science infrastructure over the social science skills that can evaluate AI’s social impact. In doing so, we are building the field of AI Ethics to replicate the very flaws this field is trying to fix.” <a href="https://arxiv.org/abs/2006.12358">Interviews with 26 responsible AI practitioners</a> working in industry highlighted a number of challenges, including that qualitative work was not prioritised. Not only is it impossible to fully understand ethics issues solely through quantitative metrics, inappropriate and misleading quantitative metrics are used to evaluate the responsible AI practitioners themselves. Interviewees reported that their fairness work was evaluated on metrics related to generating revenue, in a stark misalignment of goals.</p>
<section id="qualitative-research-helps-us-evaluate-ai-systems-beyond-short-term-metrics" class="level2">
<h2 class="anchored" data-anchor-id="qualitative-research-helps-us-evaluate-ai-systems-beyond-short-term-metrics">Qualitative research helps us evaluate AI systems beyond short term metrics</h2>
<p>When companies like Google and YouTube want to test whether the recommendations they are making (in the form of search engine results or YouTube videos, for example) are “good” - they will often focus quite heavily on “engagement” or “dwell time” - the time a user spent looking at or watching the item recommended to them. But it turns out, unsurprisingly, that a focus on engagement and dwell time, narrowly understood, <a href="https://arxiv.org/abs/2006.12358">raises all sorts of problems</a>. <a href="https://dl.acm.org/doi/abs/10.1145/3041021.3054197?casa_token=BxsZ44gFqdQAAAAA:uMs35O3Qn8oFsHM8LBYo2XJ5jnQGPD4L2NWhxa7iVa5MMwovG-zd3hoEixb9F5_up1LM0kNSG9Krew">Demographics can impact dwell time</a> (e.g.&nbsp;older users may spend longer on websites than younger users, just as part of the way they use the internet). A system that ‘learns’ from a user’s behavioural cues (rather than their ‘stated preferences’) might lock them into a limiting feedback loop, <a href="https://dl.acm.org/doi/10.1145/2959100.2959179">appealing to that user’s short term interests rather than those of their ‘Better Selves.’</a> Scholars have called for <a href="https://datascience.columbia.edu/news/2018/ethical-principles-okrs-and-kpis-what-youtube-and-facebook-could-learn-from-tukey/">more qualitative research</a> to understand user experience and build this into <a href="https://www.cell.com/patterns/fulltext/S2666-3899(22)00056-3">the development of metrics</a>.</p>
<p>This is the part where people will point out, rightly, that companies like Google and YouTube rely on a complex range of metrics and signals in their machine learning systems - and that where a website ranks on Google, or how a YouTube video performs in recommendation does not boil down to simple popularity metrics, like engagement. Google employs an extensive process to determine <a href="https://www.google.com/search/howsearchworks/mission/users/">“relevance” and “usefulness” for search results</a>. In its <a href="https://support.google.com/websearch/answer/9281931?hl=en">172-page manual for search result ‘Quality’ evaluation</a>, for example, the company explains how evaluators should assess a website’s ‘Expertise/ Authoritativeness/ Trustworthiness’ or ‘E-A-T’; and what types of content, by virtue of its harmful nature (e.g., to protected groups), should be given a ‘low’ ranking. YouTube has identified specific categories of content (such as news, scientific subjects, and historical information) for which ‘authoritativeness’ should be considered especially important. It has also determined that dubious-but-not-quite-rule-breaking information (what it calls ‘borderline content’) should not be recommended, <a href="https://blog.youtube/inside-youtube/the-four-rs-of-responsibility-raise-and-reduce/">regardless of the video’s engagement levels</a>.</p>
<p>Irrespective of how successful we consider the existing approaches of Google Search and YouTube to be (and partly, the issue is that evaluating their implementation from the outside is frustratingly difficult), the point here is that there are constant qualitative judgments being made, about what makes a search result or recommendation “good” and of how to define and quantify expertise, authoritativeness, trustworthiness, borderline content, and other values. This is true of all machine learning evaluation, even when it isn’t explicit. In a paper guiding companies about <a href="https://dl.acm.org/doi/10.1145/3351095.3372873">how to carry out internal audits of their AI systems</a>, Inioluwa Deborah Raji and colleagues emphasise the importance of interviews with management and engineering teams to “capture and pay attention to what falls outside the measurements and metrics, and to render explicit the assumptions and values the metrics apprehend.” (p.40).</p>
<p>The importance of thoughtful humanities research is heightened if we are serious about grappling with the potential broader social effects of machine learning systems (both good and bad), which are often <a href="https://policyreview.info/articles/analysis/beyond-individual-governing-ais-societal-harm">delayed, distributed and cumulative</a>.</p>
</section>
<section id="small-scale-qualitative-studies-tell-an-important-story" class="level2">
<h2 class="anchored" data-anchor-id="small-scale-qualitative-studies-tell-an-important-story">Small-scale qualitative studies tell an important story</h2>
<p>Hypothetically, let’s say you wanted to find out whether the use of AI technologies by doctors during a medical appointment would make doctors less attentive to patients - what do you think the best way of doing it would be? You could find some criteria and method for measuring ‘attentiveness’, say tracking the amount of eye contact between the doctor and patient, and analyse this across a representative sample of medical appointments where AI technologies were being used, compared to a control group of medical appointments where AI technologies weren’t being used. Or would you interview doctors about their experiences using the technology during appointments? Or talk to patients about how they felt the technology did, or didn’t, impact their experience?</p>
<p>In research circles, we describe these as ‘epistemological’ choices - your judgement of what constitutes the ‘best’ approach is inextricably linked to your judgement about <a href="https://theconversation.com/how-do-you-know-that-what-you-know-is-true-thats-epistemology-63884"><em>how</em> we can claim to ‘know’ something</a>. These are all valid methods for approaching the question, but you can imagine how they might result in different, even conflicting, insights. For example, you might end up with the following results: - The eye contact tracking experiment suggests that overall, there is no significant difference in doctors’ attentiveness to the patient when the AI tech is introduced. - The interviews with doctors and patients reveal that some doctors and patients feel that the AI technology reduces doctors’ attentiveness to patients, and others feel that it makes no difference or even increases doctors’ attention to the patient.</p>
<p>Even if people are not negatively impacted by something ‘on average’ (e.g., in our hypothetical eye contact tracking experiment above), there will remain groups of people who will experience negative impacts, perhaps acutely so. “Many of people’s most pressing questions are about effects that vary for different people,” write Matias, Pennington and Chan in a recent paper on the <a href="https://osf.io/tn6x4/">idea of N-of-one trials</a>. To tell people that their experiences aren’t real or valid because they don’t meet some threshold for statistical significance across a large population doesn’t help us account for the breadth and nature of AI’s impacts on the world.</p>
<p>Examples of this tension between competing claims to knowledge about AI systems’ impacts abound. Influencers who believe they are being systematically downranked (‘shadowbanned’) by Instagram’s algorithmic systems are told by Instagram that this simply isn’t true. Given the inscrutability of these proprietary algorithmic systems, it is impossible for influencers to convincingly dispute Instagram’s claims. Kelley Cotter refers to this as a form of <a href="https://www.tandfonline.com/doi/full/10.1080/1369118X.2021.1994624">“black box gaslighting”</a>: platforms can “leverage perceptions of their epistemic authority on their algorithms to undermine users’ confidence in what they know about algorithms and destabilise credible criticism.” Her interviews with influencers give voice to stakeholder concerns and perspectives that are elided in Instagram’s official narrative about its systems. The mismatch between different stakeholders’ accounts of ‘reality’ is instructive. For example, a widely-cited paper by Netflix employees claims that Netflix recommendation <a href="https://dl.acm.org/doi/10.1145/2843948">“influences choice for about 80% of hours streamed at Netflix.”</a> But this claim stands in stark contrast to Mattias Frey’s mixed-methods research (representative survey plus small sample for interviews) run with UK and US adults, in which <a href="https://www.ucpress.edu/book/9780520382046/netflix-recommends#:~:text=Netflix%20Recommends%20brings%20to%20light,critics%20is%20stronger%20than%20ever.">less than 1 in 5 adults said they primarily relied on Netflix recommendations</a> when deciding what films to watch. Even if this is because users underestimate their reliance on recommender systems, that’s a critically important finding - particularly when we’re trying to regulate recommendation and so many are advocating providing better user-level controls as a check on platform power. Are people really going to go to the trouble of changing their settings if they don’t think they rely on algorithmic suggestions that much anyway?</p>
</section>
<section id="qualitative-research-sheds-light-on-the-context-of-data-annotation" class="level2">
<h2 class="anchored" data-anchor-id="qualitative-research-sheds-light-on-the-context-of-data-annotation">Qualitative research sheds light on the context of data annotation</h2>
<p>Machine learning systems rely on vast amounts of data. In many cases, for that data to be useful, it needs to be labelled/ annotated. For example, a hate speech classifier (an AI-enabled tool used to identify and flag potential cases of hate speech on a website) relies on huge datasets of text labelled as ‘hate speech’ or ‘not hate speech’ to ‘learn’ how to spot hate speech. But it turns out that <a href="https://arxiv.org/abs/2112.04554"><em>who</em> is doing the annotating and in <em>what context</em> they’re doing it, matters</a>. AI-powered content moderation is often held up as the solution to harmful content online. What has continued to be underplayed is the extent to which those automated systems are and most likely will remain dependent on the manual work of human content moderators sifting through some of the worst and most traumatic online material to power the machine learning datasets on which automated content moderation depends. Emily Denton and her colleagues highlight the significance of annotators’ social identity (e.g., race, gender) and their expertise when it comes to annotation tasks, and they point out the risks associated with overlooking these factors and simply ‘aggregating’ results as ‘ground truth’ rather than properly exploring disagreements between annotators and the important insights that this kind of disagreement might offer.</p>
<p>Human commercial content moderators (such as the people that identify and remove violent and traumatic imagery on Facebook) often labour in terrible conditions, lacking psychological support or appropriate financial compensation. The <a href="https://yalebooks.yale.edu/book/9780300261479/behind-the-screen/">interview-based research of Sarah T. Roberts</a> has been pioneering in highlighting these conditions. Most demand for crowdsourced digital labour comes from the Global North, <a href="https://journals.sagepub.com/doi/full/10.1177/1024258916687250">yet the majority of these workers</a> are based in the Global South and receive low wages. Semi-structured interviews reveal the extent to which workers feel unable to bargain effectively for better pay in the current regulatory environment. As Mark Graham and his colleagues point out, these findings are hugely important in a context where several governments and supranational development organisations like the World Bank are holding up digital work as a promising tool to fight poverty.</p>
<p>The decision of how to measure ‘race’ in machine learning systems is highly consequential, especially in the context of existing efforts to evaluate these systems for their “fairness.” Alex Hanna, Emily Denton, Andrew Smart and Jamila Smith-Loud have done <a href="https://dl.acm.org/doi/10.1145/3351095.3372826">crucial work highlighting the limitation</a> of machine learning systems that rely on official records of race or their proxies (e.g.&nbsp;census records), noting that the racial categories provided by such records are “unstable, contingent, and rooted in racial inequality.” The authors emphasise the importance of conducting research in ways that prioritise the perspectives of the marginalised racial communities that fairness metrics are supposed to protect. Qualitative research is ideally placed to contribute to a consideration of “race” in machine learning systems that is grounded in the lived experiences and needs of the racially subjugated.</p>
</section>
<section id="what-next" class="level2">
<h2 class="anchored" data-anchor-id="what-next">What next?</h2>
<p>Collaborations between quantitative and qualitative researchers are valuable in understanding AI ethics from all angles.</p>
<p>Consider reading more broadly, outside your particular area. Perhaps using the links and researchers listed here as starting points. They’re just a sliver of the wealth that’s out there. You could also check out the <a href="https://socialmediacollective.org/reading-lists/critical-algorithm-studies/">Social Media Collective’s Critical Algorithm Studies reading list</a>, the reading list provided by the <a href="https://zoeglatt.com/?page_id=545">LSE Digital Ethnography Collective</a>, and <a href="https://medium.com/fair-bytes/reading-list-for-fairness-in-ai-topics-337e8606fd8d">Catherine Yeo’s suggestions</a>.</p>
<p>Strike up conversations with researchers in other fields, and consider the possibility of collaborations. Find a researcher slightly outside your field but whose work you broadly understand and like, and follow them on Twitter. With any luck, they will share more of their work and help you identify other researchers to follow. Collaboration can be an incremental process: Consider inviting the researcher to form part of a discussion panel, reach out to say what you liked and appreciated about their work and why, and share your own work with them if you think it’s aligned with their interests.</p>
<p>Within your university or company, is there anything you could do to better reward or facilitate interdisciplinary work? As Humanities Computing Professor <a href="https://www.tandfonline.com/doi/full/10.1080/03080188.2022.2031659">Willard McCarty notes</a>, somewhat discouragingly, “professional reward for genuinely interdisciplinary research is rare.” To be sure, individual researchers and practitioners have to be prepared <a href="https://sciencetechnologystudies.journal.fi/article/view/97321/56310">to put themselves out there, compromise</a> and <a href="https://www.mccarty.org.uk/essays/McCarty,%20Becoming%20interdisciplinary.pdf">challenge themselves</a> - but carefully tailored <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ijmr.12016?casa_token=9f4Qosl_MLEAAAAA%3AMFozyMi171KgZ-BYQrpS-1bK71gt70I4ecjy8lLId54-2zvTobqCGKIK_-9_hEk8-wI5vMR0Em5-">institutional incentives and enablers matter</a>.</p>


</section>

 ]]></description>
  <category>ethics</category>
  <category>machine learning</category>
  <guid>https://rachel.fast.ai/posts/2022-06-01-qualitative/index.html</guid>
  <pubDate>Tue, 31 May 2022 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2022-06-01-qualitative/Abstract-art-britto-qb140.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI Harms are Societal, Not Just Individual</title>
  <dc:creator>Rachel Thomas and Louisa Bartolo</dc:creator>
  <link>https://rachel.fast.ai/posts/2022-05-17-societal-harms/index.html</link>
  <description><![CDATA[ 




<section id="not-just-individual-but-societal-harms" class="level2">
<h2 class="anchored" data-anchor-id="not-just-individual-but-societal-harms">Not just Individual, but Societal Harms</h2>
<p>When the USA government switched to facial identification service ID.me for unemployment benefits, the software <a href="https://www.cpr.org/2021/07/07/colorado-unemployment-idme-glitch-internet-access/">failed to recognize Bill Baine’s face</a>. While the app said that he could have a virtual appointment to be verified instead, he was unable to get through. The screen had a wait time of 2 hours and 47 minutes that never updated, even over the course of weeks. He tried calling various offices, his daughter drove in from out of town to spend a day helping him, and yet he was never able to get a useful human answer on what he was supposed to do, as he went for months without unemployment benefits. In Baine’s case, it was eventually resolved when a journalist hypothesized that the issue was a spotty internet connection, and that Baine would be better off traveling to another town to use a public library computer and internet. Even then, it still took hours for Baine to get his approval.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2022-05-17-societal-harms/cpr-news-bill-blaine.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Journalist Andrew Kenney of Colorado Public Radio has covered the issues with ID.me</figcaption><p></p>
</figure>
</div>
<p>Baine was not alone. The number of people receiving unemployment benefits plummeted by 40% in the 3 weeks after ID.me was introduced. Some of these were presumed to be fraudsters, but it is unclear how many genuine people in need of benefits were wrongly harmed by this. These are individual harms, but there are broader, societal harms as well: the cumulative costs of the public having to spend ever more time on hold, trying to navigate user-hostile automated bureaucracies where they can’t get the answers they need. There is the societal cost of greater inequality and greater desperation, as more people are plunged into poverty through erroneous denial of benefits. And there is the undermining of trust in public services, which can be difficult to restore.</p>
<p><a href="https://fpf.org/wp-content/uploads/2017/12/FPF-Automated-Decision-Making-Harms-and-Mitigation-Charts.pdf">Potential for algorithmic harm</a> takes many forms: loss of opportunity (employment or housing discrimination), economic cost (credit discrimination, narrowed choices), social detriment (stereotype confirmation, dignitary harms), and loss of liberty (increased surveillance, disproportionate incarceration). And each of these four categories manifests in both individual and societal harms.</p>
<p>It should come as no surprise that algorithmic systems can give rise to societal harm. These systems are sociotechnical: they are designed by humans and teams that bring their values to the design process, and <a href="https://www.sciencedirect.com/science/article/pii/S2666389921000155">algorithmic systems continually draw information from, and inevitably bear the marks of, fundamentally unequal, unjust societies</a>. In the context of COVID-19, for example, <a href="https://www.bmj.com/content/372/bmj.n304">policy experts warned</a> that historical healthcare inequities risked making their way into the datasets and models being used to predict and respond to the pandemic. And while it’s intuitively appealing to think of large-scale systems as creating the greatest risk of societal harm, algorithmic systems can create societal harm because of the <a href="https://cerre.eu/publications/what-is-the-harm-in-size/">dynamics set off by their interconnection with other systems/ players</a>, like <a href="https://www.nature.com/articles/s42256-021-00358-3">advertisers</a>, or commercially-driven media, and the ways in which they touch on sectors or spaces of public importance.</p>
<p>Still, in the west, our ideas of harm are often anchored to an individual being harmed <a href="https://policyreview.info/articles/analysis/beyond-individual-governing-ais-societal-harm">by a particular action at a discrete moment in time</a>. As law scholar Natalie Smuha has powerfully argued, legislation (both proposed and passed) in Western countries to address algorithmic risks and harms often focuses on individual rights: regarding how an individual’s data is collected or stored, to not be discriminated against, or to know when AI is being used. Even metrics used to evaluate the fairness of algorithms are often aggregating across individual impacts, but unable to capture longer-term, more complex, or second- and third-order societal impacts.</p>
</section>
<section id="case-study-privacy-and-surveillance" class="level2">
<h2 class="anchored" data-anchor-id="case-study-privacy-and-surveillance">Case Study: Privacy and surveillance</h2>
<p>Consider the over-reliance on individual harms in discussing privacy: so often focused on whether individual users have the ability to opt in or out of sharing their data, notions of individual consent, or proposals that individuals be paid for their personal data. Yet widespread surveillance fundamentally changes society: people may begin to self-censor and to be less willing (or able) to advocate for justice or social change. Professor Alvaro Bedoya, director of the Center on Privacy and Technology at the Georgetown University Law Center, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3599201">traces a history</a> of how surveillance has been used by the state to try to shut down movements for progress– targeting religious minorities, poor people, people of color, immigrants, sex workers and those considered “other”. <a href="https://idlewords.com/2019/06/the_new_wilderness.htm">As Maciej Ceglowski</a> writes, “Ambient privacy is not a property of people, or of their data, but of the world around us… Because our laws frame privacy as an individual right, we don’t have a mechanism for deciding whether we want to live in a surveillance society.”</p>
<p><a href="https://arxiv.org/abs/2103.01168">Drawing on interviews with African data experts</a>, Birhane et al write that even when data is anonymized and aggregated, it “can reveal information on the community as a whole. While notions of privacy often focus on the individual, there is growing awareness that collective identity is also important within many African communities, and that sharing aggregate information about communities can also be regarded as a privacy violation.” Recent US-based scholarship has also highlighted the importance of thinking about <a href="https://link.springer.com/chapter/10.1007/978-3-030-82786-1_6">group level privacy</a> (whether that group is made up of individuals who identify as members of that group, or whether it’s a ‘group’ that is algorithmically determined - like individuals with similar shopping habits on Amazon). Because even aggregated anonymised data can reveal important group-level information (e.g., the location of military personnel training via exercise tracking apps) “managing privacy”, these authors argue “is often not intrapersonal but interpersonal.” And yet legal and tech design privacy solutions are often better geared towards assuring individual-level privacy than negotiating group privacy.</p>
</section>
<section id="case-study-disinformation-and-erosion-of-trust" class="level2">
<h2 class="anchored" data-anchor-id="case-study-disinformation-and-erosion-of-trust">Case Study: Disinformation and erosion of trust</h2>
<p>Another example of a collective societal harm comes from how technology platforms such as Facebook have played a significant role in elections ranging from the Philippines to Brazil, yet it can be difficult (and not necessarily possible or useful) to quantify exactly how much: something as complex as a country’s political system and participation involves many interlinking factors. But when ‘deep fakes’ make it <a href="https://heinonline.org/HOL/P?h=hein.journals/calr107&amp;i=1789">“possible to create audio and video of real people saying and doing things they never said or did”</a> or when motivated actors successfully <a href="https://policyreview.info/articles/analysis/disinformation-optimised-gaming-search-engine-algorithms-amplify-junk-news">game search engines to amplify disinformation</a>, the (potential) harm that is generated is societal, not just individual. Disinformation and the undermining of trust in institutions and fellow citizens have broad impacts, including on individuals who never use social media.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2022-05-17-societal-harms/disinfo-headlines.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Reports and Events on Regulatory Approaches to Disinformation</figcaption><p></p>
</figure>
</div>
<p>Efforts by national governments to deal with the problem through regulation have not gone down well with everyone. ‘Disinformation’ has repeatedly been highlighted as one of the tech-enabled ‘societal harms’ that the UK’s Online Safety Bill or the EU’s Digital Services Act should address, and a range of governments are taking aim at the problem by proposing or passing a slew of (in certain cases ill-advised) <a href="https://www.poynter.org/ifcn/anti-misinformation-actions/">‘anti-misinformation’ laws</a>. But there’s widespread unease around handing power to governments to set standards for what counts as ‘disinformation’. Does reifying ‘disinformation’ as a societal harm become a legitimizing tool for governments looking to silence political dissent or undermine their weaker opponents? It’s a fair and important concern - and yet simply leaving that power in the hands of mostly US-based, unaccountable tech companies is hardly a solution. What are the <a href="https://www.adalovelaceinstitute.org/event/national-approaches-online-harms-regulation/">legitimacy implications if a US company like Twitter were to ban</a> democratically elected Brazilian President Jair Bolsonaro for spreading disinformation, for example? How do we ensure that tech companies are investing sufficiently in <a href="https://law.yale.edu/isp/initiatives/wikimedia-initiative-intermediaries-and-information/wiii-blog/moderate-globally-impact-locally-series-content-moderation-global-south">governance efforts across the globe</a>, rather than responding in an ad hoc manner to proximal (i.e.&nbsp;mostly US-based) concerns about disinformation? Taking a hands off approach to platform regulation doesn’t make platforms’ efforts to deal with disinformation any less politically fraught.</p>
</section>
<section id="individual-harms-individual-solutions" class="level2">
<h2 class="anchored" data-anchor-id="individual-harms-individual-solutions">Individual Harms, Individual Solutions</h2>
<p>If we consider individual solutions our only option (in terms of policy, law, or behavior), we often limit the scope of the harms we can recognize or the nature of the problems we face. To take an example not related to AI: Oxford professor Trish Greenhalgh et al <a href="https://www.authorea.com/users/316109/articles/545687-how-covid-19-spreads-narratives-counter-narratives-and-social-dramas">analyzed the slow reluctance</a> of leaders in the West to accept that covid is airborne (e.g.&nbsp;it can linger and float in the air, similar to cigarette smoke, requiring masks and ventilation to address), rather than droplet dogma (e.g.&nbsp;washing your hands is a key precaution). One reason they highlight is the Western framing of individual responsibility as the solution to most problems. Hand-washing is a solution that fits the idea of individual responsibility, whereas collective responsibility for the quality of shared indoor air does not. The allowable set of solutions helps shape what we identify as a problem. Additionally, the fact that recent research suggests that “the level of interpersonal trust in a society” was a strong predictor of which countries managed COVID-19 most successfully should give us pause. Individualistic framings can limit our imagination about the problems we face and which solutions are likely to be most impactful.</p>
</section>
<section id="parallels-with-environmental-harms" class="level2">
<h2 class="anchored" data-anchor-id="parallels-with-environmental-harms">Parallels with Environmental Harms</h2>
<p>Before the passage of environmental laws, many existing legal frameworks were not well-suited to address environmental harms. Perhaps a chemical plant releases waste emissions into the air once per week. Many people in surrounding areas may not be aware that they are breathing polluted air, or may not be able to directly link air pollution to a new medical condition, such as asthma, (which could be related to a variety of environmental and genetic factors).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2022-05-17-societal-harms/air-pollution.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">There are parallels between air polllution and algorithmic harms</figcaption><p></p>
</figure>
</div>
<p>There are many parallels between environmental issues and AI ethics. Environmental harms include individual harms for people who develop discrete health issues from drinking contaminated water or breathing polluted air. Yet, environmental harms are also societal: the societal costs of contaminated water and polluted air can reverberate in subtle, surprising, and far-reaching ways. As law professor Nathalie Smuha writes, <a href="https://policyreview.info/articles/analysis/beyond-individual-governing-ais-societal-harm">environmental harms are often accumulative and build over time</a>. Perhaps each individual release of waste chemicals from a refinery has little impact on its own, but adds up to be significant. In the EU, environmental law allows for mechanisms to show societal harm, as it would be difficult to challenge many environmental harms on the basis of individual rights. Smuha argues that there are many similarities with AI ethics: for opaque AI systems, spanning over time, it can be difficult to prove a direct causal relationship to societal harm.</p>
</section>
<section id="directions-forward" class="level2">
<h2 class="anchored" data-anchor-id="directions-forward">Directions Forward</h2>
<p>To a large extent our message is to tech companies and policymakers. It’s not enough to focus on the potential individual harms generated by tech and AI: the broader societal costs of tech and AI matter.</p>
<p>But those of us outside tech policy circles have a crucial role to play. One way in which we can guard against the risks of the ‘societal harm’ discourse being co-opted by those with political power to legitimise undue interference and further entrench their power is by claiming the language of ‘societal harm’ as the democratic and democratising tool it can be. We all lose when we pretend societal harms don’t exist, or when we acknowledge they exist but throw our hands up. And those with the least power, like Bill Baine, are likely to suffer a disproportionate loss.</p>
<p>In his newsletter on Tech and Society, L.M. Sacasas encourages people to ask themselves <a href="https://theconvivialsociety.substack.com/p/the-questions-concerning-technology?">41 questions before using a particular technology</a>. They’re all worth reading and thinking about - but we’re listing a few especially relevant ones to get you started. Next time you sit down to log onto social media, order food online, swipe right on a dating app or consider buying a VR headset, ask yourself:</p>
<ul>
<li>How does this technology empower me? At whose expense? (Q16)</li>
<li>What feelings does the use of this technology generate in me toward others? (Q17)</li>
<li>What limits does my use of this technology impose upon others? (Q28)</li>
<li>What would the world be like if everyone used this technology exactly as I use it? (Q37)</li>
<li>Does my use of this technology make it easier to live as if I had no responsibilities toward my neighbor? (Q40)</li>
<li>Can I be held responsible for the actions which this technology empowers? Would I feel better if I couldn’t? (Q41)</li>
</ul>
<p>It’s on all of us to sensitise ourselves to the societal implications of the tech we use.</p>


</section>

 ]]></description>
  <category>ethics</category>
  <category>machine learning</category>
  <guid>https://rachel.fast.ai/posts/2022-05-17-societal-harms/index.html</guid>
  <pubDate>Mon, 16 May 2022 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2022-05-17-societal-harms/air-pollution.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>There’s no such thing as not a math person</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2022-03-15-math-person/index.html</link>
  <description><![CDATA[ 




<p>On the surface, I may seem into math: I have a math PhD, taught a graduate <a href="https://www.fast.ai/2017/07/17/num-lin-alg/">computational linear algebra</a> course, co-founded AI research lab <a href="https://www.fast.ai/about/">fast.ai</a>, and even go by the twitter handle <a href="https://twitter.com/math_rachel"><span class="citation" data-cites="math_rachel">@math_rachel</span></a>.</p>
<p>Yet many of my experiences of academic math culture have been toxic, sexist, and deeply alienating. At my lowest points, I felt like there was <a href="https://www.youtube.com/watch?v=LqjP7O9SxOM&amp;list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6">no place for me</a> in math academia or math-heavy tech culture.</p>
<p>It is not just mathematicians or math majors who are impacted by this: Western culture is awash in negative feelings and experiences regarding math, which permate from many sources and impact students of all ages. In this post, I will explore the cultural factors, misconceptions, stereotypes, and relevant studies on obstacles that turn people off to math. If you (or your child) doesn’t like math or feels anxious about your own capabilities, you’re not alone, and this isn’t just a personal challenge. The below essay is based on part of <a href="https://www.youtube.com/watch?v=VmA8-vYwjM0&amp;list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6&amp;index=8">a talk</a> I recently gave.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2022-03-15-math-person/teaching.jpg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">me, teaching sorting algorithms, at an all-women coding academy in 2015</figcaption><p></p>
</figure>
</div>
<section id="myth-of-innate-ability-myth-of-the-lone-genius" class="level2">
<h2 class="anchored" data-anchor-id="myth-of-innate-ability-myth-of-the-lone-genius">Myth of Innate Ability, Myth of the Lone Genius</h2>
<p>One common myth is the idea that certain people’s brains aren’t “wired” the right way to do math, tech, or AI, that your brain either “works that way” or not. None of the evidence supports this viewpoint, yet when people believe this, it can become a self-fulfilling prophecy. Dr.&nbsp;Omoju Miller, who earned her PhD at UC Berkeley and was a senior machine learning engineer and technical advisor to the CEO at Github, shares some of the research debunking the myth of innate ability in <a href="http://omojumiller.com/articles/The-Myth-Of-Innate-Ability-In-">this essay</a> and in <a href="https://www.youtube.com/watch?v=BFWVHSeakkg">her TEDx talk</a>. In reality, there is no such thing as “not a math person.”</p>
<p>Dr.&nbsp;Cathy O’Neil, a Harvard Math PhD and author of <a href="https://www.penguin.com.au/books/weapons-of-math-destruction-9780141985411">Weapons of Math Destruction</a>, wrote about the <a href="https://www.bloomberg.com/opinion/articles/2017-06-07/a-mathematician-s-secret-we-re-not-all-geniuses">myth of the lone genius mathematician</a>, <em>“You don’t have to be a genius to become a mathematician. If you find this statement at all surprising, you’re an example of what’s wrong with the way our society identifies, encourages and rewards talent… For each certified genius, there are at least a hundred great people who helped achieve such outstanding results.”</em></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2022-03-15-math-person/miller-oneil.jpg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Dr.&nbsp;Miller debunking the myth of innate ability, and Dr.&nbsp;O’Neil debunking the myth of the lone genius mathematician</figcaption><p></p>
</figure>
</div>
</section>
<section id="music-without-singing-or-instruments" class="level2">
<h2 class="anchored" data-anchor-id="music-without-singing-or-instruments">Music without singing or instruments</h2>
<p>Imagine a world where children are not allowed to sing songs or play instruments until they reach adulthood, after spending a decade or two transcribing sheet music by hand. This scenario is absurd and nightmarish, yet it is analogous to how math is often taught, with the most creative and interesting parts saved until almost everyone has dropped out. Dr.&nbsp;Paul Lockhart eloquently describes this metaphor in his essay, <a href="https://www.maa.org/sites/default/files/pdf/devlin/LockhartsLament.pdf">A Mathematician’s Lament</a>, on <em>“how school cheats us out of our most fascinating and imaginative art form.”</em> Dr.&nbsp;Lockhart left his role as a university math professor to teach K-12 math, as he felt that so much reform was needed in how math is taught.</p>
<p>Dr.&nbsp;David Perkins uses the analogy of how children can play baseball wthout knowing all the technical details, without having a full team or playing a full 9 innings, yet still gain a sense of the <a href="https://books.google.com.au/books/about/Making_Learning_Whole.html?id=0DF9WxgGgNsC">“whole game.”</a> Math is usually taught with an overemphasis on dry, technical details, without giving students a concept of the “whole game.” It can take years and years before enough technical details are accumulated to build something interesting. There is an overemphasis on techniques rather than meaning.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2022-03-15-math-person/musicbaseball.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">What if math was taught more like how music or sports are taught?</figcaption><p></p>
</figure>
</div>
<p>Math curriculums are usually arranged in a vertical manner, with each year building tightly on the previous, such that one bad year can ruin everything that comes after. Many people I talk to can pinpoint the year that math went bad for them: <em>“I used to like math until 6th grade, when I had a bad teacher/was dealing with peer pressure/my undiagnosed ADHD was out of control. After that, I was never able to succeed in future years.”</em> This is less true in other subjects, where one bad history teacher/one bad year doesn’t mean that you can’t succeed at history the following year.</p>
</section>
<section id="gender-race-and-stereotypes" class="level2">
<h2 class="anchored" data-anchor-id="gender-race-and-stereotypes">Gender, race, and stereotypes</h2>
<p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2836676/">Female teachers’ math anxiety affects girls’ math achievement</a>: In the USA, over 90% of primary school teachers are female, and research has found <em>“the more anxious teachers were about math, the more likely girls (but not boys) were to endorse the commonly held stereotype that ‘boys are good at math, and girls are good at reading’ and the lower these girls’ math achievement… People’s fear and anxiety about doing math—over and above actual math ability—can be an impediment to their math achievement.”</em></p>
<p><a href="https://www.nytimes.com/2015/04/27/opinion/how-to-attract-female-engineers.html">Research</a> across a number of universities has found that more women go into engineering when courses focus on problems with positive social impact.</p>
<p>Structural racism also impacts what messages teachers impart to students. An Atlantic article <a href="https://www.theatlantic.com/education/archive/2017/04/racist-math-education/524199/">How Does Race Affect a Student’s Math Education?</a> covered the research paper <a href="https://journals.tdl.org/jume/index.php/JUME/article/view/294">A Framework for Understanding Whiteness in Mathematics Education</a>, noting that <em>“Constantly reading and hearing about underperforming Black, Latino, and Indigenous students begins to embed itself into how math teachers view these students, attributing achievement differences to their innate ability to succeed in math… teachers start to expect worse performance from certain students, start to teach lower content, and start to use lower-level math instructional practices. By contrast, white and Asian students are given the benefit of the doubt and automatically afforded the opportunity to do more sophisticated and substantive mathematics.”</em></p>
</section>
<section id="the-mathematics-community-is-an-absolute-mess-which-actively-pushes-out-the-sort-of-people-who-might-make-it-better" class="level2">
<h2 class="anchored" data-anchor-id="the-mathematics-community-is-an-absolute-mess-which-actively-pushes-out-the-sort-of-people-who-might-make-it-better">The mathematics community is “an absolute mess which actively pushes out the sort of people who might make it better”</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2022-03-15-math-person/piperharron.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Dr.&nbsp;Harron’s website, and some of the coverage of her number theory thesis, including on the Scientific American blog</figcaption><p></p>
</figure>
</div>
<p>Dr.&nbsp;Piper Harron made waves with her <a href="http://www.theliberatedmathematician.com/thesis/">Princeton PhD thesis</a>, utilizing humor, analogies, sarcasm, and genuine efforts to be accessible as she described advanced concepts in a ground-breaking way, very atypical for a mathematics PhD thesis. Dr.&nbsp;Harron wrote openly in the prologue of her thesis on how alienating the culture of mathematics is, <em>“As any good grad student would do, I tried to fit in, mathematically. I absorbed the atmosphere and took attitudes to heart. I was miserable, and on the verge of failure. The problem was not individuals, but a system of self-preservation that, from the outside, feels like a long string of betrayals, some big, some small, perpetrated by your only support system.”</em> At her blog, the Liberated Mathematician, <a href="http://www.theliberatedmathematician.com/">she writes</a>, <em>“My view of mathematics is that it is an absolute mess which actively pushes out the sort of people who might make it better.”</em></p>
<p>These descriptions resonate with my own experiences obtaining a math PhD (as well as the experiences of many friends, at a variety of universities). The toxicity of academic math departments is self-perpetuating, pushing out the people who could make them better.</p>
</section>
<section id="the-full-talk" class="level2">
<h2 class="anchored" data-anchor-id="the-full-talk">The full talk</h2>
<p>This post is based on the first part of the talk I gave in the below video, which includes more detail and a Q&amp;A. The talk also includes recommendations about math apps and resources, as well as a framework for how to consider screentime. Stay tuned for a future fast.ai blog post covering math apps and screentime.</p>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/VmA8-vYwjM0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
</center>


</section>

 ]]></description>
  <category>advice</category>
  <category>education</category>
  <guid>https://rachel.fast.ai/posts/2022-03-15-math-person/index.html</guid>
  <pubDate>Mon, 14 Mar 2022 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2022-03-15-math-person/teaching.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Avoiding Data Disasters</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2021-11-04-data-disasters/index.html</link>
  <description><![CDATA[ 




<p>Things can go disastrously wrong in data science and machine learning projects when we undervalue data work, use data in contexts that it wasn’t gathered for, or ignore the crucial role that humans play in the data science pipeline. A new multi-university <a href="https://cires.org.au/">centre focused on Information Resilience</a>, funded by the Australian government’s top scientific funding body (ARC), has recently launched. <strong>Information Resilience</strong> is the capacity to detect and respond to failures and risks across the information chain in which data is sourced, shared, transformed, analysed, and consumed. I’m honored to be a member of the strategy board, and I have been thinking about what information resilience means with respect to data practices. Through a series of case studies and relevant research papers, I will highlight these risks and point towards more resilient practices.</p>
<section id="case-study-uk-covid-tracking-app" class="level2">
<h2 class="anchored" data-anchor-id="case-study-uk-covid-tracking-app">Case study: UK covid tracking app</h2>
<p>Data from a covid-symptom tracking app was used in a research paper to draw <a href="https://twitter.com/ahandvanish/status/1313973286364229638?s=20">wildly inaccurate conclusions</a> about the prevalence of Long Covid, the often debilitating neurological, vascular, and immune disease that can last for months or longer (some patients have been sick for 20 months and counting). The app suggested that only 1.5% of patients still experience symptoms after 3 months, an order of magnitude smaller than estimates of 10-35% being found by other studies.</p>
<p>How could this research project have gone so wrong? Well, the app had been designed for a completely different purpose (tracking 1-2 week long respiratory infections), didn’t include the most common Long Covid symptoms (such as neurological dysfunction), had a frustrating user-interface that led many patients to quit using it, and made the erroneous assumption that those who stopped logging must be fully recovered. The results from this faulty research paper were widely shared, including in a BBC article, offering false reassurance than Long Covid prevalence is much rarer than it is. Patients had been voicing their frustrations with the app all along, and if researchers had listened sooner, they could have collected a much higher quality and more accurate data set.</p>
<p>This research failure illustrates a few common issues in data projects:</p>
<ul>
<li>The <strong>context of the data</strong> was not taken into account. The user-interface, the categories listed, the included features– these were all designed to record data about a short-term mild respiratory infection. However, when it was used for a different purpose (long covid patients suffering for months with vascular and neurological symptoms), it did a poor job, and led to missing and incomplete data. This happens all too often, in which data gathered for one context is used for another</li>
<li>The <strong>people most impacted</strong> (long covid patients) were ignored. They had the most accurate expertise on what long covid actually entailed, yet were not listened to. Ignoring this expertise led to lower quality data and erroneous research conclusions. Patients have <a href="https://bostonreview.net/science-nature/rachel-thomas-medicines-machine-learning-problem">crucial domain expertise</a>, which is distinct from that of doctors, and must be included in medical data science projects. From the start of the pandemic, patients who had suffered from other debilitating post-viral illnesses warned that we should be on the lookout for long-term illness, even in initially “mild” cases.</li>
</ul>
</section>
<section id="data-is-crucial" class="level2">
<h2 class="anchored" data-anchor-id="data-is-crucial">Data is Crucial</h2>
<p>Collecting data about covid and its long-term effects directly from patients was a good idea, but poorly executed in this case. Due to <a href="https://www.fast.ai/2019/08/07/surveillance/">privacy and surveillance risks</a>, I frequently remind people not to record data that they don’t need. However, the pandemic has been a good reminder of how much data we really do need, and how tough it is when it’s missing.</p>
<p>At the start of the pandemic in the United States, we had very little data about what was happening– the government was not tabulating information on cases, testing, or hospitalization. How could we know how to react when we didn’t understand how many cases there were, what death rates were, how transmissible the disease was, and other crucial information? How could we make policy decisions in the absence of a basic understanding of the facts.</p>
<p>In early March 2020, two journalists and a data scientist from a medication-discovery platform began <a href="https://www.cjr.org/the_profile/covid-tracking-project.php">pulling covid data together</a> into a spreadsheet to understand the situation in the USA. This launched into a <a href="https://covidtracking.com/analysis-updates/why-we-didnt-automate-our-data-collection">15-month long project in which 500 volunteers</a> compiled and published data on COVID-19 testing, cases, hospitalizations, and deaths in the USA. During those 15 months, the Covid Tracking Project was the most comprehensive source of covid data in the USA, even more comprehensive than what the CDC had, and it was used by the CDC, numerous government agencies, and both the Trump and Biden Administrations. It was cited in academic studies and in thousands of news articles.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2021-11-04-data-disasters/covid-track.jpg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">A reflection on the covid tracking project from a core data infrastructure engineer</figcaption><p></p>
</figure>
</div>
<p>A data infrastructure engineer and contributor for the project <a href="https://covidtracking.com/analysis-updates/why-we-didnt-automate-our-data-collection">later recounted</a>, “It quickly became apparent that daily, close contact with the data was necessary to understand what states were reporting. States frequently changed how, what, and where they reported data. Had we set up a fully automated data capture system in March 2020, it would have failed within days.” The project used automation as a way to support and supplement manual work, not to replace it. At numerous points, errors in state reporting mechanisms were caught by eagle-eyed data scientists notifying discrepancies.</p>
<p>This vision of using automation to support human work resonates with our interest at fast.ai in “augmentedML”, not “autoML”. I have <a href="https://www.fast.ai/2018/07/16/auto-ml2/">written previously</a> and <a href="https://slideslive.com/38917533/lessons-learned-from-helping-200000-nonml-experts-use-ml">gave an AutoML workshop keynote</a> on how too often automation ignores the important role of human input. Rather than try to automate everything (which often fails), we should focus on how humans and machines can best work together to take advantage of their different strengths.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2021-11-04-data-disasters/augmentedML.jpg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Speaking about AugmentedML vs.&nbsp;AutoML at ICML 2019</figcaption><p></p>
</figure>
</div>
</section>
<section id="data-work-is-undervalued" class="level2">
<h2 class="anchored" data-anchor-id="data-work-is-undervalued">Data Work is Undervalued</h2>
<p>Interviews of 53 AI practitioners across 6 countries on 3 continents found a pattern that is very familiar to many of us (including me) who work in machine learning: <a href="https://research.google/pubs/pub49953/">“Everyone wants to do the model work, not the data work.”</a> Missing meta-data leads to faulty assumptions. Data collection practices often conflict with the workflows of on-the-ground partners, such as nurses or farmers, who are usually not compensated for this extraneous effort. Too often data work is arduous, invisible, and taken for granted. Undervaluing of data work leads to poor practices and often results in negative, downstream events, including dangerously inaccurate models and months of lost work.</p>
<p>Throughout the pandemic, data about covid (both initial cases and long covid) has often been lacking. Many countries have experienced testing shortages, leading to undercounts of how many people have covid. The <a href="https://www.propublica.org/article/the-cdc-only-tracks-a-fraction-of-breakthrough-covid-19-infections-even-as-cases-surge">CDC decision not to track breakthrough cases</a> unless they resulted in hospitalization made it harder to understand prevalence of break-throughs (a particularly concerning decision since break-throughs can still lead to long covid). In September, it was revealed that British Columbia, Canada was <a href="https://bc.ctvnews.ca/secrecy-over-b-c-s-true-number-of-hospitalized-covid-19-patients-1.5595394">not including covid patients in their ICU counts</a> once the patients were no longer infectious, a secretive decision that obscured how full ICUs were. Some studies of Long Covid have failed to include common symptoms, such as neurological ones, making it harder to understand the prevalence or nature.</p>
</section>
<section id="data-has-context" class="level2">
<h2 class="anchored" data-anchor-id="data-has-context">Data has Context</h2>
<p>Covid is giving us a first-hand view of how data, which we may sometimes want to think of as “objective”, are shaped by countless human decisions and factors. In the example of the symptom tracking app, decisions about which symptoms were included had a significant impact on the prevalence rate calculated. Design decisions that influenced the ease of use impacted how much data was gathered. Lack of understanding of how the app was being used (and why people quit using it) led to erroneous decisions about which cases should be considered “recovered”. These are all examples of the context for data. Here, the data gathered was reasonably appropriate for understanding initial covid infections (a week or two of respiratory symptoms), but not for patients experiencing months of neurological and vascular symptoms. Numbers can not stand alone, we need to understand how they were measured, who was included and excluded, relevant design decisions, under what situations a dataset is appropriate to use vs.&nbsp;not.</p>
<p>As another example, consider covid testing counts: Who has access to testing (this involves health inequities, due to race or urban vs.&nbsp;rural), who is encouraged to get tested (at various times, people without symptoms, children, or other groups have been discouraged from doing so), varying accuracies (e.g.&nbsp;PCR tests are less accurate on children, missing almost half of cases that later go on to seroconvert), and making decisions about what counts as a “case” (I know multiple people who had alternating test results: positive, negative, positive, or the reverse– what counts as a positive case?)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2021-11-04-data-disasters/datasheet.jpg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Datasheet for an electrical component. Image from ‘Datasheets for Datasets’</figcaption><p></p>
</figure>
</div>
<p>One proposal for capturing this context is <a href="https://arxiv.org/abs/1803.09010">Datasheets for Datasets</a>. Prior to doing her PhD at Stanford in computer vision and then co-leading Google’s AI ethics team, <a href="https://thegradient.pub/the-far-reaching-impacts-of-timnit-gebru/">Dr.&nbsp;Timnit Gebru</a> worked at Apple in circuit design and electrical engineering. In electronics, each component (such as a circuit or transistor) comes with a datasheet that lists when and where it was manufactured, under what conditions it is safe to use, and other specifications. Dr.&nbsp;Gebru drew on this background to propose a similar idea for datasets: listing the context of when and how it was created, what data was included/excluded, recommended uses, potential biases and ethical risks, work needed to maintain it, and so on. This is a valuable proposal towards making the context of data more explicit.</p>
</section>
<section id="the-people-most-impacted" class="level2">
<h2 class="anchored" data-anchor-id="the-people-most-impacted">The People Most Impacted</h2>
<p>The inaccurate research and incomplete data from the covid tracking app could have been avoided by drawing on the expertise of patients. Higher quality data could have been collected sooner and more thoroughly, if patients were consulted in the app design and in the related research studies. Participatory approaches to machine learning is an exciting and growing area of research. In any domain, the people who would be most impacted by errors or mistakes need to be included as partners in the design of the project.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2021-11-04-data-disasters/diverse-voices.jpg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">The Diverse Voices project from University of Washington Tech Policy Lab involves academic papers and practical how-to guides.</figcaption><p></p>
</figure>
</div>
<p>Often, our approaches to addressing fairness or other ethics issues, further centralize the power of system designers and operators. The organizers of an ICML workshop on the topic called <a href="https://participatoryml.github.io/">for more cooperative, democratic, and participatory</a> approaches instead. We need to think not just about <strong>explainability</strong>, but about giving people <strong>actionable recourse</strong>. As <a href="https://slideslive.com/38930604/actionable-recourse-in-machine-learning">Professor Berk Ustun highlights</a>, when someone asks why their loan was denied, usually what they want is not just an explanation but to know what they could change in order to get a loan. We need to design systems with <a href="http://contestability.org/"><strong>contestability</strong></a> in mind, to include from the start the idea that people should be able to challenge system outputs. We need to include <a href="https://techpolicylab.uw.edu/project/diverse-voices/">expert panels of perspectives</a> that are often overlooked, depending on the application, this could mean formerly or currently incarcerated people, people who don’t drive, people with very low incomes, disabled people, and many others. <a href="https://techpolicylab.uw.edu/project/diverse-voices/">The Diverse Voices project</a> from University of Washington Tech Lab provides guidance on how to do this. And it is crucial that this not just be tokenistic <a href="https://www.technologyreview.com/2020/08/25/1007589/participation-washing-ai-trends-opinion-machine-learning/">participation-washing</a>, but a meaningful, appropriately compensated, and ongoing role in their design and operation.</p>
</section>
<section id="towards-greater-data-resilience" class="level2">
<h2 class="anchored" data-anchor-id="towards-greater-data-resilience">Towards Greater Data Resilience</h2>
<p>I hope that we can improve data resilience through: - Valuing data work - Documenting context of data - Close contact with the data - Meaningful, ongoing, and compensated involvement of the people impacted</p>
<p>And I hope that when our data represents people we can remember the human side. As <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7296309/">AI researcher Inioluwa Deborah Raji wrote</a>, “Data are not bricks to be stacked, oil to be drilled, gold to be mined, opportunities to be harvested. Data are humans to be seen, maybe loved, hopefully taken care of.”</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2021-11-04-data-disasters/data-not-oil.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Quote from AI researcher Inioluwa Deborah Raji</figcaption><p></p>
</figure>
</div>


</section>

 ]]></description>
  <category>ethics</category>
  <category>machine learning</category>
  <guid>https://rachel.fast.ai/posts/2021-11-04-data-disasters/index.html</guid>
  <pubDate>Wed, 03 Nov 2021 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2021-11-04-data-disasters/covid-track.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Medicine is Political</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2021-10-12-medicine-political/index.html</link>
  <description><![CDATA[ 




<p>Experts warn that we are not prepared for the <a href="https://www.scientificamerican.com/article/a-tsunami-of-disability-is-coming-as-a-result-of-lsquo-long-covid-rsquo/">surge in disability</a> due to long covid, an illness that afflicts between <a href="https://health.ucdavis.edu/health-news/newsroom/studies-show-long-haul-covid-19-afflicts-1-in-4-covid-19-patients-regardless-of-severity/2021/03">one-fourth and one-third</a> of people who get covid, including mild cases, for months afterwards. Some early covid cases have been sick for 18 months, with no end in sight. The physiological damage that covid causes can include <a href="https://onlinelibrary.wiley.com/doi/10.1002/acn3.51350">cognitive dysfunction</a> and <a href="https://www.thelancet.com/journals/eclinm/article/PIIS2589-5370(21)00324-2/fulltext">deficits</a>, <a href="https://www.npr.org/sections/health-shots/2021/07/26/1019875347/doctors-worry-that-memory-problems-after-covid-19-may-set-stage-for-alzheimers">brain activity scans similar to those seen in Alzheimer’s patients</a>, <a href="https://gut.bmj.com/content/70/4/698">GI immune system damage</a>, <a href="https://bjo.bmj.com/content/early/2021/07/08/bjophthalmol-2021-319450">cornea damage</a>, <a href="https://www.medrxiv.org/content/10.1101/2021.06.01.21257759v1">immune dysfunction</a>, <a href="https://jasn.asnjournals.org/content/early/2021/08/25/ASN.2021060734">increased risk of kidney outcomes</a>, <a href="https://www.medrxiv.org/content/10.1101/2021.08.08.21261763v1">dysfunction in T cell memory generation</a>, <a href="https://www.theguardian.com/society/2021/sep/29/covid-can-infect-cells-in-pancreas-that-make-insulin-research-shows">pancreas damage</a>, and <a href="https://pesquisa.bvsalud.org/global-literature-on-novel-coronavirus-2019-ncov/resource/en/covidwho-1265299">ovarian failure</a>. Children are at risk too.</p>
<p>As the evidence continues to mount of alarming long term physiological impacts of covid, and tens of millions are unable to return to work, we might expect leaders to take covid more seriously. Yet we are seeing concerted efforts to downplay the long-term health effects of covid using strategies straight out of the <a href="https://www.sciencedirect.com/science/article/pii/S2352154621000474?via%3Dihub">climate denial playbook</a>, such as <a href="https://blogs.bmj.com/bmj/2021/09/13/covid-19-and-the-new-merchants-of-doubt/">funding contrarian scientists</a>, <a href="https://www.theguardian.com/world/2020/oct/09/herd-immunity-letter-signed-fake-experts-dr-johnny-bananas-covid">misleading petitions</a>, <a href="https://fas.org/blogs/fas/2020/10/social-media-conversations-in-support-of-herd-immunity-are-driven-by-bots/">social media bots</a>, and disingenuous debate tactics that make the science seem murkier than it is. In many cases, these <a href="https://blogs.bmj.com/bmj/2021/09/13/covid-19-and-the-new-merchants-of-doubt/">minimization efforts are being funded</a> by the <a href="https://www.opendemocracy.net/en/dark-money-investigations/tory-billionaire-bankrolled-herd-immunity-scientist-who-advised-pm-against-lockdown/">same billionaires</a> and institutions that fund climate change denialism. Dealing with many millions of newly disabled people will be very expensive for governments, social service programs, private insurance companies, and others. Thus, many have a significant financial interest in distorting the science around long term effects of covid to minimize the perceived impact.</p>
<p>In topics ranging from covid-19 to HIV research to the long history of wrongly assuming women’s illnesses are psychosomatic, we have seen again and again that medicine, like all science, is political. This shows up in myriad ways, such as: who provides funding, who receives that funding, which questions get asked, how questions are framed, what data is recorded, what data is left out, what categories included, and whose suffering is counted.</p>
<p>Scientists often like to think of their work as perfectly objective, perfectly rational, free from any bias or influence. Yet by failing to acknowledge the reality that there is no “view from nowhere”, they miss their own blindspots and make themselves vulnerable to bad-faith attacks. As one <a href="https://twitter.com/KHayhoe/status/1442207481330114567?s=20">climate scientist recounted</a> of the last 3 decades, “We spent a long time thinking we were engaged in an argument about data and reason, but now we realize it’s a fight over money and power… They [climate change deniers] focused their lasers on the science and like cats we followed their pointer and their lead.”</p>
<p>The American Institute for Economic Research (AIER), a libertarian think tank funded by right wing billionaire Charles Koch which invests in fossil fuels, energy utilities, and tobacco, is best known for its research denying the climate crisis. In October 2020, a document called the <a href="https://theconversation.com/5-failings-of-the-great-barrington-declarations-dangerous-plan-for-covid-19-natural-herd-immunity-148975">Great Barrington Declaration (GBD)</a> was developed at a private AIER retreat, calling for a “herd immunity” approach to covid, arguing against lockdowns, and suggesting that young, healthy people have little to worry about. The three scientists who authored the GBD have prestigious pedigrees and are politically well-connected, speaking to White House Officials and having found favor in the British government. One of them, Sunetra Gupta of Oxford, had released a wildly inaccurate <a href="https://www.medrxiv.org/content/10.1101/2020.03.24.20042291v1.full-text">paper in March 2020</a> claiming that <a href="https://www.medscape.com/viewarticle/927504">up to 68% of the UK population</a> had been exposed to covid, and that there were already significant levels of herd immunity to coronavirus in both the UK and Italy (again, this was in March 2020). Gupta received funding from <a href="https://www.opendemocracy.net/en/dark-money-investigations/tory-billionaire-bankrolled-herd-immunity-scientist-who-advised-pm-against-lockdown/">billionaire conservative donors</a>, Georg and Emily von Opel. Another one of the authors, Jay Bhattacharya of Stanford, co-authored a <a href="https://www.buzzfeednews.com/article/stephaniemlee/coronavirus-antibody-test-santa-clara-los-angeles-stanford">widely criticized pre-print</a> in April 2020 that relied on a biased sampling method to “show” that 85 times more people in Santa Clara County California had already had covid compared to other estimates, and thus suggested that the fatality rate for covid was much lower than it truly is.</p>
<p><a href="https://fas.org/blogs/fas/2020/10/social-media-conversations-in-support-of-herd-immunity-are-driven-by-bots/">Half of the social media</a> accounts advocating for herd immunity seem to be bots, characterized as engaging in abnormally high levels of retweets &amp; low content diversity. <a href="https://blogs.bmj.com/bmj/2021/09/13/covid-19-and-the-new-merchants-of-doubt/">An article in the BMJ</a> recently advised that it is “critical for physicians, scientists, and public health officials to realize that they are not dealing with an orthodox scientific debate, but a well-funded sophisticated science denialist campaign based on ideological and corporate interests.”</p>
<p>This myth of perfect scientific objectivity positions modern medicine as completely distinct from a history where women were diagnosed with “hysteria” (roaming uterus) for a variety of symptoms, where Black men were denied syphilis treatment for decades as part of a “scientific study”, and <a href="https://www.theguardian.com/books/2020/sep/10/guardian-australias-book-club-why-does-medicine-care-so-little-about-womens-bodies">multiple sclerosis was</a> “called hysterical paralysis right up to the day they invented a CAT scan machine” and demyelination could be seen on brain scans.</p>
<p>However, there is not some sort of clean break where bias was eliminated and all unknowns were solved. Black patients, <a href="https://pubmed.ncbi.nlm.nih.gov/26366984/">including children</a>, still receive <a href="https://www.scientificamerican.com/article/how-doctors-can-confront-racial-bias-in-medicine/">less pain medication</a> than white patients for the same symptoms. Women are still more likely to have their physical symptoms dismissed as psychogenic. Nearly half of women with autoimmune disorders report being <a href="https://psmag.com/social-justice/is-medicines-gender-bias-killing-young-women">labeled as “chronic complainers”</a> by their doctors in the 5 years (on average) they spend seeking a diagnosis. All this impacts what data is recorded in their charts, what symptoms are counted.</p>
<p>Medical data are not objective truths. Like all data, the context is critical. <a href="https://bostonreview.net/science-nature/rachel-thomas-medicines-machine-learning-problem">It can be missing, biased, and incorrect</a>. It is filtered through the opinions of doctors. Even blood tests and imaging scans are filtered through the decisions of what tests to order, what types of scans to take, what accepted guidelines recommend, what technology currently exists. And the technology that exists depends on research and funding decisions stretching back decades, influenced by politics and cultural context.</p>
<p>One may hope that in 10 years we will have clearer diagnostic tests for some illnesses which remain contested now, just as the ability to identify multiple sclerosis improved with better imaging. In the meantime, we should listen to patients and trust in their ability to explain their own experiences, even if science can’t fully understand them yet.</p>
<p>Science does not just progress inevitably, independent of funding and politics and framing and biases. A self-fulfilling prophecy often occurs in which doctors: 1. label a new, poorly understood, multi-system disease as psychogenic, 2. use this as justification to not invest much funding into researching physiological origins, 3. and then point to the lack of evidence as a reason why the illness must be psychogenic.</p>
<p>This is largely the experience of ME/CFS patients over the last several decades. Myalgic encephalomyelitis (ME/CFS), involves dysfunction of the immune system, autonomic systems, and energy metabolism (including <a href="https://pubmed.ncbi.nlm.nih.gov/32041178/">mitochondrial dysfunction</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/31277442/">hypoacetylation</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/30557887/">reduced oxygen uptake</a>, and <a href="https://portlandpress.com/clinsci/article-abstract/97/5/603/77282/Impaired-oxygen-delivery-to-muscle-in-chronic?redirectedFrom=fulltext">impaired oxygen delivery</a>). ME/CFS is <a href="https://content.iospress.com/articles/work/wor203173#ref019">more debilitating</a> than many chronic diseases, including chronic renal failure, lung cancer, stroke, and type-2 diabetes. It is estimated 25–29% of patients are homebound or bedbound. ME/CFS is often triggered by viral infections, so it is not surprising that we are seeing some <a href="https://www.frontiersin.org/articles/10.3389/fmicb.2021.698169/full">overlap between ME/CFS and long covid</a>. ME/CFS disproportionately impacts women, and a <a href="https://www.mdpi.com/1648-9144/57/10/1012/htm">now discredited 1970 paper</a> identified a major outbreak in 1958 amongst nurses at a British hospital as “epidemic hysteria”. This early narrative of ME/CFS as psychogenic has been difficult to shake. Even as evidence continues to accumulate of immune, metabolic, and autonomous system dysfunction, some doctors persist in believing that ME/CFS must be psychogenic. It has remained <a href="https://content.iospress.com/articles/work/wor203173#ref019">woefully underfunded</a>: from 2013-2017, NIH funding was only at 7.3% relative commensurate to its disease burden. Note that the <a href="https://content.iospress.com/articles/work/wor203173#ref019">below graph</a> is on a log scale: ME/CFS is at 7%, Depression and asthma are at 100% and diseases like cancer and HIV are closer to 1000%.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2021-10-12-medicine-political/mecfsfunding.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Graph of NIH funding on log scale, from above paper by Mirin, Dimmock, Leonard</figcaption><p></p>
</figure>
</div>
<p>Portraying patients as unscientific and irrational is the other side of the same coin for the myth that medicine is perfectly rational. Patients that disagree with having symptoms they know are physiological dismissed as psychogenic, that reject treatments from flawed studies, or who distrust medical institutions based on their experiences of racism, sexism, and mis-diagnosis, are labeled as “militant” or “irrational”, and placed in the same category with conspiracy theorists and those peddling disinformation.</p>
<p>On an individual level, receiving a psychological misdiagnosis lengthens the time it will take to get the right diagnosis, since many doctors will stop looking for physiological explanations. A study of 12,000 rare disease patients <a href="https://www.bbc.com/future/article/20180523-how-gender-bias-affects-your-healthcare">covered by the BBC</a> found that “while being misdiagnosed with the wrong physical disease doubled the time it took to get to the right diagnosis, getting a psychological misdiagnosis extended it even more – by 2.5 up to 14 times, depending on the disease.” This dynamic holds true at the disease level as well: once a disease is mis-labeled as psychogenic, many doctors will stop looking for physiological origins.</p>
<p>We are seeing increasing efforts to dismiss long covid as psychogenic in high profile platforms such as the WSJ and New Yorker. The <a href="https://www.fast.ai/2021/09/25/new-yorker/">New Yorker’s first feature article</a> on long covid, published last month, neglected to interview any clinicians who treat long covid patients nor to cite the abundant research on how covid causes damage to many organ systems, yet interviewed several doctors in unrelated fields who claim long covid is psychogenic. In response to a patient’s assertion that covid impacts the brain, the author spent an entire paragraph detailing how there is currently no evidence that covid crosses the blood-brain barrier, but didn’t mention the research on covid patients finding <a href="https://onlinelibrary.wiley.com/doi/10.1002/acn3.51350">cognitive dysfunction</a> and <a href="https://www.thelancet.com/journals/eclinm/article/PIIS2589-5370(21)00324-2/fulltext">deficits</a>, <a href="https://www.npr.org/sections/health-shots/2021/07/26/1019875347/doctors-worry-that-memory-problems-after-covid-19-may-set-stage-for-alzheimers">PET scans similar to those seen in Alzheimer’s patients</a>, <a href="https://link.springer.com/article/10.1007/s00259-021-05215-4">neurological damage</a>, and <a href="https://www.medrxiv.org/content/10.1101/2021.06.11.21258690v1">shrinking grey matter</a>. This leaves a general audience with the mistaken impression that it is unproven whether covid impacts the brain, and is a familiar tactic from bad-faith science debates.</p>
<p>The New Yorker article set up a strict dichotomy between long covid patients and doctors, suggesting that patients harbor a “disregard for expertise”; are less “concerned about what is and isn’t supported by evidence”; and are overly “impatient.” In contrast, doctors appreciate the “careful study design, methodical data analysis, and the skeptical interpretation of results” that medicine requires. Of course, this is a false dichotomy: many patients are more knowledgeable about the latest research than their doctors, some <a href="https://www.thelancet.com/journals/eclinm/article/PIIS2589-5370(21)00299-6/fulltext">patients are publishing in peer-reviewed journals</a>, and there are many <a href="https://www.nature.com/articles/s43856-021-00016-0">medical doctors that are also patients</a>. And on the other hand, doctors are just as prone as the rest of us to biases, blind spots, and institutional errors.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2021-10-12-medicine-political/actup.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">AP Photo/J. Scott Applewhit</figcaption><p></p>
</figure>
</div>
<p>In 1987, <a href="https://www.npr.org/sections/health-shots/2019/02/09/689924838/how-to-demand-a-medical-breakthrough-lessons-from-the-aids-fight">40,000 Americans had already died of AIDS</a>, yet the government and pharmaceutical companies were doing little to address this health crisis. AIDS was heavily stigmatized, federal spending was minimal, and pharmaceutical companies lacked urgency. The activists of ACT UP used a two pronged approach: creative and confrontational acts of protest, and informed scientific proposals. When the FDA refused to even discuss giving AIDS patients access to experimental drugs, ACT UP protested at their headquarters, blocking entrances and lying down in front of the building with tombstones saying “Killed by the FDA”. This opened up discussions, and <a href="https://www.npr.org/sections/health-shots/2019/02/09/689924838/how-to-demand-a-medical-breakthrough-lessons-from-the-aids-fight">ACT UP offered viable scientific proposals</a>, such as switching from the current approach of conducting drug trials on a small group of people over a long time, and instead testing a large group of people over a short time, radically speeding up the pace at which progress occurred. ACT UP used similar tactics to protest the NIH and pharmaceutical companies, demanding research on how to treat the opportunistic infections that killed AIDS patients, not solely research for a cure. The huge progress that has happened in HIV/AIDS research and treatment would not have happened without the efforts of ACT UP.</p>
<p>Across the world, we are at a pivotal time in determining how societies and governments will deal with the masses of newly disabled people due to long covid. Narratives that take hold early often have disproportionate staying power. Will we inaccurately label long covid as psychogenic, primarily invest in psychiatric research that can’t address the well-documented physiological damage caused by covid, and financially abandon the patients who are now unable to work? Or will we take the chance to transform medicine to better recognize the lived experiences and knowledge of patients, to center patient partnerships in biomedical research for complex and multi-system diseases, and strengthen inadequate disability support and services to improve life for all people with disabilities? The decisions we collectively make now on these questions will have reverberations for decades to come.</p>



 ]]></description>
  <category>science</category>
  <category>ethics</category>
  <guid>https://rachel.fast.ai/posts/2021-10-12-medicine-political/index.html</guid>
  <pubDate>Mon, 11 Oct 2021 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2021-10-12-medicine-political/actup-square.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>11 Short Videos About AI Ethics</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2021-08-17-eleven-ethics-videos/index.html</link>
  <description><![CDATA[ 




<p>I made a playlist of 11 short videos (most are 6-13 mins long) on Ethics in Machine Learning. This is from my <a href="https://course.fast.ai/videos/?lesson=5">ethics lecture</a> in <a href="https://course.fast.ai/">Practical Deep Learning for Coders v4</a>. I thought these short videos would be easier to watch, share, or skip around.</p>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/F0cxzESR7ec" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
</center>
<p><a href="https://www.youtube.com/watch?v=F0cxzESR7ec&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR"><strong>What are Ethics and Why do they Matter? Machine Learning Edition</strong></a>: Through 3 key case studies, I cover how people can be harmed by machine learning gone wrong, why we as machine learning practitioners should care, and what tech ethics are.</p>
<p><a href="https://www.youtube.com/watch?v=j3nqdZoQjy0&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR&amp;index=2"><strong>All machine learning systems need ways to identify &amp; address mistakes</strong></a>. It is crucial that all machine learning systems are implemented with ways to correctly surface and correct mistakes, and to provide recourse to those harmed.</p>
<p><a href="https://www.youtube.com/watch?v=bqCEUQq0z4o&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR&amp;index=3"><strong>The Problem with Metrics, Feedback Loops, and Hypergrowth</strong></a>: Overreliance on metrics is a core problem both in the field of machine learning and in the tech industry more broadly. As Goodhart’s Law tells us, when a measure becomes the target, it ceases to be a good measure, yet the incentives of venture capital push companies in this direction. We see out-of-control feedback loops, widespread gaming of metrics, and people being harmed as a result.</p>
<p><a href="https://www.youtube.com/watch?v=pbnqvS2yjNg&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR&amp;index=4"><strong>Not all types of bias are fixed by diversifying your dataset</strong></a>. The idea of bias is often too general to be useful. There are several different types of bias, and different types require different interventions to try to address them. Through a series of cases studies, we will go deeper into some of the various causes of bias.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2021-08-17-eleven-ethics-videos/short-ethics-playlist.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Part of the Ethics Videos Playlist</figcaption><p></p>
</figure>
</div>
<p><a href="https://www.youtube.com/watch?v=igVrrfDznOo&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR&amp;index=5"><strong>Humans are biased too, so why does machine learning bias matter?</strong></a> A common objection to concerns about bias in machine learning models is to point out that humans are really biased too. This is correct, yet machine learning bias differs from human bias in several key ways that we need to understand and which can heighten the impact.</p>
<p><a href="https://www.youtube.com/watch?v=zoAsnJAsLo8&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR&amp;index=6"><strong>7 Questions to Ask About Your Machine Learning Project</strong></a></p>
<p><a href="https://www.youtube.com/watch?v=PaowVrW3TZg&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR&amp;index=7"><strong>What You Need to Know about Disinformation</strong></a>: With a particular focus on how machine learning advances can contribute to disinformation, this covers some of the fundamental things to understand.</p>
<p><a href="https://www.youtube.com/watch?v=2A6js_EJ6MA&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR&amp;index=8"><strong>Foundations of Ethics</strong></a>: We consider different lenses through which to evaluate ethics, and what sort of questions to ask.</p>
<p><a href="https://www.youtube.com/watch?v=av7utkFXbU4&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR&amp;index=9"><strong>Tech Ethics Practices to Implement at your Workplace</strong></a>: Practical tech ethics practices you can implement at your workplace.</p>
<p><a href="https://www.youtube.com/watch?v=LbNO51E7NUs&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR&amp;index=10"><strong>How to Address the Machine Learning Diversity Crisis</strong></a>: Only 12% of machine learning researchers are women. Based on research studies, I outline some evidence-based steps to take towards addressing this diversity crisis.</p>
<p><a href="https://www.youtube.com/watch?v=IlrP4jqxYR8&amp;list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR&amp;index=11"><strong>Advanced Technology is not a Substitute for Good Policy</strong></a>: We will look at some examples of what incentives cause companies to change their behavior or not (e.g.&nbsp;being warned for years of your role in an escalating genocide vs.&nbsp;threat of a hefty fine), how many AI ethics concerns are actually about human rights, and case studies of what happened when regulation &amp; safety standards came to other industries.</p>
<p>You can find the <a href="https://www.youtube.com/playlist?list=PLtmWHNX-gukIU6V33Bc8eP8OD41I4GywR">playlist of 11 short videos here</a>. And here is a longer, <a href="https://ethics.fast.ai/">full-length free fast.ai course on practical data ethics</a>.</p>



 ]]></description>
  <category>ethics</category>
  <category>machine learning</category>
  <guid>https://rachel.fast.ai/posts/2021-08-17-eleven-ethics-videos/index.html</guid>
  <pubDate>Sun, 15 Aug 2021 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2021-08-17-eleven-ethics-videos/short-ethics-playlist.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Essential Work-From-Home Advice: Cheap and Easy Ergonomic Setups</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2020-08-06-ergonomics/index.html</link>
  <description><![CDATA[ 




<p>You weren’t expecting to spend 2020 working from home. You can’t afford a fancy standing desk. You don’t have a home office, or even much spare space, in your apartment. Your neck is getting a permanent crick from hunching over your laptop on the couch. While those of us who are able to work from home are privileged to have this option, we still don’t want to permanently damage our backs, necks, or arms from a bad ergonomic setup.</p>
<p>This is not a post for ergonomic aficionados (the setups I share could all be further optimized). This is a post for folks who don’t know where to get started, have a limited budget, and are willing to try simple, scrappy approaches. Key takeway: for 34 dollars (21 for a good mouse, and 13 for a cheap keyboard), as well as some household items, you can create an ergonomic setup like the one below. I will show many other options throughout the post, for both sitting and standing, as well as approaches you can easily assemble/disassemble (if you are using the family dinner table and need to clear it off each evening).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2020-08-06-ergonomics/ergonomic1.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption">While visiting family, I created an ergonomic setup on a counter</figcaption><p></p>
</figure>
</div>
<h2 id="damage" class="anchored">
You can permanently damage your body with bad ergonomics
</h2>
<p>You can permanently damage your back, neck, and wrists from working without an ergonomic setup. Almost two decades ago, my partner Jeremy suffered from repetitive stress injury due to working without an ergonomic setup. At the time, his arms were paralyzed and he had to take months off from work. Even now and after years filled with good ergonomics and yoga, this still impacts his life, severely limiting how much time he can spend in cars or on planes, and creating painful flare-ups. Please take this issue seriously.</p>
<h2 id="separate" class="anchored">
Key advice: Have a separate keyboard and mouse
</h2>
<p>The most important thing to know is that you want your screen approximately at eye height, and your elbows at approximately right angles to your torso as they type and use the mouse. This is the case whether you are sitting or standing. If you are using a laptop, this will be impossible with the built-in keyboard and trackpad (no matter how nice they are). It is essential to have a separate keyboard and mouse. If you only do one thing to address ergonomics, obtain a separate keyboard and mouse.</p>
<p>If you can’t afford an external monitor, no worries, you can just elevate your laptop. Over the years, I have used cardboard boxes, drinking glasses, bottles of soda, board games, and stacks of books to elevate my laptop. I will recommend some keyboards and mice that I like below, but anything is better than using the ones built into your laptop (since that forces you to keep your screen at the wrong height). For example, the picture in the intro is of a set-up I created while visiting a family member’s apartment in 2014, using books and a cardboard box to elevate my keyboard, mouse, and laptop to the appropriate heights.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2020-08-06-ergonomics/ergonomic5.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption">For the deep learning study group, I routinely used a brown cardboard box. Bonus: I could store everything in the box when we had the clear out of that room each night.</figcaption><p></p>
</figure>
</div>
<p>Above is a picture from the deep learning study group, which meets 5 days a week, for 7 weeks, every time we run the deep learning course. I use a brown cardboard box to elevate my keyboard. We have to clear out of that conference room each evening, and it is simple for me to put my items in the box. This sort of solution could work if you don’t have a dedicated office space in your home, and need to be able to set up/take down your workstation regularly.</p>
<p>I rarely worked in coffee shops pre-pandemic (and never do now), but when I had to I would still try to create an ergonomic setup (and go to a coffeeshop where there was enough space!). Here, I’ve stacked my laptop on top of my rolled-up backpack. Ideally, my screen would be higher, but this is still better than having it at table level. Don’t let the perfect be the enemy of the good. Every step you take towards a more ergonomic setup is helpful.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2020-08-06-ergonomics/ergonomic2.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption">When working at a coffee shop (pre-pandemic), I brought an external keyboard and mouse, and used my rolled-up backpack to raise the height of my laptop screen</figcaption><p></p>
</figure>
</div>
<h2 id="standing-desks" class="anchored">
About standing desks
</h2>
<p>If you have a regular desk (or even just a table) at home and want a standing desk, one option is to convert it using the <a href="https://alphacolin.com/ikea-standing-desk-for-22-dollars/">$22 standing desk approach</a>, which involves an Ikea side table and shelf. I had a previous job in which this was quite popular. Here is a photo of my work desk from that time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2020-08-06-ergonomics/ergonomic3.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption">In a previous job, many of us set up $22 standing desks using Ikea side tables</figcaption><p></p>
</figure>
</div>
<p>Standing on a hard floor can be difficult for your back. I have a GelPro mat, which I love. If you can’t afford a GelPro mat, standing on a folded-up yoga mat works great too.</p>
<p>Note that standing desks are not a cure-all. I’ve often seen people with expensive standing-desk converters (also known as desktop risers) that still have their monitor way too low. Even if you have an external monitor and desktop riser, makes sure your monitor is at an appropriate height. It is likely you will still need to stack it on top of something. If you don’t like the aesthetics of using books or other household items, you can buy a monitor stand, <a href="https://www.kensington.com/p/products/ergonomic-desk-accessorries/laptop-risers-monitor-stands/kensington-smartfit-monitor-stand-plus-for-up-to-21-screens/">such as this one</a>.</p>
<p>Using a standing desk with poor posture is not very ergonomic, so be cognizant of when you start feeling fatigued. I prefer to switch between standing and sitting throughout the day, as my energy fluctuates.</p>
<h2 id="budget-recs" class="anchored">
Budget Recommendations
</h2>
<p>My “budget recommendation” would be to get an <a href="https://www.newegg.com/p/0TP-005H-00095">Anker vertical mouse</a> for $21 and literally any keyboard. If you have to choose, I’ve found that having a good mouse is way more important than a good keyboard. It is important that you get some keyboard though, so that you can elevate your laptop screen. In the setup below, I’m using a lightweight travel keyboard that isn’t particularly ergonomic, but it works fine.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2020-08-06-ergonomics/ergonomic4.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption">The barista at this coffee shop kindly let me use 2 plastic tubs to prop up my laptop (pre-pandemic).</figcaption><p></p>
</figure>
</div>
<p>I realize that at a time when many Americans do not have enough to eat, that you may not have 34 dollars to spare (21 dollars for a mouse and 13 dollars for a cheap keyboard). However, if this is an option for you, it is well worth the cost. If you permanently damage your back, neck, or arms, no amount of money may be enough to heal them later.</p>
<h2 id="products" class="anchored">
Other products I like
</h2>
<p>My favorite mouse is the <a href="https://www.newegg.com/Logitech-910-001799/p/24W-001B-00008">Logitech wireless trackball mouse</a>. I have also used and liked the <a href="https://www.newegg.com/p/0TP-005H-00095">Anker vertical mouse</a>. For keyboards, I like <a href="https://www.newegg.com/p/12K-00XE-00002">Goldtouch</a> (I use an older version of this one) or the <a href="https://www.newegg.com/microsoft-natural-4000-b2m-00012-usb-wired/p/N82E16823109148">Microsoft Ergonomic Keyboard</a>. And if you are looking for a compact, lightweight travel keyboard, I like the <a href="https://www.iclever.com/products/BK03-Bluetooth-Portable-Keyboard">iClever foldup keyboard</a>.</p>
<p>As mentioned above, <a href="https://www.gelpro.com/">GelPro mats</a> are great if you are going to be standing, and a folded-up yoga mat is a cheaper alternative.</p>
<p>I have a <a href="https://www.therooststand.com/">Roost portable, lightweight laptop stand</a>, which is great, although I can’t use it since I switched from a Macbook Air to a Microsoft Surface Pro. None of the links in this post are affiliate links; I’m just recommending what I’ve personally used and like.</p>
<p>For more about home office set-ups, Jeremy recently posted a twitter thread about his preferred computer set-up (which includes some pricier options). It’s also worth noting that his desk has a small footprint, and fits in the corner of our living room.</p>
<center>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
I couldn't be happier with my little standing desk setup. I have tried far to many products over the years, and here's what I highly recommend:<br>1/ <a href="https://t.co/lMagQPLys1">pic.twitter.com/lMagQPLys1</a>
</p>
— Jeremy Howard (<span class="citation" data-cites="jeremyphoward">@jeremyphoward</span>) <a href="https://twitter.com/jeremyphoward/status/1285747820482318336?ref_src=twsrc%5Etfw">July 22, 2020</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>



 ]]></description>
  <category>advice</category>
  <category>work</category>
  <guid>https://rachel.fast.ai/posts/2020-08-06-ergonomics/index.html</guid>
  <pubDate>Wed, 05 Aug 2020 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2020-08-06-ergonomics/ergonomic1-short.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>4 Principles for Responsible Government Use of Technology</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2020-01-21-responsible-govt/index.html</link>
  <description><![CDATA[ 




<p>As governments consider new uses of technology, whether that be <a href="https://www.citylab.com/transportation/2019/11/firefly-digital-advertising-driver-pay-uber-lyft-cars-data/602077/"><u>sensors on taxi cabs</u></a>, <a href="https://slate.com/technology/2016/11/how-not-to-respond-to-the-next-police-surveillance-technology.html"><u>police body cameras</u></a>, or <a href="https://features.propublica.org/aggression-detector/the-unproven-invasive-surveillance-technology-schools-are-using-to-monitor-students/"><u>gunshot detectors</u></a> in public places, this raises issues around surveillance of vulnerable populations, unintended consequences, and potential misuse. There are several principles to keep in mind in how these decisions can be made in a healthier and more responsible manner. It can be tempting to reduce debates about government adoption of technology into binary for/against narratives, but that fails to capture many crucial and nuanced aspects of these decisions.</p>
<p>We recently hosted the <a href="https://www.sfdatainstitute.org/"><u>Tech Policy Workshop</u></a> at the USF Center for Applied Data Ethics. One of the themes was how governments can promote the responsible use of technology. Here I will share some key recommendations that came out of these discussions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2020-01-21-responsible-govt/govt-headlines.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Headlines of articles related to government use of technology</figcaption><p></p>
</figure>
</div>
<section id="listen-to-local-communities" class="level2">
<h2 class="anchored" data-anchor-id="listen-to-local-communities">Listen to local communities</h2>
<p>There aren’t universal ethical answers that will make sense in every country and culture. Therefore, decisions on technology use should be made in close consultation with local communities. In 2013, Oakland announced plans for a new Domain Awareness Center (DAC), which would implement over 700 cameras throughout schools and public housing, facial recognition software, automated license plate readers (ALPRs), storage capacity for 300 terabytes of data, and a centralized facility with live monitroy. <a href="https://www.nytimes.com/2019/05/15/technology/facial-recognition-san-francisco-ban.html"><u>Brian Hofer</u></a> was an Oakland resident who had never set foot in City Hall prior to this, but he was alarmed by the plans, particularly in light of Edward Snowden’s revelations, which had been released the same month. Together with other citizens and privacy advocates, Hofer was concerned about the intrusiveness of the plans and began attending city council meetings. There were a number of reasons for their concerns, including <a href="https://www.eastbayexpress.com/oakland/the-real-purpose-of-oaklands-surveillance-center/Content?oid=3789230"><u>the discovery</u></a> that city staff had been discussing using DAC to surveil protests and demonstrations. Through the advocacy of local citizens, the plans <a href="https://www.eastbayexpress.com/SevenDays/archives/2014/03/05/oakland-city-council-rolls-back-the-dac"><u>were dramatically scaled back</u></a> and the Oakland Privacy Commission was formed, which continues to provide valuable insight into potential government decisions and purchases.</p>
<p>Sadly, the concerns of local communities are often overridden, in part due to corporate interests and racist stereotypes. For instance, in Detroit, a city that is 79% Black, <a href="https://www.freep.com/story/news/local/michigan/detroit/2019/06/27/detroiters-concerned-over-facial-recognition-technology/1567113001/"><u>citizens protested</u></a> against police use of facial recognition. Yet the city council ended up voting to okay its use, in violation of the police department’s own policy. In contrast, the demographics of cities that have been successful at banning facial recognition are quite different: San Francisco is only 5% Black and Oakland is 25% Black (credit to <a href="https://twitter.com/Combsthepoet/status/1206544760049217537?s=20">Tawana Petty for highlighting these statistics</a>). The racial composition of cities is a significant factor in where and how technology is deployed and used. In another sobering example of the significance of race, <a href="https://www.theverge.com/2016/10/11/13243890/facebook-twitter-instagram-police-surveillance-geofeedia-api"><u>Baltimore Police Department used facial recognition</u></a> to identify people protesting the death of Freddie Gray, a Black man killed in police custody.</p>
</section>
<section id="beware-how-ndas-obscure-public-sector-process-and-law" class="level2">
<h2 class="anchored" data-anchor-id="beware-how-ndas-obscure-public-sector-process-and-law">Beware how NDAs obscure public sector process and law</h2>
<p>In order for citizens to have a voice in the use of technology by their local governments, the first step is that they need to know what technology is being used. Unfortunately, many local governments are shrouded in secrecy on this topic, and they often sign overly strict non-disclosure agreements (NDAs), hiding even the existence of the technology they use. In 2017 New York City passed a measure appointing <a href="https://www.citylab.com/equity/2019/12/ai-technology-computer-algorithm-cities-automated-systems/603349/"><u>a task force on Automated Decision Systems</u></a> to investigate the fairness of software being used by the city and make policy recommendations. However, members of the task force were repeatedly denied their requests for even a basic list of automated systems already in use, with the city claiming that this is proprietary information. When the city released the final report from the commission, many members dissented with it and released their own shadow report in tandem. Meredith Whittaker, a member of the task force and founder of AI Now Institute, <a href="https://www.theverge.com/2019/11/20/20974379/nyc-algorithm-task-force-report-de-blasio"><u>described the city’s failure</u></a> to share relevant information in what could have been a groundbreaking project, “<em>It’s a waste, really. This is a sad precedent.</em>”</p>
<p>The law typically develops through lots of cases over time, explained Elizabeth Joh. However, NDAs often prevent citizens from finding out that a particular technology even exists, much less how it is being used in their city. For instance, cell-site simulators (often referred to as sting-rays), which help police locate a person’s cell phone, were protected by particularly strong NDAs, in which police had to agree that it was better to drop a case than to reveal that a cell-site simulator had been used in apprehending the suspect. How can our law develop when such important details remain hidden? The traditional process of developing and refining our legal system breaks down. “<em>Typically we think we have oversight into what police can do</em>,” <a href="https://theintercept.com/2017/04/30/taser-will-use-police-body-camera-videos-to-anticipate-criminal-activity/"><u>Joh has said</u></a> previously. “<em>Now we have third-party intermediary, they have a kind of privacy shield, they’re not subject to state public record laws, and they have departments sign contracts that they are going to keep this secret</em>.”</p>
</section>
<section id="security-is-not-the-same-as-safety" class="level2">
<h2 class="anchored" data-anchor-id="security-is-not-the-same-as-safety">Security is not the same as safety</h2>
<p><a href="https://www.freep.com/story/news/local/michigan/detroit/2019/06/27/detroiters-concerned-over-facial-recognition-technology/1567113001/"><u>Project Green Light</u></a> is a public-private partnership in Detroit in which high-definition surveillance cameras outside business stream live data to police and are prioritized by police over non-participants. Over 500 businesses are a part of it. This is the largest experiment of facial recognition on a concentrated group of Black people (700,000) to date. <a href="https://www.theguardian.com/us-news/2020/jan/02/california-police-black-stops-force"><u>Black people are disproportionately likely</u></a> to be stopped by police (even though when police search Black, Latino and Native American people, they are less likely to find drugs, weapons or other contraband compared to when they search white people), <a href="https://www.washingtonpost.com/local/public-safety/study-finds-disproportionate-number-of-black-people-arrested-in-dc/2019/05/14/92cf2d26-735a-11e9-8be0-ca575670e91c_story.html"><u>disproportionately likely to be written up on minor infractions</u></a>, and thus <a href="https://www.perpetuallineup.org/"><u>disproportionately likely to have their faces appear</u></a> in police face databases (which are unregulated and not audited for mistakes). This is particularly concerning when combined with knowledge of <a href="https://www.eff.org/deeplinks/2019/02/watching-black-body"><u>America’s long history</u></a> of surveilling and abusing Black communities. While the aims of the program are ostensibly to make Detroit safer, we have to ask, “Safer FOR who? And safer FROM whom?”</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2020-01-21-responsible-govt/project-greenlight.jpeg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Graphic about Detroit’s Project Greenlight, originally from data.detroitmi.gov and shared in <a href="https://detroitcommunitytech.org/?q=content/riverwise-magazine-detroiters-want-be-seen-not-watched">Detroit Riverwise Magazine</a>”</figcaption><p></p>
</figure>
</div>
<p><a href="https://www.honeycombthepoet.org/music.html">Tawana Petty</a> is a poet and social justice organizer who was born and raised in Detroit. She serves as Director of Data Justice Programming for the <a href="https://detroitcommunitytech.org/">Detroit Community Technology Project</a> and co-leads the <a href="https://www.odbproject.org/">Our Data Bodies Project</a>. At the CADE Tech Policy Workshop she shared how Project Green Light makes her feel <em>less</em> safe, and gave a more hopeful example of how to increase safety: give people chairs to sit on their front porches and encourage them to spend more time outside talking with their neighbors. <a href="https://detroitcommunitytech.org/?q=content/riverwise-magazine-detroiters-want-be-seen-not-watched"><u>Myrtle Thompson-Curtis wrote</u></a> about the origins of the idea: in 1980 in Milwaukee “<em>a group of young African Americans remembered how elders would sit on the front porch and keep an eye on them when they were small. These watchful eyes gave them a sense of safety, of being cared for and looked out for by the community. When these youth grew into adulthood, they noticed that no one sat on their porches anymore. Instead people were putting bars on their doors and windows, fearing one another.</em>” Young people went door to door and offered free chairs to neighbors if they would agree to sit on their front porches while children walked to and from school. This program has since been replicated in St.&nbsp;Clair Shores, Michigan, to help defuse racial tensions, and now in Detroit, to illustrate an alternative to the city’s invasive Green Light Surveillance program. “<em>Security is not safety</em>,” Tawana stated, contrasting surveillance with true safety.</p>
<p>Rumman Chowdhury, the leader of the Responsible AI group at Accenture, pointed out that surveillance is often part of a stealth increase in militarization. While on the surface, militarization is sold as improving security, it can often have the opposite effect. Low-trust societies tend to be very militarized, and militarized societies tend to be low-trust. As <a href="https://www.wired.com/story/internet-made-dupes-cynics-of-us-all/"><u>Zeynep Tufekci wrote in Wired</u></a>, sociologists distinguish between high-trust societies (in which people can expect most interactions to work and to have access to due process) and low-trust societies (in which people expect to be cheated and that there is no recourse when you are wronged). In low trust societies, it is harder to make business deals, to find or receive credit, or to forge professional relationships. People in low-trust societies may also be more vulnerable to authoritarian rulers, who promise to impose order. We are already seeing a shift of the internet having gone from a high-trust environment to a low-trust environment, and the use of surveillance may be accelerating this shift in the physical world.</p>
</section>
<section id="policy-decisions-should-not-be-outsourced-as-design-decisions" class="level2">
<h2 class="anchored" data-anchor-id="policy-decisions-should-not-be-outsourced-as-design-decisions">Policy decisions should not be outsourced as design decisions</h2>
<p>When considering <a href="https://www.wbur.org/hereandnow/2017/04/12/axon-free-body-cameras-police"><u>police body cameras</u></a>, there are a number of significant decisions: should the officer be able to turn them on and off at any time? Should the camera have a blinking red light to let people know it is recording? Where should the videos be stored and who should have access to them? Even though these decisions will have a profound impact on the public, they are currently decided by private tech companies. This is just one of the examples Elizabeth Joh shared in illustrating how <strong>what should be policy decisions often end up being determined by corporations as design decisions</strong>. In the case of police body cameras, this lack of choice/control is worsened by the fact that Axon (previously known as Taser) has a monopoly on police-body cameras: since they have a relationship with 17,000 of the 18,000 police departments in the USA, cities may not even have much choice. Vendor-customer relationships influence how police do their jobs and how we can hold them accountable.</p>
<p>Heather Patterson, a privacy researcher at Intel and a member of Oakland’s Privacy Commission, spoke about how tech companies often neglect cities, failing to build products that fit with their needs and requirements, and treating them as an afterthought. In many cases, cities may want to have fewer options or collect less data, which goes against the prevailing tech approach which Mozilla Head of Policy Chris Riley described as “collect now, monetize later, store forever just in case”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2020-01-21-responsible-govt/tech-policy-horizontal.jpg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Some of the many great speakers from our Tech Policy Workshop, who spoke on a variety of topics</figcaption><p></p>
</figure>
</div>
<p>These principles can guide us towards a more responsible use of technology by local governments. Technology can be used for good when it is developed and deployed responsibly, with input from a diverse group of relevant stakeholders, and embedded with the appropriate transparency and accountability.</p>
<p>More responsible government use of technology was just one of the themes discussed at the Tech Policy Workshop. Stay tuned for more resources and insights from the workshop!</p>


</section>

 ]]></description>
  <category>ethics</category>
  <guid>https://rachel.fast.ai/posts/2020-01-21-responsible-govt/index.html</guid>
  <pubDate>Mon, 20 Jan 2020 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2020-01-21-responsible-govt/project-greenlight.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The problem with metrics is a big problem for AI</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2019-09-24-metrics/index.html</link>
  <description><![CDATA[ 




<p><em>Update: This post was expanded into a paper, <a href="https://www.cell.com/patterns/fulltext/S2666-3899(22)00056-3">Reliance on metrics is a fundamental challenge for AI</a>, by Rachel Thomas and David Uminsky, which was accepted to the <a href="https://www.sydney.edu.au/data-science/news-and-events/ethics-of-data-science-conference.html">Ethics of Data Science Conference 2020</a> and to <a href="https://www.cell.com/patterns/fulltext/S2666-3899(22)00056-3">Cell Patterns</a>. The paper version includes more grounding in previous academic work and a framework towards mitigating these harms.</em></p>
<p>Goodhart’s Law states that <em>“When a measure becomes a target, it ceases to be a good measure.”</em> At their heart, what most current AI approaches do is to optimize metrics. The practice of optimizing metrics is not new nor unique to AI, yet AI can be particularly efficient (even <em>too</em> efficient!) at doing so.</p>
<p>This is important to understand, because any risks of optimizing metrics are heightened by AI. While metrics can be useful in their proper place, there are harms when they are unthinkingly applied. Some of the scariest instances of algorithms run amok (such as <a href="https://www.fast.ai/posts/2019-05-28-google-nyt-mohan.html">Google’s algorithm contributing to radicalizing people into white supremacy</a>, <a href="https://www.washingtonpost.com/local/education/creative--motivating-and-fired/2012/02/04/gIQAwzZpvR_story.html">teachers being fired by an algorithm</a>, or <a href="https://www.vice.com/en_us/article/pa7dj9/flawed-algorithms-are-grading-millions-of-students-essays">essay grading software</a> that rewards sophisticated garbage) all result from over-emphasizing metrics. We have to understand this dynamic in order to understand the urgent risks we are facing due to misuse of AI.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-09-24-metrics/metrics.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Headlines from HBR, Washington Post, and Vice on some of the outcomes of over-optimizing metrics: rewarding gibberish essays, promoting propaganda, massive fraud at Wells Fargo, and firing good teachers</figcaption><p></p>
</figure>
</div>
The following principles will be illustrated through a series of case studies:
<ul>
<li>
Any metric is just a proxy for what you really care about
</li>
<li>
Metrics can, and will, be gamed
</li>
<li>
Metrics tend to overemphasize short-term concerns
</li>
<li>
Many online metrics are gathered in highly addictive environments
</li>
<li>
Metrics are most likely to be useful when they are treated as one piece of a bigger picture
</li>
</ul>
<h2 id="proxy" class="anchored">
We can’t measure the things that matter most
</h2>
<p>Metrics are typically just a proxy for what we really care about. The paper <a href="https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf">Does Machine Learning Automate Moral Hazard and Error?</a> covers an interesting example: the researchers investigate which factors in someone’s electronic medical record are most predictive of a future stroke. However, the researchers found that several of the most predictive factors (such as accidental injury, a benign breast lump, or colonoscopy) don’t make sense as risk factors for stroke. So, just what is going on? It turned out that the model was just identifying people who <em>utilize</em> health care a lot. They didn’t actually have data of who had a stroke (a physiological event in which regions of the brain are denied new oxygen); they had data about who had access to medical care, chose to go to a doctor, were given the needed tests, and had this billing code added to their chart. But a number of factors influence this process: who has health insurance or can afford their co-pay, who can take time off of work or find childcare, gender and racial biases that impact who gets accurate diagnoses, cultural factors, and more. As a result, the model was largely picking out people who utilized healthcare versus who did not.</p>
<p>This an example of the common phenomenon of having to use proxies: You want to know what content users like, so you measure what they click on. You want to know which teachers are most effective, so you measure their students test scores. You want to know about crime, so you measure arrests. These things are not the same. Many things we <em>do</em> care about can not be measured. Metrics can be helpful, but we can’t forget that they are just proxies.</p>
<p>As another example, Google used hours spent watching YouTube as a proxy for how happy users were with the content, <a href="https://youtube-creators.googleblog.com/2012/08/youtube-now-why-we-focus-on-watch-time.html">writing on the Google blog</a> that <em>“If viewers are watching more YouTube, it signals to us that they’re happier with the content they’ve found.”</em> Guillaume Chaslot, an AI engineer who formerly worked at Google/YouTube, shares how this had the <a href="https://medium.com/@guillaumechaslot/how-algorithms-can-learn-to-discredit-the-media-d1360157c4fa">side effect of incentivizing conspiracy theories</a>, since convincing users that the rest of the media is lying kept them watching more YouTube.</p>
<h2 id="gamed" class="anchored">
Metrics can, and will, be gamed
</h2>
<p>It is almost inevitable that metrics will be gamed, particularly when they are given too much power. One week this spring, Chaslot collected 84,695 videos from YouTube and analyzed the number of views and the number of channels from which they were recommended. This is <a href="https://twitter.com/gchaslot/status/1121603851675553793?s=20">what he found</a> (also covered in <a href="https://www.washingtonpost.com/technology/2019/04/26/youtube-recommended-russian-media-site-above-all-others-analysis-mueller-report-watchdog-group-says/">the Washington Post</a>):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-09-24-metrics/YTRT.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Chart showing Russia Today’s video on the Mueller Report as being an outlier in how many YouTube channels recommended it.</figcaption><p></p>
</figure>
</div>
<p>The state-owned media outlet Russia Today was an extreme outlier in how much YouTube’s algorithm had selected it to be recommended by a wide-variety of other YouTube channels. Such algorithmic selections, which begin autoplaying as soon as your current video is done, account for 70% of the time that users spend on YouTube. This chart strongly suggests that Russia Today has in some way gamed YouTube’s algorithm. Platforms are rife with attempts to game their algorithms, to show up higher in search results or recommended content, through fake clicks, fake reviews, fake followers, and more.</p>
<p><a href="https://www.vice.com/en_us/article/pa7dj9/flawed-algorithms-are-grading-millions-of-students-essays">Automatic essay grading software</a> focuses primarily on metrics like sentence length, vocabulary, spelling, and subject-verb agreement, but is unable to evaluate aspects of writing that are hard to quantify, such as creativity. As a result, gibberish essays randomly generated by computer programs to contain lots of sophisticated words score well. Essays from students in mainland China, which do well on essay length and sophisticated word choice, received higher scores from the algorithms than from expert human graders, suggesting that these students may be using chunks of pre-memorized text.</p>
<p>As USA education policy began over-emphasizing student test scores as the primary way to evaluate teachers, there have been <a href="https://www.nytimes.com/2010/06/11/education/11cheat.html">widespread scandals</a> of teachers and principals cheating by altering students scores, in Georgia, Indiana, Massachusetts, Nevada, Virginia, Texas, and elsewhere. One consequence of this is that <strong>teachers who don’t cheat may be penalized or <a href="https://www.washingtonpost.com/local/education/creative--motivating-and-fired/2012/02/04/gIQAwzZpvR_story.html">even fired</a></strong> (when it appears student test scores have dropped to more average levels under their instruction). When metrics are given undue importance, attempts to game those metrics become common.</p>
<h2 id="short-term" class="anchored">
Metrics tend to overemphasize short-term concerns
</h2>
<p>It is much easier to measure short-term quantities: click through rates, month-over-month churn, quarterly earnings. Many long-term trends have a complex mix of factors and are tougher to quantify. What is the long-term impact on user trust of having your brand associated with <a href="https://www.nytimes.com/2019/06/03/world/americas/youtube-pedophiles.html">promoting pedophilia</a>, <a href="https://www.theverge.com/2018/9/19/17876892/youtube-extremism-report-rebecca-lewis-data-society">white supremacy</a>, and <a href="https://www.bbc.com/news/technology-47279253">flat-earth theories</a>? What is the long-term impact on hiring to be the subject of years worth of <a href="https://www.wired.com/story/why-zuckerberg-15-year-apology-tour-hasnt-fixed-facebook/">privacy scandals</a>, <a href="https://www.vox.com/policy-and-politics/2019/1/22/18177076/social-media-facebook-far-right-authoritarian-populism">political manipulation</a>, and <a href="https://www.fast.ai/posts/2017-11-02-ethics.html">facilitating genocide</a>?</p>
<p>Simply measuring what users click on is a short-term concern, and does not take into account factors like the potential long-term impact of a long-form investigative article which may have taken months to research and which could help shape a reader’s understanding of a complex issue and even lead to significant societal changes.</p>
<p>A recent <a href="https://hbr.org/2019/09/dont-let-metrics-undermine-your-business">Harvard Business Review article</a> looked at Wells Fargo as a case study of how letting metrics replace strategy can harm a business. After identifying cross-selling as a measure of long-term customer relationships, Wells Fargo went overboard emphasizing the cross-selling metric: intense pressure on employees combined with an unethical sales culture led to 3.5 million fraudulent deposit and credit card accounts being opened without customers’ consent. The metric of cross-selling is a much more short-term concern compared to the loftier goal of nurturing long-term customer relationships. Overemphasizing metrics removes our focus from long-term concerns such as our values, trust and reputation, and our impact on society and the environment, and myopically focuses on the short-term.</p>
<h2 id="addictive" class="anchored">
Many metrics gather data of what we do in highly addictive environments
</h2>
<p>It matters which metrics we gather and in what environment we do so. Metrics such as what users click on, how much time they spend on sites, and “engagement” are heavily relied on by tech companies as proxies for user preference, and are used to drive important business decisions. Unfortunately, these metrics are gathered in environments engineered to be highly addictive, <a href="https://twitter.com/random_walker/status/1143601343279579137?s=20">laden with dark patterns</a>, and where financial and design decisions have already greatly circumscribed the range of options.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-09-24-metrics/junkfood.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Our online environment is a buffet of junk food</figcaption><p></p>
</figure>
</div>
<p>Zeynep Tufekci, a professor at UNC and regular contributor to the New York Times, compares recommendation algorithms (such as YouTube choosing which videos to auto-play for you and Facebook deciding what to put at the top of your newsfeed) to a <a href="https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth">cafeteria shoving junk food</a> into children’s faces. <em>“This is a bit like an autopilot cafeteria in a school that has figured out children have sweet teeth, and also like fatty and salty foods. So you make a line offering such food, automatically loading the next plate as soon as the bag of chips or candy in front of the young person has been consumed.”</em> As those selections get normalized, the output becomes ever more extreme: <em>“So the food gets higher and higher in sugar, fat and salt – natural human cravings – while the videos recommended and auto-played by YouTube get more and more bizarre or hateful.”</em> Too many of our online environments are like this, with metrics capturing that we love sugar, fat, and salt, not taking into account that we are in the digital equivalent of a <a href="https://en.wikipedia.org/wiki/Food_desert">food desert</a> and that companies haven’t been required to put nutrition labels on what they are offering. Such metrics are not indicative of what we would prefer in a healthier or more empowering environment.</p>
<h2 id="better" class="anchored">
When Metrics are Useful
</h2>
<p>All this is not to say that we should throw metrics out altogether. Data can be valuable in helping us understand the world, test hypotheses, and move beyond gut instincts or hunches. Metrics can be useful when they are in their proper context and place. One way to keep metrics in their place is to consider a slate of many metrics for a fuller picture (and resist the temptation to try to boil these down to a single score). For instance, knowing the rates at which tech companies hire people from under-indexed groups is a very limited data point. For evaluating diversity and inclusion at tech companies, we need to know comparative promotion rates, cap table ownership, retention rates (many tech companies are revolving doors driving people from under-indexed groups away with their toxic cultures), number of harassment victims silenced by NDAs, rates of under-leveling, and more. Even then, all this data should still be combined with <strong>listening to first-person experiences</strong> of those working at these companies.</p>
<p>Columbia professor and New York Times Chief Data Scientist <a href="https://datascience.columbia.edu/ethical-principles-okrs-and-kpis-what-youtube-and-facebook-could-learn-tukey">Chris Wiggins wrote</a> that quantitative measures should always be combined with qualitative information, <em>“Since we can not know in advance every phenomenon users will experience, we can not know in advance what metrics will quantify these phenomena. To that end, data scientists and machine learning engineers must partner with or learn the skills of user experience research, giving users a voice.”</em></p>
<p>Another key to keeping metrics in their proper place is to keep domain experts and those who will be most impacted closely involved in their development and use. Surely most teachers could have foreseen that evaluating teachers primarily on the standardized test scores of their students would lead to a host of negative consequences.</p>
<p>I am not opposed to metrics; I am alarmed about the harms caused when metrics are overemphasized, a phenomenon that we see frequently with AI, and which is having a negative, real-world impact. AI running unchecked to optimize metrics has led to Google/YouTube’s heavy promotion of white supremacist material, essay grading software that rewards garbage, and more. By keeping the risks of metrics in mind, we can try to prevent these harms.</p>



 ]]></description>
  <category>machine learning</category>
  <category>ethics</category>
  <guid>https://rachel.fast.ai/posts/2019-09-24-metrics/index.html</guid>
  <pubDate>Mon, 23 Sep 2019 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2019-09-24-metrics/metrics.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>8 Things You Need to Know about Surveillance</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2019-08-07-surveillance/index.html</link>
  <description><![CDATA[ 




<p>Over 225 police departments have partnered with Amazon to have access to Amazon’s video footage obtained as part of the “smart” doorbell product Ring, and in many cases these partnerships are <a href="https://www.vice.com/en_us/article/d3ag37/us-cities-are-helping-people-buy-amazon-surveillance-cameras-using-taxpayer-money">heavily subsidized with taxpayer money</a>. Police departments are allowing Amazon <a href="https://gizmodo.com/cops-are-giving-amazons-ring-your-real-time-911-data-1836883867">to stream 911 call</a> information directly in real-time, and Amazon requires police departments to <a href="https://gizmodo.com/everything-cops-say-about-amazons-ring-is-scripted-or-a-1836812538">read pre-approved scripts</a> when talking about the program. If a homeowner doesn’t want to share data from their video camera doorbell with police, an officer for the Fresno County Sheriff’s Office said <a href="https://gizmodo.com/amazons-ring-is-teaching-cops-how-to-persuade-customers-1837000515">they can just go directly to Amazon</a> to obtain it. This creation of an extensive surveillance network, the murky private-public partnership surrounding it, and a lack of any sort of regulations or oversight is frightening. And this is just one of many examples related to surveillance technology that have recently come to light.</p>
<p>I frequently talk with people who are not that concerned about surveillance, or who feel that the positives outweigh the risks. Here, I want to share some important truths about surveillance:</p>
<ol>
<li>
Surveillance can facilitate human rights abuses and even genocide
</li>
<li>
Data is often used for different purposes than why it was collected
</li>
<li>
Data often contains errors
</li>
<li>
Surveillance typically operates with no accountability
</li>
<li>
Surveillance changes our behavior
</li>
<li>
Surveillance disproportionately impacts the marginalized
</li>
<li>
Data privacy is a public good
</li>
<li>
We don’t have to accept invasive surveillance
</li>
</ol>
<p>While I was writing this post, a number of investigative articles came out with disturbing new developments related to surveillance. I decided that rather than attempt to include everything in one post (which would make it too long and too dense), I would go ahead and share the above facts about surveillance, as they are just a relevant as ever.</p>
<center>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
The last 24 hours:<br>- NYC police using facial recognition on 11 year old kids<br>- Cops are giving Amazon real-time 911 caller data<br>- California Facial Recognition Interconnect<br>- contd facial rec on protesters in Hong Kong<br>- Palantir founder Peter Thiel gets op-ed in NYTimes
</p>
— Rachel Thomas (<span class="citation" data-cites="math_rachel">@math_rachel</span>) <a href="https://twitter.com/math_rachel/status/1157357983036137473?ref_src=twsrc%5Etfw">August 2, 2019</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>
<h2 id="genocide" class="anchored">
<ol type="1">
<li>Surveillance can facilitate human rights abuses and even genocide
</li></ol></h2>

<p><a href="https://www.theengineroom.org/dangerous-data-the-role-of-data-collection-in-genocides/">There is a long history</a> of data about sensitive attributes being misused, including the use of the 1940 USA Census to intern Japanese Americans, a system of identity cards introduced by the Belgian colonial government that were later used during the 1994 Rwandan genocide (in which nearly a million people were murdered), and the <a href="https://www.huffpost.com/entry/ibm-holocaust_b_1301691">role of IBM in helping Nazi Germany</a> use punchcard computers to identify and track the mass killing of millions of Jewish people. More recently, the mass internment of over one million people who are part of an ethnic minority in Western China was facilitated through <a href="https://www.wired.com/story/inside-chinas-massive-surveillance-operation/">the use of a surveillance network</a> of cameras, biometric data (including images of people’s faces, audio of their voices, and blood samples), and phone monitoring.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-08-07-surveillance/ibm-hitler.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Adolf Hitler meeting with IBM CEO Tom Watson Sr.&nbsp;in 1937. Source: https://www.computerhistory.org/revolution/punched-cards/2/15/109</figcaption><p></p>
</figure>
</div>
<p>Pictured above is Adolf Hitler (far left) meeting with IBM CEO Tom Watson Sr.&nbsp;(2nd from left), shortly before Hitler awarded Watson a special “Service to the Reich” medal in 1937 (<a href="https://www.jewishgen.org/ForgottenCamps/General/TimeEng.html">for a timeline of the Holocaust, see here</a>). Watson returned the medal in 1940, although IBM continued to do business with the Nazis. IBM technology helped the Nazis conduct detailed censuses in countries they occupied, to thoroughly identify anyone of Jewish descent. <a href="https://www.huffpost.com/entry/ibm-holocaust_b_1301691">Nazi concentration camps used IBM’s punchcard machines</a> to tabulate prisoners, recording whether they were Jewish, gay, or Gypsies, and whether they died of “natural causes,” execution, suicide, or via “special treatment” in gas chambers. It is not the case that IBM sold the machines and then was done with it. Rather, IBM and its subsidiaries provided regular training and maintenance on-site at the concentration camps: printing off cards, configuring machines, and repairing them as they broke frequently.</p>
<h2 id="purposes" class="anchored">
<ol start="2" type="1">
<li>Data is often used for different purposes than why it was collected
</li></ol></h2>

<p>In the above examples, the data collection began before genocide was committed. IBM began selling to Nazi Germany well before the Holocaust (although continued for far too long), including helping with Germany’s 1933 census conducted by Adolf Hitler, which was effective at identifying far more Jewish people than had previously been recognized in Germany.</p>
<p>It is important to recognize how data and images gathered through surveillance can be weaponized later. Columbia professor <a href="https://www.nytimes.com/2019/04/10/opinion/sunday/privacy-capitalism.html">Tim Wu wrote</a> that <em>“One [hard truth] is that data and surveillance networks created for one purpose can and will be used for others. You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.”</em></p>
<p>Plenty of data collection is not involved with such extreme abuse as genocide; however, in a time of global resurgence of white supremacist, ethno-nationalist, and authoritarian movements, it would be deeply irresponsible to not consider how data &amp; surveillance can and will be weaponized against already vulnerable groups.</p>
<h2 id="errors" class="anchored">
<ol start="3" type="1">
<li>Data often has errors (and no mechanism for correcting them)
</li></ol></h2>

<p>A <a href="https://www.latimes.com/local/lanow/la-me-ln-calgangs-audit-20160811-snap-story.html">database of suspected gang members</a> maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). Even worse, there was no process in place for correcting mistakes or removing people once they’ve been added.</p>
<p>An <a href="https://www.washingtonpost.com/posteverything/wp/2016/09/08/how-the-careless-errors-of-credit-reporting-agencies-are-ruining-peoples-lives/">NPR reporter recounts his experience</a> of trying to rent an apartment and discovering that TransUnion, one of the 3 major credit bureaus, incorrectly reported him as having two felony firearms convictions. TransUnion only removed the mistakes after a dozen phone calls and notification that the story would be reported on. This is not an unusual story: the FTC’s large-scale study of credit reports in 2012 found 26% of consumers had at least one mistake in their files and 5% had errors that could be devastating. An even more opaque, <a href="https://www.washingtonpost.com/business/economy/little-known-firms-tracking-data-used-in-credit-scores/2011/05/24/gIQAXHcWII_story.html?utm_term=.19efcc7df056">unregulated “4th bureau” exists</a>: a collection of companies buying and selling personal information about people on the margins of the banking system (such as immigrants, students, and people with low incomes), with no standards on what types of data are included, no way to opt out, and no system for identifying or correcting mistakes.</p>
<h2 id="accountability" class="anchored">
<ol start="4" type="1">
<li>Surveillance typically operates with no accountability
</li></ol></h2>

<p>What makes the examples in the previous section disturbing is not just that errors occurred, but that there was no way to identify or correct them, and no accountability for those profiting off the error-laden data. Often, even the existence of the systems being used is not publicly known (much less details of how these systems work), unless discovered by journalists or revealed by whistleblowers. The <a href="https://www.metrotimes.com/news-hits/archives/2019/07/29/bipartisan-panel-why-detroits-facial-recognition-technology-should-be-banned">Detroit Police Dept</a> used facial recognition technology for nearly two years without public input and in violation of a requirement that a policy be approved by the city’s Board of Police Commissioners, until a <a href="https://www.americaunderwatch.com/">study from Georgetown Law’s Center for Privacy &amp; Technology</a> drew attention to the issue. Palantir, the defense startup founded by billionaire Peter Thiel, ran a program with <a href="https://www.theverge.com/2018/2/27/17054740/palantir-predictive-policing-tool-new-orleans-nopd">New Orleans Police Department for 6 years</a> which city council members did not even know about, much less have any oversight.</p>
<p>After two studies found that Amazon’s facial recognition software produced <a href="https://www.nytimes.com/2018/07/26/technology/amazon-aclu-facial-recognition-congress.html">inaccurate</a> and <a href="https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender">racially biased results</a>, Amazon countered that the researchers should have changed the default parameters. However, it turned out that <a href="https://gizmodo.com/defense-of-amazons-face-recognition-tool-undermined-by-1832238149">Amazon was not instructing police departments</a> that use its software to do this either. Surveillance programs are operating with few regulations, no oversight, no accountability around accuracy or mistakes, and in many cases, no public knowledge of what is going on.</p>
<h2 id="behavior" class="anchored">
<ol start="5" type="1">
<li>Surveillance changes our behavior
</li></ol></h2>

<p>Hundreds of thousands of people in <a href="https://www.bbc.com/news/world-asia-china-48656471">Hong Kong are protesting</a> an unpopular new bill which would allow extradition to China. Typically, Hong Kong locals use their rechargeable smart cards to ride the subway. However, during the protests, <a href="https://qz.com/1642441/extradition-law-why-hong-kong-protesters-didnt-use-own-metro-cards/">long lines of people</a> waited to use cash to buy paper tickets (usually something that only tourists do) concerned that they would be tracked for having attended the protests. Would fewer people protest if this was not an option?</p>
<p>In the United States, in 2015 the <a href="https://www.theverge.com/2016/10/11/13243890/facebook-twitter-instagram-police-surveillance-geofeedia-api">Baltimore Police Department used facial recognition</a> technology to surveil people protesting the death of Freddie Grey, a young Black man who was killed in police custody, and arrested protesters with outstanding warrants. Mass surveillance could have a chilling impact on our rights to move about freely, to express ourselves, and to protest. <em>“We act differently when we know we are ‘on the record.’ Mass privacy is the freedom to act without being watched and thus in a sense, to be who we really are,”</em> Columbia professor <a href="https://www.nytimes.com/2019/04/10/opinion/sunday/privacy-capitalism.html">Tim Wu wrote</a> in the New York Times.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-08-07-surveillance/geofeedia.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Flyer from the company Geofeedia. Source: https://www.aclunc.org/docs/20161011_geofeedia_baltimore_case_study.pdf</figcaption><p></p>
</figure>
</div>
<h2 id="uneven" class="anchored">
<ol start="6" type="1">
<li>Surveillance disproportionately impacts those who are already marginalized
</li></ol></h2>

<p>Surveillance is applied unevenly, causing the greatest harm to people who are already marginalized, including immigrants, people of color, and people living in poverty. These groups are more heavily policed and surveilled. <a href="https://www.law.georgetown.edu/privacy-technology-center/publications/the-perpetual-line-up/">The Perpetual Line-Up</a> from the Georgetown Law Center on Privacy and Technology studied the unregulated use of facial recognition by police, with half of all Americans appearing in law enforcement databases, and the risks of errors, racial bias, misuses, and threats to civil liberties. The researchers pointed out that African Americans are <a href="https://www.perpetuallineup.org/findings/racial-bias">more likely to appear</a> in these databases (many of which are drawn from mug shots) since they are disproportionately likely to be stopped, interrogated, or arrested. For another example, consider the contrast between how easily people over 65 can apply for Medicare benefits by filling out an online form, with <a href="https://tcf.org/content/report/disparate-impact-surveillance/">the invasive personal questions</a> asked of a low-income mother on Medicaid about her lovers, hygiene, parental shortcomings, and personal habits.</p>
<p>In an article titled <a href="https://www.fastcompany.com/90317495/another-tax-on-the-poor-surrendering-privacy-for-survival">Trading privacy for survival is another tax on the poor</a>, Ciara Byrne wrote, <em>“Current public benefits programs ask applicants extremely detailed and personal questions and sometimes mandate home visits, drug tests, fingerprinting, and collection of biometric information… Employers of low-income workers listen to phone calls, conduct drug tests, monitor closed-circuit television, and require psychometric tests as conditions of employment. Prisoners in some states have to consent to be voiceprinted in order to make phone calls.”</em></p>
<h2 id="aggregate" class="anchored">
<ol start="7" type="1">
<li>Data privacy is a public good, like air quality or safe drinking water
</li></ol></h2>

<p>Data is more revealing in aggregate. It can be nearly impossible to know what your individual data could reveal when combined with the data of others or with data from other sources, or when machine learning inference is performed on it. For instance, as <a href="https://www.nytimes.com/2018/01/30/opinion/strava-privacy.html">Zeynep Tufekci wrote</a> in the New York Times, individual Strava users could not have predicted how in aggregate their data could be used to identify the locations of US military bases. <em>“Data privacy is not like a consumer good, where you click ‘I accept’ and all is well. Data privacy is more like air quality or safe drinking water, a public good that cannot be effectively regulated by trusting in the wisdom of millions of individual choices. A more collective response is needed.”</em></p>
<p>Unfortunately, this also means that you can’t fully safeguard your privacy on your own. You may choose not to purchase Amazon’s ring doorbell, yet you can still show up in the video footage collected by others. You might strengthen your online privacy practices, yet conclusions will still be inferred about you based on the behavior of others. As Professor Tufekci wrote, <strong>we need a collective response</strong>.</p>
<h2 id="hope" class="anchored">
<ol start="8" type="1">
<li>We don’t have to accept invasive surveillance
</li></ol></h2>

<p>Many people are uncomfortable with surveillance, but feel like they have no say in the matter. While the threats surveillance poses are large, it is not too late to act. We are seeing success: in response to community organizing and an audit, Los Angeles Police Department <a href="https://www.latimes.com/local/lanow/la-me-lapd-predictive-policing-big-data-20190405-story.html">scrapped a controversial program</a> to predict who is most likely to commit violent crimes. <a href="https://www.metrotimes.com/news-hits/archives/2019/07/29/bipartisan-panel-why-detroits-facial-recognition-technology-should-be-banned">Citizens, researchers, and activists in Detroit</a> have been effective at drawing attention to the Detroit Police Department’s unregulated use of facial recognition and a bill calling for a 5-year moratorium has been introduced to the state legislature. Local governments in <a href="https://www.bbc.com/news/technology-48276660">San Francisco</a>, <a href="https://www.sfchronicle.com/bayarea/article/Oakland-bans-use-of-facial-recognition-14101253.php">Oakland</a>, and <a href="https://www.bostonglobe.com/metro/2019/06/27/somerville-city-council-passes-facial-recognition-ban/SfaqQ7mG3DGulXonBHSCYK/story.html">Somerville</a> have banned the use of facial recognition by police.</p>
<p>For further resources, please check out: - <a href="https://www.law.georgetown.edu/privacy-technology-center/">Georgetown Law Center on Privacy and Technology</a> - <a href="https://www.odbproject.org/tools/">Digital Defense Playbook</a></p>



 ]]></description>
  <category>ethics</category>
  <guid>https://rachel.fast.ai/posts/2019-08-07-surveillance/index.html</guid>
  <pubDate>Tue, 06 Aug 2019 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2019-08-07-surveillance/ibm-hitler.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Advice for Better Blog Posts</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2019-05-13-blogging-advice/index.html</link>
  <description><![CDATA[ 




<p>A blog is like a resume, only better. I’ve been invited to give keynote talks based on my posts, and I know of people for whom blog posts have led to job offers. I’ve encouraged people to start blogging in <a href="http://www.fast.ai/2017/04/06/alternatives/">several of</a> <a href="https://rachel.fast.ai/posts/2017-12-18-personal-brand/">my previous</a> <a href="https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045">posts</a>, and I even required students in my <a href="https://github.com/fastai/numerical-linear-algebra">computational linear algebra course</a> to write a blog post (although they weren’t required to publish it), since good technical writing skills are useful in the workplace and in interviews. Also, explaining something you’ve learned to someone else is a way to cement your knowledge. I gave a list of tips for <a href="https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045">getting started with your first blog post</a> previously, and I wanted to offer some more advanced advice here.</p>
<section id="who-is-your-audience" class="level2">
<h2 class="anchored" data-anchor-id="who-is-your-audience">Who is your audience?</h2>
<p>Advice that my speech coach gave me about <a href="https://rachel.fast.ai/posts/2017-12-18-personal-brand/">preparing talks</a>, which I think also applies to writing, is to choose one particular person that you can think of as your target audience. <strong>Be as specific as possible.</strong> It’s great if this is a real person (and it is totally fine if they are not actually going to read your post or attend your talk), although it doesn’t have to be (you just need to be extra-thorough in making up details about them if it’s not). Either way, what is their background? What sort of questions or misconceptions might they have about the topic? At various points, the person I’m thinking of has been a friend or colleague, one of my students, or my younger self.</p>
<p>Being unclear about your audience can lead to a muddled post: for instance, I’ve seen blog posts that contain both beginner material (e.g.&nbsp;defining what training and test sets are) as well as very advanced material (e.g.&nbsp;describing complex new architectures). Experts would be bored and beginners would get lost.</p>
</section>
<section id="dos-and-donts" class="level2">
<h2 class="anchored" data-anchor-id="dos-and-donts">Dos and Don’ts</h2>
<p>When you read other people’s blog posts, think about what works well. What did you like about it? And when you read blog posts that you don’t enjoy as much, think about why not? What would make the post more engaging for you? Note that not every post will appeal to every person. Part of having a target audience means that there are people who are not in your target audience, which is fine. And sometimes I’m not somebody else’s target audience. As with all advice, this is based on my personal experience and I’m sure that there are exceptions.</p>
<section id="things-that-often-work-well" class="level3">
<h3 class="anchored" data-anchor-id="things-that-often-work-well">Things that often work well:</h3>
<ul>
<li>Bring together many useful resources (but don’t include everything! the value is in <strong>your curation</strong>)</li>
<li>Do provide motivation and context. If you are going to explain how an algorithm works, first give some examples of real-world applications where it is used, or how it is different from other options.</li>
<li>People are convinced by several different things: stories, statistics, research, and visuals. Try using a blend of these.</li>
<li>If you’re using a lot of code, try writing in a Jupyter notebook (which can be <a href="https://cduvallet.github.io/posts/2018/03/ipython-notebooks-jekyll">converted into a blog post</a>) or a <a href="https://www.kaggle.com/kernels">Kaggle Kernel</a>.</li>
</ul>
</section>
<section id="donts" class="level3">
<h3 class="anchored" data-anchor-id="donts">Don’ts</h3>
<ul>
<li><strong>Don’t reinvent the wheel.</strong> If you know of a great explanation of something elsewhere, link to it! Include a quote or one sentence summary about the resource you’re linking to.</li>
<li><strong>Don’t try to build everything up from first principles.</strong> For example, if you want to explain the transformer architecture, don’t begin by defining machine learning. Who is your target audience? People already familiar with machine learning will lose interest, and those who are brand new to machine learning are probably not seeking out posts on the transformer architecture. You can assume that your reader already has a certain background (sometimes it is helpful to make this explicit).</li>
<li><strong>Don’t be afraid to have an opinion.</strong> For example, TensorFlow (circa 2016, before eager execution) <a href="https://twitter.com/math_rachel/status/821044526571614208">made me feel unintelligent</a>, even though everyone else seemed to be saying how awesome it was. I was pretty nervous <a href="https://www.fast.ai/2017/01/03/keras/">writing a blog post</a> that said this, but a lot of people responded positively.</li>
<li><strong>Don’t be too dull or dry.</strong> If people lose interest, they will stop reading, so you want to hook them (and keep them hooked!)</li>
<li><strong>Don’t plagiarize.</strong> Always cite sources, and use quote marks around direct quotes. Do this even as you are first gathering sources and taking notes, so you don’t make a mistake later and forget which material is someone else’s. It is wrong to plagiarize the work of others and ultimately will hurt your reputation. Cite and link to people who have given you ideas.</li>
<li><strong>Don’t be too general.</strong> You don’t have to cover everything on a topic– focus on the part that interests (or frustrates) you most.</li>
</ul>
</section>
</section>
<section id="put-the-time-in-to-do-it-well" class="level2">
<h2 class="anchored" data-anchor-id="put-the-time-in-to-do-it-well">Put the time in to do it well</h2>
<p>As DeepMind researcher and University of Oxford PhD student <a href="https://hackernoon.com/interview-with-deep-learning-researcher-and-leader-of-openmined-andrew-trask-77cd33570a8c">Andrew Trask advised</a>, “<em>The secret to getting into the deep learning community is high quality blogging… Don’t just write something ok, either—take 3 or 4 full days on a post and try to make it as short and simple (yet complete) as possible.</em>” Honestly, I’ve spent far more than 3 or 4 days on <a href="https://rachel.fast.ai/posts/2018-04-29-categorical-embeddings/">many of</a> <a href="https://www.fast.ai/posts/2017-03-01-changing-careers.html">my most</a> <a href="https://rachel.fast.ai/posts/2018-07-23-automl3/">popular</a> <a href="https://rachel.fast.ai/posts/2018-07-12-automl1/">posts</a>.</p>
<p>However, this doesn’t mean that you need to be a “naturally gifted” writer. I attended a poor, public high school in a small city in Texas, where I had few writing assignments and didn’t really learn to write a proper essay. An introductory English class my first semester of college highlighted how much I struggled with writing, and after that, I tried to avoid classes that would require much writing (part of the reason I studied math and computer science is that those were the only fields I knew of that involved minimal writing AND didn’t have lab sessions). It wasn’t until I was in my 30s and <a href="https://medium.com/tech-diversity-files/if-you-think-women-in-tech-is-just-a-pipeline-problem-you-haven-t-been-paying-attention-cb7a2073b996">wanted to start blogging</a> that I began to practice writing. I typically go through many, many drafts, and do lots of revisions. As with most things, <a href="https://hbr.org/2007/07/the-making-of-an-expert">skill is not innate</a>; it is something you build through deliberate practice.</p>
<p>Note: I realize many people may not have time to blog– perhaps you are a parent, dealing with chronic illness, suffering burnout from a toxic job, or prefer to do other things in your free time– that’s alright! You can still have a successful career without blogging, this post is only for those who are interested.</p>
</section>
<section id="write-a-blog-version-of-your-academic-paper" class="level2">
<h2 class="anchored" data-anchor-id="write-a-blog-version-of-your-academic-paper">Write a blog version of your academic paper</h2>
<p>The top item on my wish list for AI researchers is that more of them would write blog posts to accompany their papers:</p>
<center>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
my wish list for AI researchers <a href="https://t.co/Cel5x32K9O">https://t.co/Cel5x32K9O</a> <a href="https://t.co/AyYBqwYDFX">pic.twitter.com/AyYBqwYDFX</a>
</p>
— Rachel Thomas (<span class="citation" data-cites="math_rachel">@math_rachel</span>) <a href="https://twitter.com/math_rachel/status/983874014392152064?ref_src=twsrc%5Etfw">April 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>
<p>Far more people may read your blog post than will read an academic paper. This is a chance to get your message to a broader audience, in a more conversational and accessible format. You can and should link to your academic paper from your blog post, so there’s no need to worry about including all the technical details. People will read your paper if they want more detail!</p>
<p>Check out these excellent pairs of academic papers and blog posts for inspiration: - <a href="http://gendershades.org/overview.html">Gender Shades</a> (blog post &amp; visualization) and <a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification</a> (paper), by Joy Buolamwini &amp; Timnit Gebru - <a href="https://medium.com/@harinisuresh/the-problem-with-biased-data-5700005e514c">The Problem with “Biased Data”</a> (blog post) and <a href="https://arxiv.org/abs/1901.10002">A Framework for Understanding Unintended Consequences of Machine Learning</a> (paper), by Harini Suresh &amp; John Guttag - <a href="http://nlp.fast.ai/">Introducing state of the art text classification with universal language models</a> (blog post) and <a href="https://arxiv.org/abs/1801.06146">Universal Language Model Fine-tuning for Text Classification</a> (paper), by Jeremy Howard &amp; Sebastian Ruder</p>
<p>I usually advise new bloggers that your target audience could be you-6-months-ago. For grad students, you may need to change this to you-2-years-ago. Assume that unlike your paper reviewers, the reader of your blog post has not read the related research papers. Assume your audience is intelligent, but not in your subfield. What does it take to explain your research to a friend in a different field?</p>
</section>
<section id="getting-started-with-your-first-post" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-your-first-post">Getting Started with your first post</h2>
<p>Here are some tips I’ve <a href="https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045">shared previously</a> to help you start your first post:</p>
<ul>
<li>Make a list of links to other blog posts, articles, or studies that you like, and write brief summaries or highlight what you particularly like about them. Part of my first blog post came from my making just such a list, because I couldn’t believe more people hadn’t read the posts and articles that I thought were awesome.</li>
<li>Summarize what you learned at a conference you attended, or in a class you are taking.</li>
<li>Any email you’ve written twice should be a blog post. Now, if I’m asked a question that I think someone else would also be interested in, I try to write it up.</li>
<li>You are best positioned to help people one step behind you. The material is still fresh in your mind. Many experts have forgotten what it was like to be a beginner (or an intermediate) and have forgotten why the topic is hard to understand when you first hear it.</li>
<li>What would have helped you a year ago? What would have helped you a week ago?</li>
<li>If you’re wondering about the actual logistics, <a href="https://medium.com/new-story">Medium</a> makes it super simple to get started. Another option is to use <a href="https://help.github.com/articles/using-jekyll-as-a-static-site-generator-with-github-pages/">Jekyll and Github pages</a>. I can personally recommend both, as I have 2 blogs and use one for each (my <a href="https://medium.com/@racheltho">other blog is here</a>).</li>
</ul>
</section>
<section id="related-posts" class="level2">
<h2 class="anchored" data-anchor-id="related-posts">Related Posts</h2>
<ul>
<li><a href="https://rachel.fast.ai/posts/2017-12-18-personal-brand/">Making Peace with Personal Branding</a></li>
<li><a href="https://www.fast.ai/posts/2017-04-06-alternatives.html">Alternatives to a Degree to Prove Yourself in Deep Learning</a></li>
<li><a href="https://www.fast.ai/posts/2018-04-10-stanford-salon.html">A Discussion about Accessibility in AI at Stanford</a></li>
</ul>


</section>

 ]]></description>
  <category>advice</category>
  <guid>https://rachel.fast.ai/posts/2019-05-13-blogging-advice/index.html</guid>
  <pubDate>Sun, 12 May 2019 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2019-05-13-blogging-advice/blog.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Tech’s Long Hours Are Discriminatory and Counterproductive</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2019-02-12-long-hours/index.html</link>
  <description><![CDATA[ 




<section id="one-third-of-workers-are-ill-or-disabled-and-this-industry-is-shutting-them-out" class="level2">
<h2 class="anchored" data-anchor-id="one-third-of-workers-are-ill-or-disabled-and-this-industry-is-shutting-them-out">One-third of workers are ill or disabled — and this industry is shutting them out</h2>
<p>Whether you realize it or not, you are likely interacting with ill or disabled people regularly. According to <a href="https://www.talentinnovation.org/_private/assets/DisabilitiesInclusion_KeyFindings-CTI.pdf">recent survey data</a>, a high portion of the U.S. workforce reports having a disability (30 percent), even though a much smaller percentage says they’ve self-identified as disabled to their employer (only 3.2 percent). Often, these illnesses and disabilities are impossible for others to observe, so many people choose to keep their conditions a secret from managers and co-workers to avoid discrimination.</p>
<p><a href="https://medium.com/@racheltho/the-tech-industry-is-failing-people-with-disabilities-and-chronic-illnesses-8e8aa17937f3">Health is not binary</a>; it can fluctuate and is subjective. I have experienced a number of health challenges, including having brain surgery twice (once while pregnant) and one life-threatening brain infection (which can take years to recover from). Trust me when I say that you can’t assess someone’s health based on their appearance or mood. And yet, <a href="https://hbr.org/2017/12/the-case-for-improving-work-for-people-with-disabilities-goes-way-beyond-compliance">over one-third</a> of people with disabilities say they have experienced negative bias in their current job.</p>
<p>I work in the <a href="https://medium.com/@racheltho/the-tech-industry-is-failing-people-with-disabilities-and-chronic-illnesses-8e8aa17937f3">tech industry</a>, where there is an overt glorification — and in many cases, a requirement — of working unhealthily long hours. This is in spite of <a href="https://hbr.org/2015/08/the-research-is-clear-long-hours-backfire-for-people-and-for-companies">research</a> showing that putting in longer hours doesn’t lead to greater productivity and instead is harmful. And when you’re ill or disabled and working in this field, the long hours can be not just counterproductive but discriminatory.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-02-12-long-hours/8hoursday_banner_1856.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A banner from 1856 reads, “8 hours labour, 8 hours recreation, 8 hours rest.” Source: Wikimedia</figcaption><p></p>
</figure>
</div>
</section>
<section id="fewer-hours-in-the-day" class="level2">
<h2 class="anchored" data-anchor-id="fewer-hours-in-the-day">Fewer hours in the day</h2>
<p>Many people with chronic illnesses or disabilities simply have fewer hours in the day. We may need more sleep than comparatively healthy people — and yet still wake up feeling awful — as well as have to carefully budget limited energy. Conditions often require frequent doctor visits, blood tests, MRIs, physical therapy, and other appointments, plus there’s dealing with the administrative burden of managing scheduling, billing, and insurance claims, all of which frequently involve errors.</p>
<p>In <a href="https://noendinsight.co/2018/12/13/episode-11-john/">an episode</a> of the podcast <em>No End in Sight</em>, which is focused on chronic illness, a front-end software engineer named John pinpointed his experience feeling time-crunched. John has bipolar disorder and Fabry disease, a rare genetic disorder that causes reduced kidney function and chronic pain and requires him to get regular IV infusion treatments. He described being told during a job interview at Microsoft that he needed to spend more of his free time coding:</p>
<blockquote class="blockquote">
<p>I really felt looked down on as being lazy. And really, I’m not lazy. I have chronic illness, and I’m trying to do the best — like, I’m not trying to push myself too hard because I don’t want to throw myself into a bipolar tailspin. And I also don’t want to hurt my hands and have it be even worse to type… I was told by this abled person how to go about living assuming that I was abled, and it was just really frustrating. I’ve contributed at least a thousand hours to open source, and I’m supposed to just keep doing more. When does it end?</p>
</blockquote>
<p>Natasha Walton, who founded the <a href="https://medium.com/tech-disability-project/my-flexible-work-schedule-was-accidentally-accommodating-7d42528eedc2">Tech Disability Project</a>, has fibromyalgia and post-traumatic stress disorder. She <a href="https://twitter.com/nlmooney/status/999730372781879296">noted on Twitter</a> that certain aspects of the day, like sleep and fitness routines, are not optional for her. “They account for the time I spend meeting my body’s basic needs each and every day so that I can participate in the wider world,” she explained.</p>
<p>The tech work environment is hostile even to healthy people. The “ideal worker” in tech is in perfect health, child-free, and has no other commitments. I’ve had several jobs in tech that I could do for a time, and even do quite well, but that I knew would be unsustainable for me long-term. The question was not if I would burn out, but when. Numerous co-workers have also seemed on the brink of burnout regardless of whether they had a chronic illness. I even have tech-industry friends who developed permanent chronic illnesses while in toxic work environments.</p>
<p>There are companies where people like me would not be welcome based on unreasonable employee demands. Last year, Andrew Ng’s deeplearning.ai posted a controversial job ad that not only specified that employees typically spend 70–90 hours per week working and studying (later changed to <a href="https://www.reddit.com/r/MachineLearning/comments/70vuj5/d_twitter_thread_on_andrew_ngs_transparent/">70+ hours</a>), but that doing so is the natural consequence of believing you can change the world. Many companies operate on this assumption, even if most are not quite so frank about it.</p>
<p>Elon Musk posted a <a href="https://twitter.com/elonmusk/status/1067175527180513280">declaration</a> that to change the world, people need to work 80 hours per week, peaking above 100 at times. Uber formerly had an explicit <a href="https://www.buzzfeednews.com/article/carolineodonovan/how-ubers-hard-charging-corporate-culture-left-employees">company value</a> to “work harder, longer, and smarter” and served dinner at 8:15 p.m. “Working seven days a week, sometimes until 1 or 2 a.m., was considered normal,” <a href="https://www.buzzfeednews.com/article/carolineodonovan/how-ubers-hard-charging-corporate-culture-left-employees">said</a> one former employee. A <em>New York Times</em> <a href="https://www.nytimes.com/2015/08/16/technology/inside-amazon-wrestling-big-ideas-in-a-bruising-workplace.html">article</a> about Amazon described “marathon conference calls on Easter Sunday and Thanksgiving, criticism from bosses for spotty Internet access on vacation, and hours spent working at home most nights or weekends,” as well as employees being given low-performance ratings directly after cancer treatment, major surgeries, or giving birth to a stillborn child.</p>
</section>
<section id="the-research-on-productivity" class="level2">
<h2 class="anchored" data-anchor-id="the-research-on-productivity">The research on productivity</h2>
<p>As much as possible, we need to get away from the shallow idea that the quantity of time worked is what matters. The tech industry’s obsession with ridiculously long hours is not only inaccessible to many disabled people and harmful to everyone’s health and relationships, but as Olivia Goldhill <a href="https://qz.com/work/1486863/boasting-about-how-many-hours-you-work-is-a-sign-of-failure/">pointed out</a> for <em>Quartz at Work</em>, research on productivity suggests it’s just inefficient:</p>
<blockquote class="blockquote">
<p>As countless studies have shown, this simply isn’t true. Productivity dramatically decreases with longer work hours, and completely drops off once people reach 55 hours of work a week, to the point that, on average, someone working 70 hours in a week achieves no more than a colleague working 15 fewer hours.</p>
</blockquote>
<p>Alex Soojung-Kim Pang’s book <a href="https://www.indiebound.org/book/9780465074877"><em>Rest</em></a> covers the crucial role that leisure time and downtime play in our creativity, health, and productivity. Prolific, talented figures including Charles Darwin, Henri Poincaré, G.H. Hardy, mathematician Paul Halmos, Charles Dickens, and many others were known to engage in only four or five hours of highly concentrated work per day. Pang also highlights an overlooked aspect of the “rule” popularized by Malcolm Gladwell that to become an expert takes 10,000 hours of practice. Gladwell based it on psychologist K. Anders Ericsson’s <a href="http://projects.ict.usc.edu/itw/gel/EricssonDeliberatePracticePR93.pdf">study</a> of top musical performers, but Pang observes that the top performers also slept more and took afternoon naps:</p>
<blockquote class="blockquote">
<p>We’ve come to believe that world-class performance comes after 10,000 hours of practice. But that’s wrong. It comes after 10,000 hours of deliberate practice, 12,500 hours of deliberate rest, and 30,000 hours of sleep.</p>
</blockquote>
<p>More support for rest-boosted productivity is detailed in a <em>Harvard Business Review</em> <a href="https://hbr.org/2015/08/the-research-is-clear-long-hours-backfire-for-people-and-for-companies">roundup</a> titled “The Research Is Clear: Long Hours Backfire for People and for Companies.” It highlights a variety of other study results:</p>
<ul>
<li>Managers could not tell the difference between employees who worked 80-hour weeks and those who pretended to — though they still penalized employees who were open about working less.</li>
<li>Overwork is linked to impaired sleep, and <a href="https://hbr.org/2006/10/sleep-deficit-the-performance-killer">sleep deprivation</a> has long been known to lengthen reaction time, interfere with problem-solving, and even induce an impairment equivalent to being drunk.</li>
<li>Depression, heavy drinking, diabetes, memory problems, heart disease, and poorer judgment calls are all repercussions tied to being overworked.</li>
<li>Predictable, required time off (like nights and weekends) make teams more productive.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-02-12-long-hours/timekeeping.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Roman timekeeping: Four clocks show the amount of night and day at times of the year. Image from Wikimedia</figcaption><p></p>
</figure>
</div>
</section>
<section id="the-importance-of-flexible-work-environments" class="level2">
<h2 class="anchored" data-anchor-id="the-importance-of-flexible-work-environments">The importance of flexible work environments</h2>
<p>Accommodations, even simple ones, can mean a world of difference to employees with illnesses or disabilities. Brianne Benness, founder of the <em>No End in Sight</em> podcast mentioned above, <a href="https://medium.com/tech-disability-project/my-flexible-work-schedule-was-accidentally-accommodating-7d42528eedc2">has written</a> about how a flexible job with remote work helped her stay employed during her at-the-time undiagnosed illness: “When I woke up in a lot of pain, I could tell my boss I was working from home… When the pain in my neck made it too distracting to sit at my desk, I could move to a couch and lie down with my head supported.”</p>
<p>But when Brianne switched to a job with a more rigid in-office schedule, both her health and work level plummeted. With nowhere to lie down with her laptop, she would not only get distracted by the pain but put her focus on trying to seem productive. Meanwhile, she lost energy fast during the day, struggled with brain fog, and would go to bed as soon as she got home. She nailed the conundrum in explaining, “I know that when my brain is firing on all cylinders, I can get more done in five hours than I can get done in a full week when my brain is plodding. But I don’t know how to share that value with an employer.”</p>
<blockquote class="blockquote">
<p>The tech industry problems of exclusion, discrimination, and unhealthy work habits run deep, but there is also a widespread appetite for change.</p>
</blockquote>
<p>Some business leaders and employers are recognizing the value on their own. A <em>Harvard Business Review</em> <a href="https://hbr.org/2014/01/to-raise-productivity-let-more-employees-work-from-home">article</a> details an experiment by Stanford professor Nicholas Bloom and Ctrip travel website cofounder James Liang in which they let half of Ctrip’s employees work from home for nine months. They found that the group working from home was both more productive and only half as likely to quit as other employees. Bloom said he was blown away by the results, and the benefits of flexible work were much greater than he expected.</p>
<p>Massive employers like PricewaterhouseCoopers are experimenting, too. PwC is the second largest professional services firm in the world, and last year, it <a href="https://www.employeebenefits.co.uk/issues/august-2018/pwc-flexible-working-recruitment-programme/">announced</a> a new flexible work program in which potential employees can choose how many hours per week or how many months per year they are available to work. It seems to have created a valuable appeal in recruiting. When one of the company’s leaders, Anne Donovan, <a href="https://hbr.org/2019/01/what-pwc-learned-from-its-policy-of-flexible-work-for-everyone">shared her advice</a> on switching to a more flexible culture, she asserted that everyone deserves the same degree of flexibility and that culture comes from the top.</p>
</section>
<section id="we-have-a-long-way-to-go" class="level2">
<h2 class="anchored" data-anchor-id="we-have-a-long-way-to-go">We have a long way to go</h2>
<p>In <a href="https://www.juliaferraioli.com/presos/writing-accessible-go/">her keynote</a> at GopherCon, Google developer advocate Julia Ferraioli pointed out that, in tech, we often make products accessible but not the processes or the teams we use to build them. True inclusion means having disabled people on your team; the people creating the technology need to be representative of the people using technology, which, increasingly, is everyone.</p>
<p>In previous posts, I have shared extensive research on how gender and racial bias manifest in the tech industry, including in <a href="https://medium.com/tech-diversity-files/if-you-think-women-in-tech-is-just-a-pipeline-problem-you-haven-t-been-paying-attention-cb7a2073b996">retention</a><strong>,</strong> <a href="https://medium.com/tech-diversity-files/the-real-reason-women-quit-tech-and-how-to-address-it-6dfb606929fd">promotions</a><strong>,</strong> <a href="https://medium.com/tech-diversity-files/donations-and-women-in-tech-panels-are-not-a-diversity-strategy-do-better-c3c51022a916">onboarding</a>, and <a href="https://medium.com/@racheltho/how-to-make-tech-interviews-a-little-less-awful-c29f35431987">hiring</a>. But in researching bias around disability in the field, I found far less, and this is in part because tech companies aren’t tracking it. When a <em>TechCrunch</em> reporter, who has a severe disability, asked Intel, Apple, Twitter, Facebook, Slack, Google, and Salesforce why none of them <a href="https://techcrunch.com/2016/11/07/parallel-pr-universe/">included disability</a> in their diversity reports, the companies gave evasive, off-the-record responses. Since the zeroth step to increasing inclusion is to understand the scope and details of the problem, this is an indicator we have a long way to go.</p>
<p>The ideas are out there, though. Ted Kennedy Jr., an attorney and state senator who lost his leg as a child due to cancer, recently <a href="https://www.nytimes.com/2018/12/27/opinion/disability-rights-employment.html">wrote about</a> common, straightforward themes among companies that are inclusive of people with disabilities:</p>
<ul>
<li>They hire people with disabilities.</li>
<li>They encourage and advance those employees.</li>
<li>They provide accessible tools and technology and have a formal accommodations program.</li>
<li>They empower those employees with mentoring and coaching initiatives (Note: Not all mentorship is the same. <a href="https://hbr.org/2010/09/why-men-still-get-more-promotions-than-women">Research</a> has shown that public endorsement of a mentee’s authority and championing their ideas is far more effective than advice on how the mentee should change and gain self-knowledge.)</li>
</ul>
<p>I certainly can’t and don’t speak for everyone with chronic illnesses or disabilities, and I encourage you to listen and read the accounts of others. The tech industry problems of exclusion, discrimination, and unhealthy work habits run deep, but there is also a widespread appetite for change. Reconsider the culture at your workplace by hiring and promoting people with disabilities, de-emphasizing hours spent working in favor of quality of work, and allowing a more flexible setup.</p>
<p><em>This post was originally published Feb 12, 2019 on Medium. This is a follow-up to an_ <a href="https://medium.com/@racheltho/the-tech-industry-is-failing-people-with-disabilities-and-chronic-illnesses-8e8aa17937f3">earlier post</a> I wrote on how the tech industry is failing people with disabilities and chronic illnesses.</em></p>
<p><em>Thank you to Julia Ferraioli, Jeremy Howard, and Negar Rostamzadeh for giving me valuable feedback on earlier drafts of this post.</em></p>


</section>

 ]]></description>
  <category>inclusion</category>
  <category>work</category>
  <guid>https://rachel.fast.ai/posts/2019-02-12-long-hours/index.html</guid>
  <pubDate>Mon, 18 Feb 2019 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2019-02-12-long-hours/overwork.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Five Things That Scare Me About AI</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2019-01-29-five-scary-things/index.html</link>
  <description><![CDATA[ 




<p>AI is being increasingly used to make important decisions. Many AI experts (including <a href="https://www.recode.net/2016/8/13/12467506/google-brain-jeff-dean-ama-reddit-artificial-intelligence-robot-takeover">Jeff Dean</a>, head of AI at Google, and <a href="https://www.theregister.co.uk/2015/03/19/andrew_ng_baidu_ai/">Andrew Ng</a>, founder of Coursera and deeplearning.ai) say that warnings about sentient robots are overblown, but other harms are not getting enough attention. I agree. I am an AI researcher, and <a href="https://www.youtube.com/watch?v=LqjP7O9SxOM">I’m worried</a> about some of the societal impacts that we’re already seeing. In particular, these 5 things scare me about AI:</p>
<ol>
<li>
Algorithms are often implemented without ways to address mistakes.
</li>
<li>
AI makes it easier to not feel responsible.
</li>
<li>
AI encodes &amp; magnifies bias.
</li>
<li>
Optimizing metrics above all else leads to negative outcomes.
</li>
<li>
There is no accountability for big tech companies.
</li>
</ol>
<p>At the end, I’ll briefly share some positive ways that we can try to address these.</p>
<p>Before we dive in, I need to clarify one point that is important to understand: <strong>algorithms (and the complex systems they are a part of) can make mistakes</strong>. These mistakes come from a variety of sources: bugs in the code, inaccurate or biased data, approximations we have to make (e.g.&nbsp;you want to measure health and you use hospital readmissions as a proxy, or you are interested in crime and use arrests as a proxy. These things are related, but not the same), misunderstandings between different stakeholders (policy makers, those collecting the data, those coding the algorithm, those deploying it), how computer systems interact with human systems, and more.</p>
<p>This article discusses a variety of algorithmic systems. I don’t find debates about definitions particularly interesting, including what counts as “AI” or if a particular algorithm qualifies as “intelligent” or not. Please note that the dynamics described in this post hold true both for simpler algorithms, as well as more complex ones.</p>
<h2 id="appeals" class="anchored">
<ol type="1">
<li>Algorithms are often implemented without ways to address mistakes.
</li></ol></h2>

<p>After the state of <a href="https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy">Arkansas implemented software</a> to determine people’s healthcare benefits, many people saw a drastic reduction in the amount of care they received, but were given <strong>no explanation and no way to appeal</strong>. Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week, transforming her life for the worse. Eventually, <strong>a lengthy court case uncovered errors</strong> in the software implementation, and Tammy’s hours were restored (along with those of many others who were impacted by the errors).</p>
<p>Observations of 5th grade teacher Sarah Wysocki’s classroom yielded positive reviews. Her <a href="https://www.washingtonpost.com/local/education/creative--motivating-and-fired/2012/02/04/gIQAwzZpvR_story.html">assistant principal wrote</a>, “<em>It is a pleasure to visit a classroom in which the elements of sound teaching, motivated students and a positive learning environment are so effectively combined</em>.” Two months later, <strong>she was fired by an opaque algorithm, along with over 200 other teachers</strong>. The head of the PTA and a parent of one of Wyscoki’s students described her as “<em>One of the best teachers I’ve ever come in contact with. Every time I saw her, she was attentive to the children, went over their schoolwork, she took time with them and made sure.</em>” That people are losing needed healthcare without an explanation or being fired without explanation is truly dystopian!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-01-29-five-scary-things/no-accountability.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Headlines from the Verge and the Washington Post</figcaption><p></p>
</figure>
</div>
<p>As I covered in a <a href="https://www.fast.ai/posts/2018-08-07-hbr-bias-algorithms.html">previous post</a>, people use outputs from algorithms differently than they use decisions made by humans: - Algorithms are more likely to be implemented with <strong>no appeals process</strong> in place. - Algorithms are often used <strong>at scale</strong>. - Algorithmic systems are <strong>cheap</strong>. - People are more likely to assume algorithms are <strong>objective or error-free</strong>. As <a href="https://www.youtube.com/watch?v=TRzBk_KuIaM">Peter Haas said</a>, “<em>In AI, we have Milgram’s ultimate authority figure,</em>” referring to Stanley Milgram’s <a href="https://en.wikipedia.org/wiki/Milgram_experiment">famous experiments</a> showing that most people will obey orders from authority figures, even to the point of harming or killing other humans. How much more likely will people be to trust algorithms perceived as objective and correct?</p>
<p>There is a lot of overlap between these factors. If the main motivation for implementing an algorithm is cost-cutting, adding an appeals process (or even diligently checking for errors) may be considered an “unnecessary” expense. Cathy O’Neill, who earned her math PhD at Harvard, wrote a book <a href="https://weaponsofmathdestructionbook.com/">Weapons of Math Destruction</a>, in which she covers how algorithms are disproportionately impacting poor people, whereas the privileged are more likely to still have access to human attention (in hiring, education, and more).</p>
<h2 id="responsible" class="anchored">
<ol start="2" type="1">
<li>AI makes it easier to not feel responsible.
</li></ol></h2>

<p>Let’s return to the case of the <a href="https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy">buggy software used to determine health benefits</a> in Arkansas. How could this have been prevented? In order to prevent severely disabled people from mistakenly losing access to needed healthcare, we need to talk about responsibility. Unfortunately, <strong>complex systems lend themselves to a dynamic in which nobody feels responsible for the outcome</strong>.</p>
<p>The creator of the algorithm for healthcare benefits, Brant Fries (who has been earning royalties off this algorithm, which is in use in over half the 50 states), blamed state policy makers. I’m sure the state policy makers could blame the implementers of the software. When asked if there should be a way to communicate how the algorithm works to the disabled people losing their healthcare, Fries callously said, “<em>It’s probably something we should do. Yeah, I also should probably dust under my bed</em>,” and then later clarified that he thought it was someone else’s responsibility.</p>
<p>This passing of the buck and failure to take responsibility is common in many bureaucracies. As <a href="https://www.youtube.com/watch?v=NTl0yyPqf3E">danah boyd observed</a>, <em>“Bureaucracy has often been used to shift or evade responsibility. Who do you hold responsible in a complex system?”</em> Boyd gives the examples of high-ranking bureaucrats in Nazi Germany, who did not see themselves as responsible for the Holocaust. boyd continues, <em>“Today’s algorithmic systems are extending bureaucracy.”</em></p>
<p>Another example of nobody feeling responsible comes from the case of <a href="http://www.sciencemag.org/news/2018/02/artificial-intelligence-could-identify-gang-crimes-and-ignite-ethical-firestorm">research to classify gang crime</a>. A database of gang members assembled by the Los Angeles Police Department (and 3 other California law enforcement agencies) was <a href="https://www.latimes.com/local/lanow/la-me-ln-calgangs-audit-20160811-snap-story.html">found to have 42 babies</a> who were under the age of 1 when added to the gang database (28 were said to have admitted to being gang members). Keep in mind these are just some of the most obvious errors- we don’t know how many other people were falsely included.</p>
<p>I don’t bring this up for the primary purpose of pointing fingers or casting blame. However, a world of complex systems in which nobody feels responsible for the outcomes (which can include severely disabled people losing access to the healthcare they need, or innocent people being labeled as gang members) is not a pleasant place. <strong>Our work is almost always a small piece of a larger whole, yet a sense of responsibility is necessary to try to address and prevent negative outcomes.</strong></p>
<h2 id="bias" class="anchored">
<ol start="3" type="1">
<li>AI encodes &amp; magnifies bias.
</li></ol></h2>

<p><em>But isn’t algorithmic bias just a reflection of how the world is?</em> I get asked a variation of this question <a href="https://www.youtube.com/watch?v=WC1kPtG8Iz8&amp;index=5&amp;t=0s&amp;list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6">every time</a> <a href="https://www.youtube.com/watch?v=LqjP7O9SxOM">I give a</a> <a href="https://www.infoq.com/presentations/unconscious-bias-machine-learning">talk about bias</a>. To which my answer is: No, <strong>our algorithms and products impact the world and are part of feedback loops</strong>. Consider an algorithm to predict crime and determine where to send police officers: sending more police to a particular neighhorhood is <strong>not just an effect, but also a cause</strong>. More police officers can lead to more arrests in a given neighborhood, which could cause the algorithm to send even more police to that neighborhood (a mechanism described in <a href="https://arxiv.org/abs/1706.09847">this paper</a> on runaway feedback loops).</p>
<p>Bias is being encoded and <a href="https://www.wired.com/story/machines-taught-by-photos-learn-a-sexist-view-of-women/">even magnified</a> in a variety of applications: - software used to <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">decide prison sentences</a> that has twice as high a false positive rate for Black defendents as for white defendents - computer vision software from <a href="https://www.nytimes.com/2019/01/24/technology/amazon-facial-technology-study.html">Amazon</a>, <a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">Microsoft, and IBM</a> performs significantly worse on people of color</p>
<p>[Research by Joy Buolamwini and Timnit Gebru found that commercial computer vision software performed significantly worse on women with dark skin. Gendershades.org]{gendershades3.png){width=60%}</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=25nC0n9ERq4">Word embeddings</a>, which are a building block for language tools like Gmail’s SmartReply and Google Translate, generate useful analogies such as Rome:Italy :: Madrid:Spain, as well as <a href="https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/">biased analogies</a> such as man:computer programmer :: woman: homemaker.</li>
<li>Machine learning used in recruiting software developed at Amazon <a href="https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report">penalized applicants</a> who attended all-women’s colleges, as well as any resumes that contained the word “women’s.”</li>
<li>Over <a href="https://arxiv.org/abs/1711.08536">2/3 of the images in ImageNet</a>, the most studied image data set in the world, are from the Western world (USA, England, Spain, Italy, Australia).</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-01-29-five-scary-things/imagenet_geodiv.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Chart from ‘No Classification without Representation’ by Shankar, et. al, shows the origin of ImageNet photos: 45% US, 8% UK, 6% Italy, 3% Canada, 3% Australia, 3% Spain,…</figcaption><p></p>
</figure>
</div>
<p>Since a Cambrian explosion of machine learning products is occuring, the biases that are calcified now and in the next few years may have a disproportionately huge impact for ages to come (and will be much harder to undo decades from now).</p>
<h2 id="metrics" class="anchored">
<ol start="4" type="1">
<li>Optimizing metrics above all else leads to negative outcomes.
</li></ol></h2>

<p>Worldwide, people watch <a href="https://youtube.googleblog.com/2017/02/you-know-whats-cool-billion-hours.html">1 billion hours of YouTube per day</a> (yes, that says PER DAY). A large part of YouTube’s successs has been due to its recommendation system, in which a video selected by an algorithm automatically begin playing once the previous video is over. Unfortunately, <a href="https://www.buzzfeednews.com/article/carolineodonovan/down-youtubes-recommendation-rabbithole">these recommendations</a> are disproportionately for conspiracy theories promoting <a href="https://twitter.com/zeynep/status/973271727798083584">white supremacy</a>, climate change denial, and <a href="https://twitter.com/gchaslot/status/967585220001058816">denial of the mass shootings</a> that plague the USA. What is going on? YouTube’s algorithm is trying to <a href="https://medium.com/@guillaumechaslot/how-algorithms-can-learn-to-discredit-the-media-d1360157c4fa">maximize how much time</a> people spend watching YouTube, and <strong>conspiracy theorists watch significantly more YouTube than people who trust a variety of media sources</strong>. Unfortunately, a recommendation system trying only to maximize time spent on its own platform will incentivize content that tells you <strong>the rest of the media is lying</strong>.</p>
<p>“<em>YouTube may be one of the most powerful radicalizing instruments of the 21st century</em>,” Professor <a href="https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html">Zeynep Tufekci wrote in the New York Times</a>. <a href="https://medium.com/@guillaumechaslot/how-algorithms-can-learn-to-discredit-the-media-d1360157c4fa">Guillaume Chaslot</a> is a former YouTube engineer turned whistleblower. He has been outspoken about the harms caused by YouTube, and he <a href="https://www.theguardian.com/technology/2018/feb/02/youtube-algorithm-election-clinton-trump-guillaume-chaslot">partnered with the Guardian</a> and the Wall Street Journal to study the extremism and bias in YouTube’s recommendations.</p>
<p>[Photo of Guillaume Chaslot from the Guardian article]{chaslot.png){width=60%}</p>
<p>YouTube is owned by Google, which is earning billions of dollars by aggressively introducing vulnerable people to conspiracy theories, while the rest of society bears the <a href="https://en.wikipedia.org/wiki/Externality">externalized costs</a> of rising authoritarian governments, a resurgence in white supremacist movements, failure to act on climate change (even as extreme weather is creating increasing numbers of refugees), growing distrust of mainstream news sources, and a failure to pass sensible gun laws.</p>
<p>This problem is an example of <a href="https://press.princeton.edu/titles/11218.html">the tyranny of metrics</a>: metrics are just a proxy for what you really care about, and unthinkingly optimizing a metric can lead to unexpected, negative results. One analog example is that when the UK began publishing the success rates of surgeons, <a href="https://www.telegraph.co.uk/science/2016/06/03/one-in-three-heart-surgeons-refuse-difficult-operations-to-avoid/">heart surgeons began turning down</a> risky (but necessary) surgeries to try to keep their scores as high as possible.</p>
<p>Returning to the account of the popular <a href="https://www.washingtonpost.com/local/education/creative--motivating-and-fired/2012/02/04/gIQAwzZpvR_story.html">5th grade teacher who was fired by an algorithm</a>, she suspects that the underlying reason she was fired was that her incoming students had unusually high test scores the previous year (making it seem like their scores had dropped to a more average level after her teaching), and that their former teachers may have cheated. As USA education policy began over-emphasizing student test scores as the primary way to evaluate teachers, there have been <a href="https://www.nytimes.com/2010/06/11/education/11cheat.html">widespread scandals</a> of teachers and principals cheating by altering students scores, in Georgia, Indiana, Massachusetts, Nevada, Virginia, Texas, and elsewhere. When metrics are given undue importance, attempts to game those metrics become common.</p>
<h2 id="accountability" class="anchored">
<ol start="5" type="1">
<li>There is no accountability for big tech companies.
</li></ol></h2>

<p>Major tech companies are the primary ones driving AI advances, and their algorithms impact billions of people. Unfortunately, these companies have zero accountability. YouTube (owned by Google) is helping to <a href="https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html">radicalize</a> people into white supremacy. <a href="https://www.buzzfeednews.com/article/alexkantrowitz/google-allowed-advertisers-to-target-jewish-parasite-black">Google allowed advertisers</a> to target people who search racist phrases like “black people ruin neighborhoods” and <a href="https://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters">Facebook allowed advertisers</a> to target groups like “jew haters”. Amazon’s facial recognition technology <a href="https://www.theverge.com/2018/7/26/17615634/amazon-rekognition-aclu-mug-shot-congress-facial-recognition">misidentified 28 members of congress</a> as criminals, yet it is <a href="https://www.washingtonpost.com/news/the-switch/wp/2018/05/22/amazon-is-selling-facial-recognition-to-law-enforcement-for-a-fistful-of-dollars">already in use</a> by police departments. <a href="https://www.theverge.com/2018/2/27/17054740/palantir-predictive-policing-tool-new-orleans-nopd">Palantir’s predictive policing technology</a> was used for 6 years in New Orleans, with city council members not even knowing about the program, much less having any oversight. The newsfeed/timeline/recommendation algorithms of all the major platforms tend to reward incendiary content, prioritizing it for users.</p>
<p>In early 2018, <a href="https://www.reuters.com/article/us-myanmar-rohingya-facebook/u-n-investigators-cite-facebook-role-in-myanmar-crisis-idUSKCN1GO2PN">the UN ruled</a> that Facebook had played a “determining role” in the ongoing genocide in Myanmar. “<em>I’m afraid that Facebook has now turned into a beast</em>,” said the UN investigator. This result was not a surprise to anyone who had been following the situation in Myanmar. <a href="https://www.wired.com/story/how-facebooks-rise-fueled-chaos-and-confusion-in-myanmar/">People warned Facebook executives</a> about how the platform was being used to spread dehumanizing hate speech and incite violence against an ethnic minority as early as 2013, and again in 2014 and 2015. As early as 2014, news outlets such as <a href="https://www.aljazeera.com/indepth/features/2014/06/facebook-myanmar-rohingya-amplifying-hate-speech-2014612112834290144.html">Al Jazeera</a> were covering Facebook’s role in inciting ethnic violence in Myanmar.</p>
<p>One person <a href="https://www.wired.com/story/how-facebooks-rise-fueled-chaos-and-confusion-in-myanmar/">close to the case</a> said, “<em>That’s not 20/20 hindsight. The scale of this problem was significant and it was already apparent.</em>” <strong>Facebook execs were warned in 2015 that Facebook could play the same role in Myanmar that <a href="http://news.bbc.co.uk/2/hi/africa/3257748.stm">radio broadcasts had played during the 1994 Rwandan genocide</a></strong>. As of 2015, Facebook only <a href="https://www.reuters.com/investigates/special-report/myanmar-facebook-hate/">employed 4 contractors</a> who spoke Burmese (the primary language in Myanmar).</p>
<p>Contrast Facebook’s inaction in Myanmar with their swift action in Germany after the passage of a new law, which could have resulted in penalties of up to 50 million euros. <a href="http://thehill.com/policy/technology/361722-facebook-opens-second-german-office-to-comply-with-hate-speech-law">Facebook hired 1,200 German contractors</a> in under a year. In 2018, five years after Facebook was first warned about how they were being used to incite violence in Myanmar, they hired “dozens” of Burmese contractors, a fraction of their response in Germany. The <strong>credible threat of a large financial penalty</strong> may be the only thing Facebook responds to.</p>
<p>While it can be easy to focus on regulations that are misguided or ineffective, we often take for granted <a href="https://arxiv.org/abs/1803.09010">safety standards and regulations</a> that have largely worked well. One major success story comes from <a href="https://99percentinvisible.org/episode/nut-behind-wheel/">automobile safety</a>. Early cars had sharp metal knobs on dashboard that lodged in people’s skulls during crashes, plate glass windows that shattered dangerously, and non-collapsible steering columns that would frequently impale drivers. Beyond that, there was a widespread belief that the only issue with cars was the people driving them, and car manufactures did not want data on car safety to be collected. It took consumer safety advocates decades to push the conversation to how cars could be designed with greater safety, and to pass laws regarding seat belts, driver licenses, crash tests, and the collection of car crash data. For more on this topic, <a href="https://arxiv.org/abs/1803.09010">Datasheets for Datasets</a> covers cases studies of how standardization came to the electronics, pharmaceutical, and automobile industries, and 99% Invisible has a <a href="https://99percentinvisible.org/episode/nut-behind-wheel/">deep dive on the history of car safety</a> (with parallels and contrasts to the gun industry).</p>
<h2 id="better" class="anchored">
How We Can Do Better
</h2>
<p>The good news: none of the problems listed here are inherent to algorithms! There are ways we can do better:</p>
<ul>
<li>Make sure there is a meaningful, human appeals process. Plan for how to catch and address mistakes in advance.</li>
<li>Take responsibility, even when our work is just one part of the system.</li>
<li>Be on the lookout for bias. Create datasheets for data sets.</li>
<li>Choose not to just optimize metrics.</li>
<li>Push for thoughtful regulations and standards for the tech industry.</li>
</ul>
<p>The problems we are facing can feel scary and complex. However, it is still very early on in this age of AI and increasing algorithmic automation. Now is a great time to take action: we can change our culture, cultivate a greater sense of responsibility for our work, seek out thoughtful accountability to counterbalance the inordinate power that major tech companies have, and choose to create more humane products and systems. Technology is just a tool, and it can be used for good or bad. Let’s work to use it for good, to improve the lives of many, rather than just generate wealth for a small number of people.</p>
<h2 class="anchored">
Related Posts
</h2>
<p>You may be interested in these related posts on tech and ethics:</p>
<ul>
<li><a href="https://www.fast.ai/posts/2018-09-24-ai-ethics-resources.html">AI Ethics Resources</a></li>
<li><a href="https://www.fast.ai/posts/2018-08-07-hbr-bias-algorithms.html">What HBR Gets Wrong About Algorithms and Bias</a></li>
<li><a href="https://medium.com/s/story/techs-long-hours-are-discriminatory-counter-productive-17dc61071ed5">Tech’s long hours are discriminatory &amp; counter-productive</a></li>
<li><a href="https://www.youtube.com/watch?v=LqjP7O9SxOM&amp;list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6">Artificial Intelligence Needs All of Us</a> (TEDx talk)</li>
</ul>



 ]]></description>
  <category>ethics</category>
  <category>machine learning</category>
  <guid>https://rachel.fast.ai/posts/2019-01-29-five-scary-things/index.html</guid>
  <pubDate>Mon, 28 Jan 2019 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2019-01-29-five-scary-things/no-accountability.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The tech industry is failing people with disabilities and chronic illnesses</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2019-01-21-failing/index.html</link>
  <description><![CDATA[ 




<p><em>This was originally published on <a href="https://medium.com/@racheltho/the-tech-industry-is-failing-people-with-disabilities-and-chronic-illnesses-8e8aa17937f3">Medium</a> as part of a longer post, which I have now split in two.</em></p>
<p><a href="https://www.ncbi.nlm.nih.gov/books/NBK285684/"><strong>Thirty percent of people</strong></a> <strong>under the age of 65 have a chronic health condition</strong>. Despite this prevalence, people with chronic illnesses or disabilities often face discrimination. <a href="https://hbr.org/2017/12/the-case-for-improving-work-for-people-with-disabilities-goes-way-beyond-compliance"><strong>Over a third of disabled people</strong></a> <strong>have experienced negative bias</strong> in their current job. The <a href="https://www.bls.gov/news.release/disabl.nr0.htm"><strong>unemployment rate is twice as high</strong></a> for people with disabilities, compared to the general population, even though companies which are inclusive of people with disabilities are <a href="https://www.nytimes.com/2018/12/27/opinion/disability-rights-employment.html">more financially successful</a>. <a href="https://www.air.org/resource/air-index-pay-gap-workers-disabilities"><strong>People with disabilities earn significantly less</strong></a> than non-disabled people with the same education level. While there is a dearth of data specific to the tech industry (<a href="https://techcrunch.com/2016/11/07/parallel-pr-universe/">major tech companies gave evasive, off-the-record answers</a> when asked by a disabled reporter why they don’t include disability in their diversity reports), there are plenty of <a href="https://medium.com/tech-disability-project/the-gift-of-grey-spaces-in-our-careers-bc01b925eb34">first-hand</a> <a href="https://medium.com/tech-disability-project/being-my-own-boss-balancing-disability-and-my-career-dc0bd971a94">accounts</a> of exclusion and <a href="https://medium.com/tech-disability-project/my-own-worst-enemy-is-the-heckler-in-my-mind-ea5f622e3164">bias</a>.</p>
<p>Because I occasionally tweet about my health <a href="https://twitter.com/search?q=%40math_rachel%20brain&amp;src=typd">struggles</a>, (which include two brain surgeries, a life-threatening brain infection, and two ICU stays), a number of people in the tech industry have contacted me to privately share about their own experiences with disability or illness. Many of them say that they fear discrimination within the tech industry if they were to speak publicly about their health. <em>Will people think that I’m less capable? Will my manager be less likely to give me important and high impact assignments? Will this change the lens through which people view my achievements or productivity?</em> Sadly, these fears can be well-founded.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-01-21-failing/pictograms.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Image of accessibility pictograms used by the National Park Service.</figcaption><p></p>
</figure>
</div>
<p><strong>The people who build technology need to be representative of the people who use that technology</strong>, which increasingly includes everyone. Some of the users of your product have chronic illnesses and disabilities (whether you realize it or not) and <strong>they need to be involved in the creation of your product.</strong> In this 2-part set of posts, I want to address some misconceptions about disability, and to offer advice on how to improve, both in our interpersonal interactions and in our work cultures.</p>
<section id="the-disability-pay-gap-unemployment-and-job-loss" class="level2">
<h2 class="anchored" data-anchor-id="the-disability-pay-gap-unemployment-and-job-loss">The disability pay gap, unemployment, and job loss</h2>
<p>Haben Girma, the first deafblind person to earn a law degree at Harvard, <a href="https://www.popsugar.com/smart-living/Haben-Girma-Essay-About-Working-Disability-45311620">wrote of her experience of discrimination</a>, even as a high achiever who excelled at school and work, “<em>Hard work alone will not overcome the widespread</em> <a href="https://www.popsugar.com/moms/My-Child-Disability-Keeps-Proving-People-Wrong-45254208"><em>discrimination against people with disabilities</em></a><em>. Employers need to break down disability barriers. People with disabilities succeed when communities choose to be inclusive</em><strong><em>. The biggest barriers exist not in the person, but in the social, physical, and digital environments</em></strong>_._” (emphasis mine)</p>
<p><a href="https://www.theguardian.com/commentisfree/2018/apr/11/gender-pay-gap-disability-disabled-people-job">Data from the UK</a> shows that <em>“the</em> <a href="https://www.equalityhumanrights.com/en/publication-download/research-report-107-disability-pay-gap"><em>disability pay gap</em></a> <em>— the difference between what non-disabled and disabled workers earn — is 13.6%. On top of that,</em> <a href="https://www.scope.org.uk/About-Us/Media/Press-releases/November-2016/More-disabled-people-in-work,-but-employment-gap-r"><em>disabled people are significantly more likely to be unemployed</em></a><em>,</em> <a href="https://blog.scope.org.uk/2016/11/16/whats-behind-the-disability-employment-gap/"><em>lose a job</em></a> <em>and be in</em> <a href="https://www.jrf.org.uk/mpse-2015/disability"><em>low-waged work</em></a> <em>than non-disabled people. We’re also routinely given fewer responsibilities at work and turned down for promotion, or</em> <a href="http://www.papworthtrust.org.uk/sites/default/files/Disability%20Facts%20and%20Figures%202016.pdf"><em>refused the job in the first place</em></a><em>.”</em> In the USA, there is a <a href="https://www.air.org/resource/air-index-pay-gap-workers-disabilities">disability pay gap</a> at all educational levels, which grows greater for higher levels of education.</p>
<p>Many tech companies have explicit reputations as being unwelcoming to those with health issues. At Amazon’s headquarters, multiple employees were given low performance reviews or put on performance improvement plans (a precursor to being fired) after having cancer or major surgeries, as reported in a 2015 <a href="https://www.nytimes.com/2015/08/16/technology/inside-amazon-wrestling-big-ideas-in-a-bruising-workplace.html">New York Times article</a>. <em>“<em><strong><em>A woman who had thyroid cancer was given a low performance rating</em></strong> <em>after she returned from treatment. She says her manager explained that while she was out, her peers were accomplishing a great deal…</em> <strong><em>A woman who had breast cancer was told that she was put on a ‘performance improvement plan’, because ‘difficulties’ in her ‘personal life’ had interfered with fulfilling her work goals</em></strong></em>… A former HR executive said she was required to put_ <strong><em>a woman who had recently returned after undergoing serious surgery, and another who had just had a stillborn child, on performance improvement plans</em></strong>_.”</em> (emphasis mine) While being fired immediately after cancer treatment or surgery is particularly egregious, bias often shows up in more subtle, yet still harmful, forms, such as: not accommodating medical or scheduling needs; inaccessible buildings or processes; inadequate HR resources; resentment towards a coworker that has taken medical leave; or even distrust/disbelief that someone’s experiences could be so different than your own.</p>
<p>The tech industry’s obsession with working <a href="https://twitter.com/AndrewYNg/status/908052152722976768">ridiculously long hours</a> is inaccessible to many disabled and ill people, for whom adequate rest is often not optional, or who may have regular doctor’s appointments, physical therapy, or tests. On top of being exclusionary, this insistence on long hours is <a href="https://qz.com/work/1486863/boasting-about-how-many-hours-you-work-is-a-sign-of-failure/"><strong>contrary to research on productivity</strong></a> <strong>— it doesn’t even lead to more productive companies</strong>! (More on this point in <a href="https://medium.com/s/story/techs-long-hours-are-discriminatory-counter-productive-17dc61071ed5?source=friends_link&amp;sk=b0f3b22e289d89efa3ddab45c3b178f2">part 3</a>).</p>
<p><em>This was part 1 of a 3-part series. Check out</em> <a href=""><em>part 2 here</em></a> and <a href=""><em>part 3 here</em></a><em>, which covers one aspect of the tech industry that excludes many people with disabilities (and is contrary to productivity research): the glorification of long hours.</em></p>
<p><em>A huge thank you to Julia Ferraioli, Jeremy Howard, and Negar Rostamzadeh for giving me valuable feedback on earlier drafts of this post.</em></p>


</section>

 ]]></description>
  <category>inclusion</category>
  <guid>https://rachel.fast.ai/posts/2019-01-21-failing/index.html</guid>
  <pubDate>Sun, 20 Jan 2019 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2019-01-21-failing/pictograms.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Things everyone should know about chronic illness &amp; disability</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2019-01-21-should-know/index.html</link>
  <description><![CDATA[ 




<p><em>This was originally published on <a href="https://medium.com/@racheltho/the-tech-industry-is-failing-people-with-disabilities-and-chronic-illnesses-8e8aa17937f3">Medium</a> as part of a longer post, which I have now split in two.</em></p>
<section id="health-is-not-binary" class="level2">
<h2 class="anchored" data-anchor-id="health-is-not-binary">Health is not binary</h2>
<p>Illness &amp; disability can fluctuate. Perhaps some days a person needs a wheelchair or cane and other days they don’t. Some days they are capable of tasks that other days they can’t do. Google developer advocate, Julia Ferraioli, who has had a vision impairment off-and-on for the last two years, shared in her <a href="https://www.juliaferraioli.com/presos/writing-accessible-go/">GopherCon keynote</a>, that “<em>Disability is not binary.</em> <em>There are some days when I can churn out code, and some when I can’t read anything at all</em>.”</p>
<p>I have some dramatic stories of my most acute health crises, but what is harder for me to explain is the long stretches of time where my health was in-between — when I was not fully well, but could give high-profile talks (although it would take days to recover), attend conferences (even if just for a few hours each day), or teach (even if I felt awful and spent all my time outside of class resting). I hear similar accounts from many people whose symptoms fluctuate over time, or who experience flares alternating with periods of reduced symptoms. Even the recovery from a particular illness or surgery is rarely linear, and a better week may be followed by a worse week. Many people worry that if they share about their disability, people may accuse them of “faking it” if they are not perfectly consistent in their behavior.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-01-21-should-know/wheelchair.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Image of wheelchair. Source: https://www.publicdomainpictures.net/en/view-image.php?image=167689</figcaption><p></p>
</figure>
</div>
<p>Gabe Moses, who often uses a wheelchair and sometimes uses crutches, <a href="https://thebodyisnotanapology.com/magazine/who-really-needs-a-wheelchair-lets-stop-accusing-disabled-folk-of-being-lazy/">wrote a post</a> addressing how wheelchair users are often wrongly accused of faking their disability. Gabe has unstable joints, unpredictable muscle spasms, and sudden numbness, which makes getting around without a wheelchair exhausting, painful, and dangerous (although possible, at least sometimes). <a href="https://thebodyisnotanapology.com/magazine/who-really-needs-a-wheelchair-lets-stop-accusing-disabled-folk-of-being-lazy/">He writes</a>, “<strong><em>The black-and-white narrative of disability we’ve been sold makes people who haven’t experienced disability themselves see wheelchair users this way: wheelchair users are people who can’t walk, at all. Anyone else who dares to use one is either lazy or faking.</em></strong> <em>The truth is, wheelchair users are a highly varied group of people. Some people do need their wheelchairs because of paralysis or other conditions that make walking impossible. Others use them because of fatigue, chronic pain, balance problems, or other conditions that make it impossible to walk long distances, even if they are capable of standing and walking to some degree.</em>”</p>
</section>
<section id="illness-disability-can-be-invisible" class="level2">
<h2 class="anchored" data-anchor-id="illness-disability-can-be-invisible">Illness &amp; disability can be invisible</h2>
<p><strong>“You seem like you are doing great!”</strong> people tell me when I bring up my health challenges. Sometimes people who see me seeming upbeat during a social event or watch one of my talks assume that I must be fully recovered. Whereas in reality, it is common to feel ok at some times but ill at others (and this can vary from week to week, or even throughout a given day), to appear healthier than I feel inside (particularly if I’m trying to show my best side at a professional event), or to need an unusually long amount of time to recover from events where I seemed “normal”. People are complex and multi-faceted, and you can’t assess their health based on appearance or mood.</p>
<p>Product designer <a href="https://twitter.com/amelielamont/status/1074707475503505408">Amélie Lamont shared</a> that while on disability leave from work, she spoke at the conference (after clearing it with her doctor and after having had to cancel her talk the previous year due to her health). It took her days to recover, yet her employer assumed since she seemed fine during the 30-minute talk, that she must be well enough to return to work full-time. In doing so, her employer was failing to understand that illness can be invisible, and that Amélie and her doctor are the only ones who can determine when she is actually ready to return to work.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-01-21-should-know/neurips.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Image of tweet saying, “The top two recommended accounts for the #NIPS2017 hashtag are women! math_rachel and hannawallach”</figcaption><p></p>
</figure>
</div>
<p>To give one small personal example, at NeurIPS 2017, I attended <a href="https://twitter.com/search?src=typd&amp;q=%40math_rachel%20%23NIPS2017">interesting talks</a>, mentored at two round table discussions for the <a href="https://wimlworkshop.org/">Women in ML workshop</a> (which inspired me to later write blog posts on <a href="https://www.fast.ai/2017/12/18/personal-brand/">personal branding</a> and <a href="https://www.fast.ai/2018/01/08/startups/">founding startups</a>), and met many new people. At one point, I was one of the two top people recommended to follow for the <a href="https://twitter.com/search?src=typd&amp;q=%40math_rachel%20%23NIPS2017">#NIPS2017</a> hashtag. Unless I spoke about it, most people I talked to at the conference probably wouldn’t have guessed how unwell I was (I was recovering from a life-threatening brain infection, and had an infected brain cyst which would require an additional surgery), or that I only attended a few hours of talks or events each day, took breaks to lie down on the floor of the conference center, had daily headaches, and went to bed early each evening. This paints two different pictures of my time at the same event.</p>
<p>A friend, who has a disease which impacts her mobility, told me that at <a href="https://neurips.cc/">NeurIPS 2018</a> there were seats that were supposed to be reserved for people with disabilities, yet they were always full. Since my friend “looks healthy”, she felt too uncomfortable to ask someone else who “looks healthy” if she could have one of these seats; perhaps they have an invisible illness as well. She also felt unsure whether she even wanted other people in the AI community to know about her disability, as this would change how they view her.</p>
</section>
<section id="trust-people-to-understand-their-own-bodies-experiences" class="level2">
<h2 class="anchored" data-anchor-id="trust-people-to-understand-their-own-bodies-experiences">Trust people to understand their own bodies &amp; experiences</h2>
<p>Many people with illness and disabilities have spent years trying to get diagnosed, to get their symptoms taken seriously, to get treatments they needed approved or covered by insurance, or to get accommodations they need at work. For instance, <a href="http://www.bbc.com/future/story/20180523-how-gender-bias-affects-your-healthcare">a study in the UK</a> found that almost a third of patients with a brain tumor had to visit a doctor more than 5 times before receiving their diagnosis (these times are longer for women and low-income patients). <a href="http://www.bbc.com/future/story/20180523-how-gender-bias-affects-your-healthcare">One woman</a> with a brain tumor recalled: “<em>One of the GPs I saw actually made fun of me, saying ‘what did I think my headaches were, a brain tumor?’ I had to request a referral to neurology. I went back repeated times to be given antidepressants, sleep charts, analgesia, etc. No one took me seriously.</em>” On average, patients with autoimmune diseases have to visit 5 doctors over a 5-year period before receiving a proper diagnosis.</p>
<p>Research shows that doctors take the <a href="https://www.thecut.com/2018/03/doctors-dont-know-how-to-deal-with-womens-pain-books.html">pain of women</a> less seriously than the pain of men, and the <a href="https://www.scientificamerican.com/article/how-doctors-can-confront-racial-bias-in-medicine/">pain of people of color</a> less seriously than the pain of white people (resulting in <strong>longer time delays to receive treatment, lower quality of care, and worse outcomes</strong>). Since so many doctors were dismissive of her symptoms, it took author Aubrey Hirsch 6 years and a dozen doctors to get her Grave’s disease diagnosed. By that point, the disease had done permanent damage to her bones, heart, and eyes, which could have been prevented by earlier treatment. <a href="https://twitter.com/aubreyhirsch/status/890959115656208384">Hundreds of people</a> responded to <a href="https://thenib.com/medicine-s-women-problem">the powerful comic she created</a> with their own experiences of having severe symptoms dismissed.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-01-21-should-know/icu.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Image of an ICU from Wikimedia Commons</figcaption><p></p>
</figure>
</div>
<p>A few years ago, I went to the ER in the worst pain of my life, and was given no tests, but just sent home with aspirin (it turned out I actually needed brain surgery). Another time, I had not slept in days because of heart problems, and an ER doctor told me to “relax and take melatonin” when in reality my PICC line had been inserted 5cm too deep and was mistakenly inside my heart. I was once not believed about having dislocated my shoulder, which later resulted in my needing a surgery which my surgeon said could have been avoided if I had gotten treatment at that earlier time.</p>
</section>
<section id="the-trauma-of-the-medical-system" class="level2">
<h2 class="anchored" data-anchor-id="the-trauma-of-the-medical-system">The trauma of the medical system</h2>
<p>If you don’t have first-hand experience with chronic illness or disability, it can be difficult to understand how encompassing it can be. If you do not have first-hand experience trying to navigate the byzantine and traumatizing medical system, it can be surprising just how <strong>complicated, error-prone, and often hostile to patients</strong> it is. A <a href="https://www.ncbi.nlm.nih.gov/pubmed/10741582">growing body of research</a> is linking medical trauma to PTSD.</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
reasons it can be hard to disclose an invisible disability <a href="https://t.co/RK15Bc8IQG">https://t.co/RK15Bc8IQG</a> <a href="https://t.co/CdxsoATLLa">pic.twitter.com/CdxsoATLLa</a>
</p>
— Rachel Thomas (<span class="citation" data-cites="math_rachel">@math_rachel</span>) <a href="https://twitter.com/math_rachel/status/1072299466852655104?ref_src=twsrc%5Etfw">December 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Tweet showing a paragraph from <a href="https://medium.com/tech-disability-project/asking-for-accommodations-invisible-disability-3e8242f9ec79">this blog post</a>, listing reasons it is hard to disclose an invisible disability.</p>
<p>Liz Allen, who works in tech and has a complex range of immune issues, gives <a href="https://medium.com/tech-disability-project/asking-for-accommodations-invisible-disability-3e8242f9ec79">many reasons why she was hesitant to ask HR</a> for the accommodations she needed, including that her story is long and traumatic to retell, educating others can be emotionally exhausting, she is often doubted or disbelieved, and her fear of being viewed as incompetent. <strong>Remember that it takes bravery for someone to share about their health with you; honor that and respond in a supportive and accommodating way.</strong></p>
</section>
<section id="avoid-giving-unsolicited-health-advice." class="level2">
<h2 class="anchored" data-anchor-id="avoid-giving-unsolicited-health-advice.">Avoid giving unsolicited health advice.</h2>
<p>Most people with chronic illnesses or disabilities have spent hundreds of hours researching their conditions, met with dozens of specialists, and tried a variety of treatments (both medical and lifestyle). Even when it’s well-intentioned, offering unsolicited health advice can come across as patronizing or dismissive. There is a huge amount of variety in different conditions and what works for different people, so avoid statements like “{yoga, x diet, physical therapy} is good for everyone.” Know that if a simple solution existed, the person would have already figured it out. <strong>Trust people to know their own bodies and to be experts on their own lives.</strong></p>
</section>
<section id="the-limits-of-language" class="level2">
<h2 class="anchored" data-anchor-id="the-limits-of-language">The limits of language</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2019-01-21-should-know/dictionary.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Image from Wikimedia Commons of a Latin Dictionary</figcaption><p></p>
</figure>
</div>
<p><strong>Don’t compare everyday experiences to someone else’s chronic condition</strong>. Feeling tired is not the same as chronic fatigue. Being sad is not the same as clinical depression. Doing <a href="https://twitter.com/juliaferraioli/status/1071857988938465280">Vipassana meditation for an hour</a> is not the same as living with chronic pain for years. <strong>Part of this is a failure of language: words like “pain” and “fatigue” are used to describe a wide range of experiences.</strong> A friend with multiple sclerosis told me how regular fatigue (after a hike or long day) is very different from multiple sclerosis fatigue (which can involve nerve pain, muscle pain, and double vision), yet they are described with the same word, which leads to poor understanding. Brianne Benness, who has a chronic illness and started a <a href="https://noendinsight.co/share-your-story/">podcast about chronic illness</a>, also spoke about this limit of language, <a href="https://noendinsight.co/2018/11/29/episode-9-clare/">in response to an interviewee</a> with epilepsy, Ehlers-Danlos syndrome, and cochlear implants, “<em>I think I end up talking with people a lot right now about how</em> <strong><em>we don’t have good words for what that feels like</em></strong>.” People who have never experienced severe pain may not be able to imagine what it feels like, yet it is still possible and necessary to practice empathy.</p>
</section>
<section id="medical-leave-is-not-a-vacation" class="level2">
<h2 class="anchored" data-anchor-id="medical-leave-is-not-a-vacation">Medical Leave is not a “vacation”</h2>
<p><strong>Don’t tell someone that you wish you could spend the day in bed or take a month off from work like them</strong> — nobody wants to be in pain or suffering. Comparing medical leave to a “vacation” is dismissive of the pain, limitations, and actual work that go into medical leave. I also want to address the concern that an employee going on disability leave or requiring special accomodations results in extra work for their coworkers. If this does occur, it is not the fault of the ill or disabled person; it is the fault of the employer for not adjusting workloads, dropping lower priority tasks, or hiring additional temporary help (ditto for when child-free people are given extra work to cover for parents — this is the employer’s fault for not ensuring humane and fair work conditions for everyone). While the moral and legal cases for inclusion should be more than enough, note that <a href="https://www.nytimes.com/2018/12/27/opinion/disability-rights-employment.html">companies that champion people with disabilities</a> outperform other companies financially, with 28% higher revenues, 200% higher net income, and 30% higher profit margins.</p>
</section>
<section id="listen-to-other-peoples-experiences" class="level2">
<h2 class="anchored" data-anchor-id="listen-to-other-peoples-experiences">Listen to other people’s experiences</h2>
<p>Even if you don’t realize it, you are interacting with chronically ill and disabled people regularly. I hope the experiences, advice, and quotes shared in this post will be helpful in increasing empathy and understanding, so that we can make the tech industry more inclusive. I don’t and can’t speak for everyone with chronic illnesses or disabilities, so I hope that you will seek out and listen to the experiences of other people with chronic illnesses or disabilities (and the links in this post are one way to get started!)</p>
<p><em>This was part 2 of a 3-part series. Check out</em> <a href=""><em>part 1 here</em></a> and <a href="https://medium.com/s/story/techs-long-hours-are-discriminatory-counter-productive-17dc61071ed5?source=friends_link&amp;sk=b0f3b22e289d89efa3ddab45c3b178f2"><em>part 3 here</em></a><em>, which covers one aspect of the tech industry that excludes many people with disabilities (and is contrary to productivity research): the glorification of long hours.</em></p>
<p><em>A huge thank you to Julia Ferraioli, Jeremy Howard, and Negar Rostamzadeh for giving me valuable feedback on earlier drafts of this post.</em></p>


</section>

 ]]></description>
  <category>inclusion</category>
  <guid>https://rachel.fast.ai/posts/2019-01-21-should-know/index.html</guid>
  <pubDate>Sun, 20 Jan 2019 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2019-01-21-should-know/icu.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>What Harvard Business Review Gets Wrong About Algorithms and Bias</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2018-08-07-hbr-bias-algorithms/index.html</link>
  <description><![CDATA[ 




<p>The Harvard Business Review recently published an article, <a href="https://hbr.org/2018/07/want-less-biased-decisions-use-algorithms">Want Less-Biased Decisions? Use Algorithms.</a> by Alex P. Miller. The article focuses on the fact that humans make very biased decisions (which is true), yet ignores many important related issues, including:</p>
<ul>
<li>algorithms are often implemented without any appeals method in place (due to the misconception that algorithms are objective, accurate, and won’t make mistakes)</li>
<li>algorithms are often used at a much larger scale than human decision makers, in many cases, replicating an identical bias at scale (part of the appeal of algorithms is how cheap they are to use)</li>
<li>users of algorithms may not understand probabilities or confidence intervals (even if these are provided), and may not feel comfortable overriding the algorithm in practice (even if this is technically an option)</li>
<li>instead of just focusing on the least-terrible existing option, it is more valuable to ask how we can create <strong>better, less biased decision-making tools</strong> by leveraging the strengths of humans and machines working together</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-08-07-hbr-bias-algorithms/algorithm.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Photo Credit: Jonathan Lidbeck https://www.flickr.com/photos/jondissed/2278335691</figcaption><p></p>
</figure>
</div>
<p>Miller acknowledges that critics of the “algorithmic revolution” are “concerned that algorithms are often <strong>opaque, biased, and unaccountable tools</strong> being wielded in the interests of institutional power”, although he then focuses exclusively on the <em>biased</em> part for the remainder of the article, without addressing the <em>opaque</em> or <em>unaccountable</em> charges (as well as how these interact with bias).</p>
<h2 id="vs" class="anchored">
Humans vs.&nbsp;machines is not a helpful framing
</h2>
<p>The media often frames advances in AI through a lens of humans vs.&nbsp;machines: who is the champion at X task. This framework is both inaccurate as to how most algorithms are used, as well as a very limited way to think about AI. In all cases, algorithms have a human component, in terms of who gathers the data (and what biases they have), which design decisions are made, how they are implemented, how results are used to make decisions, the understanding various stakeholders have of correct uses and limitations of the algorithm, and so on.</p>
<p>Most people working on medical applications of AI are not trying to replace doctors; they are trying to create tools that will allow doctors to be more accurate and more efficient, improving quality of care. The best chess “players” are neither humans nor computers, but rather, teams of humans and computers working together.</p>
<p>Miller’s HBR article points out (correctly) that humans are very biased, and then compares our current not-so-great approaches to see which is less terrible. The article does <strong>not</strong> ask the question, <strong>how can we develop less biased ways to make decisions (perhaps using some combination of humans and algorithms)?</strong> which is a far more interesting and important question.</p>
<h2 id="different" class="anchored">
Algorithms are often used differently than human decision makers
</h2>
<p>Algorithms are often used at a larger scale, mass-producing identical biases, and assumed to be error-proof or objective. The studies that Miller shares compares them in an apples-to-apples way, which doesn’t acknowledge how differently they are often used in practice.</p>
<p>Cathy O’Neil writes in <a href="https://weaponsofmathdestructionbook.com/">Weapons of Math Destruction</a> that the algorithms she is critiquing <i>tend to punish the poor. <b>They specialize in bulk, and they’re cheap. That’s part of their appeal.</b> The wealthy, by contrast, often benefit from personal input. A white-shoe law firm or an exclusive prep school will lean far more on recommendations and face-to-face interviews than will a fast-food chain or a cash-strapped urban school district. The privileged, we’ll see time and again, are processed more by people, the masses by machines.</i> (emphasis mine)</p>
<p>One example from O’Neil’s book is that of a college student with bipolar disorder who wanted to get a summer job bagging groceries. Every store he applied to was using the same pyschometric evaluation software to screen candidates, and he was rejected from every store. This captures another danger of algorithms: even though humans often have similar biases, not all humans will make the exact same decisions (e.g.&nbsp;perhaps that college student would have been able to find one place to hire him, even if some of the people making decisions had biases about mental health).</p>
<p>Many people will put more trust in algorithmic decisions than they might in human decisions. While the researchers designing the algorithms may have a good grasp on probability and confidence intervals, often the general public using them will not. Even if people are given the power to override algorithmic decisions, it is crucial to understand if they will feel comfortable doing so in practice.</p>
<h2 id="appeals" class="anchored">
The need for meaningful appeals and explanations
</h2>
<p><strong>Many of the most chilling stories of algorithmic bias don’t involve meaningful explanations or a meaningful appeals process</strong>. This seems to be a particular trend amongst algorithmic decision making systems, perhaps since people mistakenly assume algorithms are objective, they believe there is no need for appeals. Also, as explained above, algorithmic decision making systems are often used as a cost-cutting device, and allowing appeals would be more expensive.</p>
<p><a href="https://weaponsofmathdestructionbook.com/">Cathy O’Neil writes</a> the account of a teacher who is beloved by her students, their parents, and the principal, yet is inexplicably fired by an algorithm. She is never able to get an answer as to why she was fired. Stories like this would be somewhat less disturbing if there had been a relatively quick and simple way for her to appeal the decision, or even to know for sure what factors it was related to.</p>
<p>The Verge <a href="https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy">investigated software</a> used in over half of U.S. states to determine how much healthcare people receive. After its implemention in Arkansas, people (many with severe disabilities) drastically had their healthcare cut. For instance, Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week. <strong>She couldn’t get any explanation for why her healthcare was cut.</strong> Eventually, a court case revealed that there were mistakes in the software implementation of the algorithm, negatively impacting people with diabetes or cerebral palsy. However, Dobbs and many other people reliant on these health care benefits live in fear that their benefits could again be cut suddenly and inexplicably.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-08-07-hbr-bias-algorithms/justice.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">image source: wikimedia commons</figcaption><p></p>
</figure>
</div>
<p>The creator of the algorithm, who is a professor and earning royalties off of this software, was asked whether there should be a way to communicate decisions, “It’s probably something we should do. I should also probably dust under my bed.” He later clarified that he thought it was someone else’s responsibility. <strong>We can not keep claiming the problems caused by our technology are someone else’s responsibility.</strong></p>
<p>For a separate computer system used in Colorado to determine public benefits in the mid-2000s, it was discovered that more than 900 incorrect rules had been coded into the system, resulting in problems like pregnant women being denied Medicaid. It is often hard for lawyers to even discover these flaws, since the inner-workings of the algorithms are typically protected as trade secrets. Systems used to make decisions related to healthcare, hiring/firing, criminal justice, and other life-altering areas should include some sort of <strong>human appeals process</strong>, that is relatively fast and easy to navigate. Many of the most chilling stories of algorithmic decision making would not be nearly as concerning if there had been an easy way to appeal and correct faulty decisions. Mistakes are possible in anything we do, so <strong>it’s important to have a tight loop in which we make it easy to discover and correct mistakes</strong>.</p>
<h2 id="real-world" class="anchored">
Complicated, real-world systems
</h2>
<p><strong>When we think about AI, we need to think about complicated, real-world systems</strong>. The studies in the HBR article treat decision making as an isolated action, without taking into account that this decision-making happens within complicated real-world systems. A decision about whether someone is likely to commit another crime is not an isolated decision: it lives within the complicated system of our criminal justice system. We have a responsibility to understand the real-world systems with which our work will interact, and to not lose sight of the actual <strong>people</strong> who will be impacted.</p>
<p>The COMPAS recidivism algorithm is used in some US courtrooms for decisions related to pre-trial bail, sentencing, and parole. It was the subject of a <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">ProPublica investigation</a> finding that the false positive rate (people that were labeled “high risk” but were not re-arrested) for white defendants was 24% and for Black defendants was 45%. Later research found that COMPAS (which uses 137 inputs in a black-box algorithm) was <a href="https://www.theverge.com/2018/1/17/16902016/compas-algorithm-sentencing-court-accuracy-problem">no more accurate</a> than a simple linear equation on two variables. COMPAS was also <a href="https://www.theverge.com/2018/1/17/16902016/compas-algorithm-sentencing-court-accuracy-problem">not more accurate</a> than untrained Mechanical Turk workers. (You can find out more about various definitions of fairness in Princeton CS Professor Arvind Narayanan’s excellent <a href="https://www.youtube.com/watch?v=wqamrPkF5kk">21 Definitions of Fairness</a> talk).</p>
<p>Kristian Lum, statistics PhD and lead data scientist at the Human Rights Digital Analysis Group, <a href="https://www.youtube.com/watch?v=hEThGT-_5ho">organized a workshop</a> together with Elizabeth Bender, a staff attorney for the NY Legal Aid Society and former public defender, and Terrence Wilkerson, an innocent man who had been arrested and could not afford bail. Together, they shared first hand experience about the obstacles and inefficiencies that occur in the legal system, providing valuable context to the debate around COMPAS. Bender shared that for public defenders meet with defendants at Rikers Island, where many pre-trial detainees in NYC who can’t afford bail are held, involves a bus ride that is two hours each way and they then only get 30 minutes to see the defendant, assuming the guards are on time (which is not always the case). Wilkerson explained how frequently innocent defendents who can’t afford bail accept guilty plea bargains just so they can get out of jail faster. Again, all this is for people that have not even faced a trial yet! This panel was an excellent way to illuminate the real-world systems and educate about the first-hand impact. I hope more statisticians and computer scientists will follow this example.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-08-07-hbr-bias-algorithms/lum2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dr.&nbsp;Kristian Lum’s Panel at the Fairness, Accountability, and Transparency in ML Conference</figcaption><p></p>
</figure>
</div>
<p>As this example shows, <strong>algorithms can often exacerbate underlying societal problems</strong>. There are deep, structural problems with the US courts and prison systems, including <a href="https://www.vox.com/cards/police-brutality-shootings-us/us-police-racism">racial bias</a>, the <a href="https://www.aclu.org/blog/smart-justice/we-cant-end-mass-incarceration-without-ending-money-bail">use of cash bail</a> (nearly half a million people in the USA are languishing in jail before even facing a trial, because they are too poor to afford bail), predatory <a href="http://time.com/5013760/american-private-prisons-donald-trump/">for-profit prisons</a>, and extreme <a href="https://en.wikipedia.org/wiki/United_States_incarceration_rate">over-use of prisons</a> (the US is home to 4% of the world’s population and 22% of the world’s prisoners). We have a responsibility to understand the systems and underlying problems our algorithms may interact with.</p>
<h2 id="critics" class="anchored">
Most critics of unjust bias aren’t anti-algorithm
</h2>
<p><strong>Most critics of biased algorithms are opposed to unjust bias; they are not people who hate algorithms.</strong> Miller says that critics of biased algorithms “rarely ask how well the systems they analyze would operate without algorithms,” suggesting that those speaking out against biased algorithms are perhaps unaware of how biased humans are or perhaps just don’t like algorithms. I spent a great deal of time <a href="https://medium.com/tech-diversity-files/if-you-think-women-in-tech-is-just-a-pipeline-problem-you-haven-t-been-paying-attention-cb7a2073b996">researching and writing</a> <a href="https://medium.com/tech-diversity-files/how-diversity-branding-hurts-diversity-fb29dd705481">about</a> <a href="https://medium.com/tech-diversity-files/the-real-reason-women-quit-tech-and-how-to-address-it-6dfb606929fd">studies</a> <a href="https://medium.com/@racheltho/how-to-make-tech-interviews-a-little-less-awful-c29f35431987">of human bias</a> (particularly as to how they pertain to the tech industry), long before I began writing about bias in machine learning.</p>
<p>When I tweet or share about biased or unethical algorithms, I frequently encounter push-back that I must be <strong>anti-algorithms</strong> or <strong>opposed to tech</strong>. This couldn’t be further from the truth: I have a PhD in math; I’ve worked as a quant, data scientist, and software engineer; I created a free, online <a href="https://github.com/fastai/numerical-linear-algebra">Computational Linear Algebra course</a> and co-founded fast.ai, which runs <a href="http://course.fast.ai/">Practical Deep Learning for Coders</a> and won <a href="http://www.fast.ai/2018/04/30/dawnbench-fastai/">Stanford’s Computer Vision Speed Test</a> through clever use of algorithms.</p>
<p>I’m in no way unique in this: most of the outspoken critics of biased algorithms that come to mind have PhDs in computer science, math, or statistics, and continue to be active in their fields. Just check out some of the speakers from the <a href="https://www.fatml.org/schedule/2018">Fairness Accountability and Transparency Conference</a> (and <a href="https://www.youtube.com/channel/UCs16j6ot-CYq-ZqYpO-vqMg/videos">watch their talks</a>!). One such example is <a href="http://randomwalker.info/">Arvind Narayanan</a>, a computer science professor at Princeton, winner of the <a href="https://arxiv.org/abs/1102.4374">Kaggle Social Network Challenge</a>, teacher of a <a href="https://www.coursera.org/learn/cryptocurrency">popular cryptocurrency course</a>, and also speaks out <a href="https://www.youtube.com/watch?v=wqamrPkF5kk">against algorithmic bias</a>.</p>
<p>I hope that the popular discussion of biased algorithms can move beyond unnuanced rebuttals and more deeply engage with the issues involved.</p>



 ]]></description>
  <category>ethics</category>
  <guid>https://rachel.fast.ai/posts/2018-08-07-hbr-bias-algorithms/index.html</guid>
  <pubDate>Mon, 06 Aug 2018 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2018-08-07-hbr-bias-algorithms/algorithm.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Google’s AutoML: Cutting Through the Hype</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2018-07-23-automl3/index.html</link>
  <description><![CDATA[ 




<p><em>This is part 3 in a series. <a href="http://www.fast.ai/2018/07/12/auto-ml-1/">Part 1 is here</a> and <a href="http://www.fast.ai/2018/07/16/auto-ml2/">Part 2 is here</a>.</em></p>
<p>To announce Google’s AutoML, Google CEO Sundar Pichai <a href="https://blog.google/technology/ai/making-ai-work-for-everyone/">wrote</a>, “Today, designing neural nets is extremely time intensive, and requires an expertise that limits its use to a smaller community of scientists and engineers. That’s why we’ve created an approach called AutoML, showing that it’s possible for neural nets to design neural nets. We hope AutoML will take an ability that a few PhDs have today and <strong>will make it possible in three to five years for hundreds of thousands of developers to design new neural nets for their particular needs</strong>.” (emphasis mine)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-23-automl3/sundar_pichai.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Google CEO Sundar Pichai says that we all need to design our own neural nets</figcaption><p></p>
</figure>
</div>
<p>When Google’s Head of AI, Jeff Dean, suggested that <strong>100x computational power could replace the need for machine learning expertise</strong>, computationally expensive neural architecture search was the only example he gave to illustrate this point. (<a href="https://www.youtube.com/watch?v=kSa3UObNS6o">around 23:50 in his TensorFlow DevSummit keynote</a>)</p>
<p>This raises a number of questions: do hundreds of thousands of developers <strong>need</strong> to “design new neural nets for their particular needs” (to quote <a href="https://blog.google/technology/ai/making-ai-work-for-everyone/">Pichai’s vision</a>), or <strong>is there an effective way for neural nets to generalize to similar problems</strong>? Can large amounts of computational power really replace machine learning expertise?</p>
<p>In evaluating Google’s claims, it’s valuable to keep in mind <strong>Google has a vested financial interest in convincing us that the key to effective use of deep learning is more computational power</strong>, because this is an area where they clearly beat the rest of us. If true, we may all need to purchase Google products. On its own, this doesn’t mean that Google’s claims are false, but it’s good be aware of what financial motivations could underlie their statements.</p>
<p>In my previous posts, I shared an introduction to the <a href="http://www.fast.ai/2018/07/16/auto-ml2/#auto-ml">history of AutoML</a>, defined <a href="http://www.fast.ai/2018/07/16/auto-ml2/#nas">what neural architecture search is</a>, and <a href="http://www.fast.ai/2018/07/12/auto-ml-1/#complex">pointed out</a> that for <strong>many machine learning projects, designing/choosing an architecture is nowhere near the hardest, most time-consuming, or most painful part of the problem</strong>. In today’s post, I want to look specifically at Google’s AutoML, a product which has received a lot of media attention, and address the following:</p>
<ul>
<li>
What is Google’s AutoML?
</li>
<li>
What is transfer learning?
</li>
<li>
Neural architecture search vs.&nbsp;transfer learning: two opposite approaches
</li>
<li>
In need of more evidence
</li>
<li>
Why all the hype about Google’s AutoML?
</li>
<li>
How can we address the shortage of machine learning expertise?
</li>
</ul>
<h2 id="goog" class="anchored">
What is Google’s AutoML?
</h2>
<p>Although the field of <a href="https://www.automl.org/">AutoML</a> has been around for years (including <a href="http://www.ml4aad.org/automl/">open-source AutoML libraries</a>, <a href="https://www.ml4aad.org/workshops/">workshops</a>, <a href="https://www.ml4aad.org/automl/literature-on-neural-architecture-search/">research</a>, and <a href="http://automl.chalearn.org/">competitions</a>), in May 2017 Google co-opted the term <em>AutoML</em> for its <a href="http://www.fast.ai/2018/07/16/auto-ml2/#nas">neural architecture search</a>. In blog posts accompanying announcements made at the conference Google I/O, Google CEO <a href="https://blog.google/technology/ai/making-ai-work-for-everyone/">Sundar Pichai wrote</a>, <i>“That’s <b>why we’ve created an approach called AutoML</b>, showing that it’s possible for neural nets to design neural nets”</i> and Google AI researchers <a href="https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html">Barret Zoph and Quoc Le wrote</a> <i>“In our approach (<b>which we call”AutoML”</b>), a controller neural net can propose a “child” model architecture…“</i></p>
<p>Google’s Cloud AutoML was <a href="https://techcrunch.com/2018/01/17/googles-automl-lets-you-train-custom-machine-learning-models-without-having-to-code/">announced in January 2018</a> as a suite of machine learning products. So far it consists of one publicly available product, <strong>AutoML Vision</strong>, an API that identifies or classifies objects in pictures. According to the <a href="https://cloud.google.com/automl/">product page</a>, Cloud AutoML Vision relies on two core techniques: <strong>transfer learning</strong> and <strong>neural architecture search</strong>. Since we’ve already explained <a href="http://www.fast.ai/2018/07/16/auto-ml2/#nas">neural architecture search</a>, let’s now take a look at transfer learning, and see how it relates to neural architecture search.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-23-automl3/automl-headlines.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Headlines from just a few of the many, many articles written about Google’s AutoML and Neural Architecture Search</figcaption><p></p>
</figure>
</div>
<p><em>Note: Google Cloud AutoML also has a <a href="https://twitter.com/math_rachel/status/1014301136902283264">drag-and-drop ML product</a> that is still in alpha. I applied for access to it over 2 months ago, but I have not heard back from Google yet. I plan to write a post once it’s released.</em></p>
<h2 id="transfer" class="anchored">
What is transfer learning?
</h2>
<p><a href="http://nlp.fast.ai/#transfer"><strong>Transfer learning</strong></a> is a powerful technique that lets people with smaller datasets or less computational power achieve state-of-the-art results, by taking advantage of pre-trained models that have been trained on similar, larger data sets. Because the model learned via transfer learning doesn’t have to learn from scratch, it can generally reach higher accuracy with much less data and computation time than models that don’t use transfer learning.</p>
<p>Transfer learning is a core technique that we use throughout our free <a href="http://course.fast.ai/">Practical Deep Learning for Coders</a> course– and that our students have been applying in production in everything from their own startups to Fortune 500 companies. Although transfer learning seems to be considered “less sexy” than neural architecture search, it is being used to achieve ground-breaking academic results, such as in Jeremy Howard and Sebastian Ruder’s <a href="http://nlp.fast.ai/">application of transfer learning to NLP</a>, which achieved state-of-the-art classification on 6 datasets and is serving as a basis for <a href="https://blog.openai.com/language-unsupervised/">further research in this area</a> at OpenAI.</p>
<h2 id="vs" class="anchored">
Neural architecture search vs.&nbsp;transfer learning: two opposing approaches
</h2>
<p>The underlying idea of transfer learning is that neural net architectures will generalize for similar types of problems: for example, that many images have underlying features (such as corners, circles, dog faces, or wheels) that show up in a variety of different types of images. In contrast, <strong>the underlying idea of promoting neural architecture search for every problem is the opposite</strong>: that each dataset has a unique, highly specialized architecture it will perform best with.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-23-automl3/features.png" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Examples from Matthew Zeiler and Rob Fergus of 4 features learned by image classifiers: corners, circles, dog faces, and wheels</figcaption><p></p>
</figure>
</div>
<p>When neural architecture search discovers a new architecture, you must learn weights for that architecture from scratch, while with transfer learning, you begin with existing weights from a pre-trained model. In this sense, you <strong>you can’t use</strong> neural architecture search and transfer learning on the same problem: if you’re learning a new architecture, you would need to train new weights for it; whereas if you are using transfer learning on a pretrained model, you can’t make substantial changes to the architecture.</p>
<p>Of course, you can apply transfer learning to an architecture learned through neural architecture search (which I think is a good idea!). This requires only that a few researchers use neural architecture search and open-source the models that they find. <strong>It is not necessary for all machine learning practitioners to be using neural architecture search themselves on all problems</strong> when they can instead use transfer learning. However, <a href="https://www.youtube.com/watch?v=kSa3UObNS6o">Jeff Dean’s keynote</a>, <a href="https://blog.google/technology/ai/making-ai-work-for-everyone/">Sundar Pichai’s blog post</a>, Google Cloud’s promotional materials, and the media coverage all suggest the opposite: that everybody needs to be able to use neural architecture search directly.</p>
<h2 id="good" class="anchored">
What Neural Architecture Search is good for
</h2>
<p>Neural architecture search is good for finding new architectures! Google’s <a href="https://arxiv.org/pdf/1802.01548.pdf">AmoebaNet</a> was learned via neural architecture search, and (with the inclusion of <a href="https://www.fast.ai/posts/2018-04-30-dawnbench-fastai.html">fast.ai advances</a> such as an aggressive learning schedule and changing the image size as training progresses) is now the <strong>cheapest way to train ImageNet on a single machine</strong>!</p>
<p>AmoebaNet was not designed with a reward function that involved the ability to scale, and so it didn’t scale as well as ResNet to multiple machines, but a neural net that scales well could potentially be learned in the future, optimized for different qualities.</p>
<h2 id="evidence" class="anchored">
In need of more evidence
</h2>
<p>We haven’t seen evidence that every dataset would be best modeled with its own custom model, as opposed to instead fine-tuning an existing model. Since neural architecture search requires a larger training set, this would particularly be an issue for smaller data sets. Even some of Google’s own research uses transferable techniques instead of finding a new architecture for each data set, such as <a href="https://arxiv.org/abs/1707.07012">NASNet</a> (<a href="https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html">blog post here</a>), which learned an architectural building block on Cifar10 and then used that building block to create an architecture for ImageNet. I don’t know of any widely-entered machine learning competitions that have been won using neural architectures search yet.</p>
<p>Furthermore, we don’t know that the mega-computationally expensive approach to neural architecture search that Google touts is the superior approach. For instance, more recent papers such as <a href="https://arxiv.org/pdf/1802.03268.pdf">Efficient Neural Architecture Search (ENAS)</a> and<br>
<a href="https://www.groundai.com/project/darts-differentiable-architecture-search/">Differentiable architecture search (DARTS)</a> propose significantly more efficient algorithms. <strong>DARTS takes just 4 GPU days</strong>, compared to <strong>1800 GPU days for NASNet</strong> and <strong>3150 GPU days for AmoebaNet</strong> (all learned to the same accuracy on Cifar-10). Jeff Dean is an author on the ENAS paper, which proposed a technique that is 1000x <strong>less</strong> computationally expensive, which seems inconsistent with his emphasis at the TF DevSummit one month later on using approaches that are 100x <strong>more</strong> computationally expensive.</p>
<h2 id="hype" class="anchored">
Then why all the hype about Google’s AutoML?
</h2>
<p>Given the above limitations, why has Google AutoML’s hype been so disproportionate to its proven usefulness (at least so far)? I think there are a few explanations:</p>
<ol type="1">
<li><p>Google’s AutoML highlights some of the <strong>dangers of having an academic research lab embedded in a for-profit corporation</strong>. There is a temptation to try to build products around interesting academic research, without assessing if they fulfill an actual need. This is also the story of many AI start-ups, such as MetaMind or Geometric Intelligence, that end up as acquihires without ever having produced a product. My <a href="https://www.fast.ai/posts/2018-01-08-startups.html">advice for startup founders</a> is to avoid productionizing your PhD thesis and to avoid hiring only academic researchers.</p></li>
<li><p><strong>Google excels at marketing</strong>. Artificial intelligence is seen as an inaccessible and intimidating field by many outsiders, who don’t feel that they have a way to evaluate claims, particularly from lionized companies like Google. Many journalists fall prey to this as well, and uncritically channel Google’s hype into glowing articles. I periodically talk to people that do not work in machine learning, yet are excited about various Google ML products that they’ve never used and can’t explain anything about.</p>
<p>One example of Google’s misleading coverage of its own achievements occurred when <a href="https://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html">Google AI researchers released</a> “a deep learning technology to reconstruct the true human genome”, compared their own work to Nobel prize-winning discoveries (the hubris!), and the story was picked up <a href="https://www.wired.com/story/google-is-giving-away-ai-that-can-build-your-genome-sequence/">by Wired</a>. However, Steven Salzberg, a distinguished professor of Biomedical Engineering, Computer Science, and Biostatistics at Johns Hopkins University <a href="https://www.forbes.com/sites/stevensalzberg/2017/12/11/no-googles-new-ai-cant-build-your-genome-sequence/#6d91bdd57747">debunked Google’s post</a>. Salzberg <a href="https://www.forbes.com/sites/stevensalzberg/2017/12/11/no-googles-new-ai-cant-build-your-genome-sequence/#6d91bdd57747">pointed out</a> that the research <b>didn’t actually reconstruct the human genome</b> and was “little more than an incremental improvement over existing software, and it might be even less than that.” A number of other <a href="https://twitter.com/math_rachel/status/940450068796022784">genomics researchers chimed in to agree</a> with Salzberg.</p>
<p>There is some great work happening at Google, but it would be easier to appreciate if we didn’t have to sift through so much misleading hype to figure out what is legitimate.</p>
<center>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
</p><p>Google’s DeepVariant “is little more than an incremental improvement over existing software, and it might be even less than that.” <a href="https://twitter.com/StevenSalzberg1?ref_src=twsrc%5Etfw"><span class="citation" data-cites="StevenSalzberg1">@StevenSalzberg1</span></a> <br><br>What do others genomics researchers think?<a href="https://t.co/vaAECQhvSi">https://t.co/vaAECQhvSi</a></p>
<p></p>
<p>— Rachel Thomas (<span class="citation" data-cites="math_rachel">@math_rachel</span>) <a href="https://twitter.com/math_rachel/status/940450068796022784?ref_src=twsrc%5Etfw">December 12, 2017</a></p>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center></li>
<li><p><strong>Google has a vested interest in convincing us that the key to effective use of deep learning is more computational power</strong>, because this is an area where they clearly beat the rest of us. AutoML is often very computationally expensive, such as in the examples of Google using 450 K40 GPUs for 7 days (the equivalent of 3150 GPU days) to learn AmoebaNet.</p>
<p>While <b>engineers and the media often drool over bare-metal power and anything bigger</b>, history has shown that innovation is often birthed instead by constraint and creativity. Google works on the biggest data possible using the most expensive computers possible; how well can this really <b>generalize to the problems that the rest of us face</b> living in a constrained world of limited resources?</p>
<p>Innovation comes from doing things differently, not from doing things bigger. The recent <a href="https://www.fast.ai/posts/2018-04-30-dawnbench-fastai.html">success of fast.ai</a> in the Stanford DAWNBench competition is one example of this.</p></li>
</ol>
<center>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Innovation come from doings things differently, not doing things bigger. <a href="https://twitter.com/jeremyphoward?ref_src=twsrc%5Etfw"><span class="citation" data-cites="jeremyphoward">@jeremyphoward</span></a> <a href="https://t.co/3TJYs8OCbr">https://t.co/3TJYs8OCbr</a> <a href="https://t.co/I55a6gT1OF">pic.twitter.com/I55a6gT1OF</a>
</p>
— Rachel Thomas (<span class="citation" data-cites="math_rachel">@math_rachel</span>) <a href="https://twitter.com/math_rachel/status/991726810000850944?ref_src=twsrc%5Etfw">May 2, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>
<h2 id="shortage" class="anchored">
How can we address the shortage of machine learning expertise?
</h2>
<p>To return to the issue that Jeff Dean raised in his <a href="https://www.youtube.com/watch?v=kSa3UObNS6o">TensorFlow DevSummit keynote</a> about the global shortage of machine learning practitioners, a different approach is possible. We can remove the biggest obstacles to using deep learning in several ways by:</p>
<ol type="1">
<li>making deep learning easier to use</li>
<li>debunking myths about what it takes <a href="https://www.fast.ai/posts/2017-11-16-what-you-need.html">to do deep learning</a></li>
<li>increasing access for people that lack the money or credit cards needed to use a cloud GPU</li>
</ol>
<h3 id="easier" class="anchored">
Making Deep Learning Easier to Use
</h3>
<p>Research to make deep learning easier to use has a huge impact, making it faster and simpler to train better networks. Examples of exciting discoveries that have now become standard practice are:</p>
<ul>
<li><strong>Dropout</strong> allows training on smaller datasets without over-fitting.</li>
<li><strong>Batch normalization</strong> allows for faster training.</li>
<li><strong>Rectified linear units</strong> help avoid gradient explosions.</li>
</ul>
<p>Newer research to improve ease of use includes: - The <a href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0">learning rate finder</a> makes the training process more robust. - <a href="http://www.fast.ai/2018/07/02/adam-weight-decay/">Super convergence</a> speeds up training, requiring fewer computational resources. - <a href="http://course.fast.ai/lessons/lesson8.html">“Custom heads”</a> for existing architectures (e.g.&nbsp;modifying ResNet, which was initially designed for classification, so that it can be used to find bounding boxes or perform style transfer) allow for easier architecture reuse across a range of problems.</p>
<p>None of the above discoveries involve bare-metal power; instead, all of them were creative ideas of ways to do things differently.</p>
<h3 id="myths" class="anchored">
Address Myths About What it Takes to Do Deep Learning
</h3>
<p>Another obstacle is the many myths that cause people to believe that deep learning isn’t for them: falsely believing that their data is too small, that they don’t have the right education or background, or that their computers aren’t big enough. One such myth says that only machine learning PhDs are capable of using deep learning, and many companies that can’t afford to hire expensive experts don’t even bother trying. However, it’s not only possible for companies to <a href="https://www.fast.ai/posts/2016-11-17-not-all-the-same.html">train the employees</a> they already have to become machine learning experts, it’s even preferable, because your current employees already have domain expertise for the area you work in!</p>
<p>For the vast majority of people I talk with, the <a href="https://www.fast.ai/posts/2017-11-16-what-you-need.html">barriers to entry</a> for deep learning are far <a href="https://www.technologyreview.com/video/610690/accessible-ai-expanding-the-reach-of-intelligent-solutions/">lower than they expected</a>: one year of coding experience and access to a GPU.</p>
<h3 id="colab" class="anchored">
Increasing Access: Google Colab Notebooks
</h3>
<p>Although the cost of cloud GPUs (around 50 cents per hour) are within the budgets of many of us, I’m periodically contacted by students from around the world that can’t afford <strong>any GPU use</strong> at all. In some countries, rules about banking and credit cards can make it difficult for students to use services like AWS, even when they have the money. <a href="https://colab.research.google.com/notebooks/welcome.ipynb">Google Colab notebooks</a> are a solution! Colab notebooks provide a Jupyter notebook environment that requires no setup to use, runs entirely in the cloud, and gives users access to a free GPU (although long-running GPU use is not allowed). They can also be used to create documentation that contains working code samples running in an interactive environment. Google colab notebooks will do much more to democratize deep learning than Google’s AutoML will; perhaps this would be a better target for Google’s marketing machine in the future.</p>
<p><strong>If you haven’t already, check out Part 1: <a href="http://www.fast.ai/2018/07/12/auto-ml-1/">What is it that machine learning practitioners do?</a> and Part 2: <a href="http://www.fast.ai/2018/07/16/auto-ml2/">An Opinionated Introduction to AutoML Neural Architecture Search</a> of this series.</strong></p>



 ]]></description>
  <category>technical</category>
  <category>machine learning</category>
  <guid>https://rachel.fast.ai/posts/2018-07-23-automl3/index.html</guid>
  <pubDate>Sun, 22 Jul 2018 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2018-07-23-automl3/sundar_pichai.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>An Opinionated Introduction to AutoML and Neural Architecture Search</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2018-07-16-automl2/index.html</link>
  <description><![CDATA[ 




<p><em>This is part 2 in a series. Check out <a href="https://rachel.fast.ai/posts/2018-07-12-automl1/">part 1 here</a> and <a href="http://www.fast.ai/2018/07/23/auto-ml-3/">part 3 here</a>.</em></p>
<p>Researchers from CMU and DeepMind recently released an interesting new paper, called <a href="https://www.groundai.com/project/darts-differentiable-architecture-search/">Differentiable Architecture Search (DARTS)</a>, offering an alternative approach to <em>neural architecture search</em>, a very hot area of machine learning right now. Neural architecture search has been heavily hyped in the last year, with Google’s CEO <a href="https://blog.google/technology/ai/making-ai-work-for-everyone/">Sundar Pichai</a> and Google’s Head of AI <a href="https://www.youtube.com/watch?v=kSa3UObNS6o">Jeff Dean</a> promoting the idea that <em>neural architecture search</em> and the <em>large amounts of computational power</em> it requires <strong>are essential to making machine learning available to the masses</strong>. Google’s work on neural architecture search has been widely and adoringly covered by the tech media (see <a href="https://nordic.businessinsider.com/google-has-started-using-ai-to-build-more-advanced-ai-2017-5/">here</a>, <a href="https://www.wired.com/story/googles-learning-software-learns-to-write-learning-software/">here</a>, <a href="https://www.zdnet.com/article/google-launches-cloud-automl-an-effort-to-simplify-and-automate-the-grunt-work-behind-ai-and-machine/">here</a>, and <a href="https://venturebeat.com/2018/05/09/googles-ai-chief-on-automl-autonomous-weapons-and-the-future/">here</a> for examples).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-16-automl2/automl-headlines.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Headlines from just a few of the many, many articles written about Google’s AutoML and Neural Architecture Search</figcaption><p></p>
</figure>
</div>
<p>During his <a href="https://www.youtube.com/watch?v=kSa3UObNS6o">keynote</a> (starts around 22:20) at the TensorFlow DevSummit in March 2018, Jeff Dean posited that perhaps in the future <strong>Google could replace machine learning expertise with 100x computational power</strong>. He gave computationally expensive <em>neural architecture search</em> as a primary example (the only example he gave) of why we need 100x computational power in order to make ML accessible to more people.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-16-automl2/jeff_dean.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Slide from Jeff Dean’s Keynote at the TensorFlow Dev Summit</figcaption><p></p>
</figure>
</div>
<p>What is neural architecture search? Is it the key to making machine learning available to non-machine learning experts? I will dig into these questions in this post, and in my next post, I will look specifically at Google’s AutoML. Neural architecture search is a part of a broader field called <em>AutoML</em>, which has also been receiving a lot of hype and which we will consider first.</p>
<p>Part 2 table of contents:</p>
<ul>
<li>
What is AutoML?
</li>
<li>
How useful is AutoML?
</li>
<li>
What is neural architecture search?
</li>
<li>
What about DARTS?
</li>
<li>
How useful is Neural Architecture Search?
</li>
<li>
How else could we make machine learning practitioners more effective?
</li>
</ul>
<h2 id="auto-ml" class="anchored">
What is AutoML?
</h2>
<p>The term AutoML has traditionally been used to describe <em>automated methods for model selection and/or hyperparameter optimization</em>. These methods exist for many types of algorithms, such as random forests, gradient boosting machines, neural networks, and more. The field of AutoML includes <a href="https://www.automl.org/automl/">open-source AutoML libraries</a>, <a href="https://www.automl.org/workshops/">workshops</a>, <a href="https://www.automl.org/automl/literature-on-neural-architecture-search/">research</a>, and <a href="http://automl.chalearn.org/">competitions</a>. Beginners often feel like they are just guessing as they test out different hyperparameters for a model, and automating the process could make this piece of the machine learning pipeline easier, as well as speeding things up even for experienced machine learning practitioners.</p>
<p>There are a number of AutoML libraries, the oldest of which is <a href="http://www.cs.ubc.ca/labs/beta/Projects/autoweka/">AutoWEKA</a>, which was first released in 2013 and automatically chooses a model and selects hyperparameters. Other notable AutoML libraries include <a href="http://automl.github.io/auto-sklearn/stable/">auto-sklearn</a> (which extends AutoWEKA to python), <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html">H2O AutoML</a>, and <a href="http://automl.info/tpot/">TPOT</a>. <a href="https://www.automl.org/">AutoML.org</a> (formerly known as ML4AAD, Machine Learning for Automated Algorithm Design) has been organzing <a href="https://www.automl.org/workshops/">AutoML workshops</a> at the academic machine learning conference <a href="https://icml.cc/">ICML</a> yearly since 2014.</p>
<h2 id="#useful-automl" class="anchored">
How useful is AutoML?
</h2>
<p>AutoML provides a way to select models and optimize hyper-parameters. It can also be useful in getting a baseline to know what level of performance is possible for a problem. So does this mean that data scientists can be replaced? Not yet, as we need to keep the context of <a href="https://rachel.fast.ai/posts/2018-07-12-automl1/"><em>what else it is that machine learning practitioners do</em></a>.</p>
<p>For many machine learning projects, choosing a model is just one piece of the <a href="http://www.fast.ai/2018/07/12/auto-ml-1/#complex">complex process</a> of building machine learning products. As I covered in my <a href="https://rachel.fast.ai/posts/2018-07-12-automl1/">previous post</a>, <a href="http://www.fast.ai/2018/07/12/auto-ml-1/#fail">projects can fail</a> if participants don’t see how interconnected the various parts of the pipeline are. I thought of <a href="http://www.fast.ai/2018/07/12/auto-ml-1/#steps">over 30 different steps</a> that can be involved in the process. I highlighted two of the most time-consuming aspects of machine learning (in particular, deep learning) as <strong>cleaning data</strong> (and <a href="http://www.fast.ai/2018/07/12/auto-ml-1/#steps">yes, this is an inseparable part of machine learning</a>) and <a href="http://www.fast.ai/2018/07/12/auto-ml-1/#training"><strong>training models</strong></a>. While AutoML can help with selecting a model and choosing hyperparameters, it is important to keep perspective on what other data expertise is still needed and on the difficult problems remain.</p>
<p>I will suggest some alternate approaches to AutoML for making machine learning practitioners more effective in the final section.</p>
<h2 id="nas" class="anchored">
What is neural architecture search?
</h2>
<p>Now that we’ve covered some of what AutoML is, let’s look at a particularly active subset of the field: <strong>neural architecture search</strong>. Google CEO Sundar Pichai <a href="https://blog.google/technology/ai/making-ai-work-for-everyone/">wrote that</a>, <i>“designing neural nets is extremely time intensive, and requires an expertise that limits its use to a smaller community of scientists and engineers. That’s why we’ve created an approach called AutoML, showing that <b> it’s possible for neural nets to design neural nets</b>.”</i></p>
<p>What Pichai refers to as using “neural nets to design neural nets” is known as <strong>neural architecture search</strong>; typically <strong>reinforcement learning</strong> or <strong>evolutionary algorithms</strong> are used to design the new neural net architectures. This is useful because it allows us to discover architectures far more complicated than what humans may think to try, and these architectures can be optimized for particular goals. Neural architecture search is often very computationally expensive.</p>
<p>To be precise, neural architecture search usually involves learning something like a layer (often called a “cell”) that can be assembled as a stack of repeated cells to create a neural network:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-16-automl2/evolvable-cell.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Diagram from Zoph et. al.&nbsp;2017. On the left is the full neural network of stacked cells, and on the right is the inside structure of a cell</figcaption><p></p>
</figure>
</div>
<p>The literature of <a href="http://www.ml4aad.org/automl/literature-on-neural-architecture-search/">academic papers on neural architecture search</a> is extensive, so I will highlight just a few recent papers here:</p>
<ul>
<li>The term <em>AutoML</em> jumped to “mainstream” prominence with <a href="https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html">work by Google AI researchers</a> (<a href="https://arxiv.org/pdf/1611.01578.pdf">paper here</a>) Quoc Le and Barret Zoph, which was featured <a href="https://www.technologyreview.com/s/607894/why-googles-ceo-is-excited-about-automating-artificial-intelligence/">at Google I/O in May 2017</a>. This work used reinforcement learning to find new architectures for the computer vision problem Cifar10 and the NLP problem Penn Tree Bank, and achieved similar results to existing architectures.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-16-automl2/automl_diagram.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Diagram from Le and Zoph’s blog post: the simpler architecture on the left was designed by a human and the more complicated architecture on the right was designed by a neural net.</figcaption><p></p>
</figure>
</div>
<ul>
<li><p><strong>NASNet</strong> from <a href="https://arxiv.org/abs/1707.07012">Learning Transferable Architectures for Scalable Image Recognition</a> (<a href="https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html">blog post here</a>). This work searches for an architectural building block on a small data set (Cifar10) and then builds an architecture for a large data set (ImageNet). This research was <strong>very computationally intensive</strong> with it taking 1800 GPU days (the equivalent of almost 5 years for 1 GPU) to learn the architecture (the team at Google used <strong>500 GPUs for 4 days</strong>!).</p></li>
<li><p><strong>AmoebaNet</strong> from <a href="https://arxiv.org/abs/1802.01548">Regularized Evolution for Image Classifier Architecture Search</a> This research was <strong>even more computationally intensive</strong> than NASNet, with it taking the equivalent of 3150 GPU days (the equivalent of almost 9 years for 1 GPU) to learn the architecture (the team at Google used 450 K40 GPUs for 7 days!). AmoebaNet consists of “cells” learned via an <em>evolutionary algorithm</em>, showing that artificially-evolved architectures can match or surpass human-crafted and reinforcement learning-designed image classifiers. After incorporating <a href="https://www.fast.ai/posts/2018-04-30-dawnbench-fastai.html">advances from fast.ai</a> such as an aggressive learning schedule and changing the image size as training progresses, AmoebaNet is now the <strong>cheapest way to train ImageNet on a single machine</strong>.</p></li>
<li><p><a href="https://arxiv.org/pdf/1802.03268.pdf"><strong>Efficient Neural Architecture Search (ENAS)</strong></a>: used much fewer GPU-hours than previously existing automatic model design approaches, and notably, was 1000x less expensive than standard Neural Architecture Search. This research was done using a single GPU for just 16 hours.</p></li>
</ul>
<h3 id="darts" class="anchored">
What about DARTS?
</h3>
<p><a href="https://www.groundai.com/project/darts-differentiable-architecture-search/">Differentiable architecture search (DARTS)</a>. This research was recently released from a team at Carnegie Mellon University and DeepMind, and I’m excited about the idea. DARTS assumes the space of candidate architectures is continuous, not discrete, and this allows it to use gradient-based aproaches, which are vastly more efficient than the inefficient black-box search used by most neural architecture search algorithms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-16-automl2/darts.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Diagram from DARTS, which treats the space of all possible architectures as continuous, not discrete</figcaption><p></p>
</figure>
</div>
<p>To learn a network for Cifar-10, <strong>DARTS takes just 4 GPU days</strong>, compared to <strong>1800 GPU days for NASNet</strong> and <strong>3150 GPU days for AmoebaNet</strong> (all learned to the same accuracy). This is a huge gain in efficiency! Although more exploration is needed, this is a promising research direction. Given how Google frequently equates neural architecture search with huge computational expense, efficient ways to do architecture search have most likely been under-explored.</p>
<h2 id="nas-useful" class="anchored">
How useful is Neural Architecture Search?
</h2>
<p>In his <a href="https://www.youtube.com/watch?v=kSa3UObNS6o">TensorFlow DevSummit keynote</a> (starts around 22:20), Jeff Dean suggested that a significant part of deep learning work is trying out different architectures. This was the only step of machine learning that Dean highlighted in his short talk, and I was surprised by his emphasis. Sundar Pichai’s <a href="https://blog.google/technology/ai/making-ai-work-for-everyone/">blog post</a> contained a similar assertion.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-16-automl2/neural_arch_search.jpg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Jeff Dean’s slide showing that neural architecture search can try 20 different models to find the most accurate</figcaption><p></p>
</figure>
</div>
<p>However, choosing a model is just one piece of the <a href="http://www.fast.ai/2018/07/12/auto-ml-1/#complex">complex process</a> of building machine learning products. In most cases, architecture selection is nowhere near the hardest, most time-consuming, or most significant part of the problem. <strong>Currently, there is no evidence that each new problem would be best modeled with it’s own unique architecture,</strong> and most practitioners consider it unlikely this will ever be the case.</p>
<p>Organizations like Google working on architecture design and sharing the architectures they discover with the rest of us are providing an important and helpful service. However, the underlying architecture search method is only needed for that tiny fraction of researchers that are working on foundational neural architecture design. The rest of us can just use the architectures they find via <a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html#transfer"><em>transfer learning</em></a>.</p>
<h2 id="vs" class="anchored">
How else could we make machine learning practitioners more effective? AutoML vs.&nbsp;Augmented ML
</h2>
<p>The field of <em>AutoML</em>, including <em>neural architecture search</em>, has been largely focused on the question: <b>how can we automate model selection and hyperparameter optimization?</b> However, automation ignores the important role of human input. I’d like to propose an alternate question: <strong>how can humans and computers work together to make machine learning more effective?</strong> The focus of <em>augmented ML</em> is on figuring out how a human and machine can best work together to take advantage of their different strengths.</p>
<p>An example of <em>augmented ML</em> is Leslie Smith’s <a href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0"><em>learning rate finder</em></a> (<a href="https://arxiv.org/abs/1506.01186">paper here</a>), which is implemented in the <a href="https://github.com/fastai/fastai">fastai library</a> (a high level API that sits on top of PyTorch) and taught as a key technique in our <a href="http://course.fast.ai/">free deep learning course</a>. The <em>learning rate</em> is a hyperparameter that can determine how quickly your model trains, or even whether it successfully trains at all. The learning rate finder allows a human to find a good learning rate in a single step, by looking at a generated chart. It’s faster than AutoML approaches to the same problem, improves the data scientist’s understanding of the training process, and encourages more powerful multi-step approaches to training models.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-16-automl2/learning_rate_finder.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Diagram from Surmenok’s blog post on the learning rate finder, showing relationship between learning rate and loss</figcaption><p></p>
</figure>
</div>
<p>There’s another problem with the focus on automating hyperparameter selection: it overlooks the possibility that some types of model are more widely useful, have fewer hyperparameters to tune, and are less sensitive to choice of hyperparameters. For example, a key benefit of random forests over gradient boosting machines (GBMs) is that random forests are more robust, whereas GBMs tend to be fairly sensitive to minor changes in hyperparameters. As a result, random forests are widely used in industry. <strong>Researching ways to effectively remove hyperparameters (through smarter defaults, or through new models) can have a huge impact.</strong> When I <a href="https://vimeo.com/214233053">first became interested</a> in deep learning in 2013, it was overwhelming to feel that there were such a huge number of hyperparameters, and I’m happy that newer research and tools has helped eliminate many of those (especially for beginners). For instance, in the fast.ai course, beginners start by only having to choose a single hyperparameter, the learning rate, and we even give you a tool to do that!</p>
<h2 id="end" class="anchored">
Stay tuned…
</h2>
<p>Now that we have an overview of what the fields of AutoML and neural architecture search are, we can take a closer look at Google’s AutoML in the next post.</p>
<p><strong>If you haven’t already, check out Part 1: <a href="http://www.fast.ai/2018/07/12/auto-ml-1/">What is it that machine learning practitioners do?</a> and Part 3: <a href="https://www.fast.ai/2018/07/23/auto-ml-3/">Google’s AutoML: Cutting Through the Hype</a> of this series.</strong></p>
<p><strong>Please be sure to check out Part 3 of this post next week!</strong></p>



 ]]></description>
  <category>technical</category>
  <category>machine learning</category>
  <guid>https://rachel.fast.ai/posts/2018-07-16-automl2/index.html</guid>
  <pubDate>Sun, 15 Jul 2018 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2018-07-16-automl2/automl-headlines.png" medium="image" type="image/png" height="60" width="144"/>
</item>
<item>
  <title>What do machine learning practitioners actually do?</title>
  <dc:creator>Rachel Thomas</dc:creator>
  <link>https://rachel.fast.ai/posts/2018-07-12-automl1/index.html</link>
  <description><![CDATA[ 




<p><em>This post is part 1 of a series. <a href="http://www.fast.ai/2018/07/16/auto-ml2/">Part 2</a> is an opinionated introduction to AutoML and neural architecture search, and <a href="http://www.fast.ai/2018/07/23/auto-ml-3/">Part 3</a> looks at Google’s AutoML in particular.</em></p>
<p>There are frequent media headlines about both the <strong>scarcity of machine learning talent</strong> (see <a href="https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html">here</a>, <a href="https://www.bloomberg.com/news/articles/2018-02-07/just-how-shallow-is-the-artificial-intelligence-talent-pool">here</a>, and <a href="https://towardsdatascience.com/ai-scientists-how-can-companies-deal-with-the-shortage-of-talent-11ab48566677">here</a>) and about the promises of companies <strong>claiming their products automate machine learning</strong> and eliminate the need for ML expertise altogether (see <a href="https://www.zdnet.com/article/google-launches-cloud-automl-an-effort-to-simplify-and-automate-the-grunt-work-behind-ai-and-machine/">here</a>, <a href="https://www.theverge.com/2018/6/12/17452742/deep-learning-ai-learn-lobe-made-easy-coding">here</a>, and <a href="https://www.engadget.com/2018/01/17/google-unveils-automl-ai-training-tool/">here</a>). In his keynote at the <a href="https://www.youtube.com/watch?v=kSa3UObNS6o">TensorFlow DevSummit</a>, Google’s head of AI Jeff Dean estimated that there are <strong>tens of millions</strong> of organizations that have electronic data that could be used for machine learning but <strong>lack the necessary expertise and skills</strong>. I follow these issues closely since <a href="https://www.forbes.com/sites/mariyayao/2017/04/10/why-we-need-to-democratize-ai-machine-learning-education/#4dd4ae321197">my work</a> <a href="https://www.technologyreview.com/s/610633/the-startup-diversifying-the-ai-workforce-beyond-just-techies/">at fast.ai</a> focuses on enabling more people to use machine learning and on making it easier to use.</p>
<p>In thinking about how we can automate some of the work of machine learning, as well as how to make it more accessible to people with a wider variety of backgrounds, it’s first necessary to ask, <em>what is it that machine learning practitioners do?</em> Any solution to the shortage of machine learning expertise requires answering this question: whether it’s so we know what skills to teach, what tools to build, or what processes to automate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-12-automl1/woc4.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">What do machine learning practitioners do? (Image Source: #WOCinTech Chat)</figcaption><p></p>
</figure>
</div>
<p>This post is the first in a 3-part series. It will address what it is that machine learning practitioners do, with Part 2 explaining AutoML and neural architecture search (which several high profile figures have suggested will be key to decreasing the need for data scientists) and Part 3 will cover Google’s heavily hyped AutoML product in particular.</p>
<h2 id="complex" class="anchored">
Building Data Products is Complex Work
</h2>
<p>While many academic machine learning sources focus almost exclusively on predictive modeling, that is just one piece of what machine learning practitioners do in the wild. The processes of appropriately framing a business problem, collecting and cleaning the data, building the model, implementing the result, and then monitoring for changes are interconnected in many ways that often make it hard to silo off just a single piece (without at least being aware of what the other pieces entail). As Jeremy Howard et al.&nbsp;wrote in <a href="https://www.oreilly.com/ideas/drivetrain-approach-data-products">Designing great data products</a>, <em>Great predictive modeling is an important part of the solution, but it no longer stands on its own; as products become more sophisticated, it disappears into the plumbing.</em></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-12-automl1/complicated-gears.gif" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Building Data Products is Complex Work (Source: Wikimedia Commons)</figcaption><p></p>
</figure>
</div>
<p>A team from Google, D. Sculley et al., wrote the classic <a href="https://ai.google/research/pubs/pub43146">Machine Learning: The High-Interest Credit Card of Technical Debt</a> about the code complexity and technical debt often created when using machine learning in practice. The authors identify a number of system-level interactions, risks, and anti-patterns, including:</p>
<ul>
<li><em>glue code</em>: massive amount of supporting code written to get data into and out of general-purpose packages</li>
<li><em>pipeline jungles</em>: the system for preparing data in an ML-friendly format may become a jungle of scrapes, joins, and sampling steps, often with intermediate files output</li>
<li>re-use input signals in ways that create unintended tight coupling of otherwise disjoint systems</li>
<li>risk that changes in the external world may make models or input signals change behavior in unintended ways, and these can be difficult to monitor</li>
</ul>
<p>The authors write, <i>A remarkable portion of real-world “machine learning” work is devoted to tackling issues of this form… It’s worth noting that glue code and pipeline jungles are symptomatic of integration issues that may have a <b>root cause in overly separated “research” and “engineering” roles</b>… It may be surprising to the academic community to know that <b>only a tiny fraction of the code in many machine learning systems is actually doing “machine learning”</b>.</i> (emphasis mine)</p>
<h2 id="fail" class="anchored">
When machine learning projects fail
</h2>
<p>In a <a href="https://www.fast.ai/posts/2016-12-08-org-structure.html">previous post</a>, I identified some failure modes in which machine learning projects are not effective in the workplace:</p>
<ul>
<li>The data science team builds really cool stuff that never gets used. There’s no buy-in from the rest of the organization for what they’re working on, and some of the data scientists don’t have a good sense of what can realistically be put into production.</li>
<li>There is a backlog with data scientists producing models much faster than there is engineering support to put them in production.</li>
<li>The data infrastructure engineers are separate from the data scientists. The pipelines don’t have the data the data scientists are asking for now, and the data scientists are under-utilizing the data sources the infrastructure engineers have collected.</li>
<li>The company has definitely decided on feature/product X. They need a data scientist to gather some data that supports this decision. The data scientist feels like the PM is ignoring data that contradicts the decision; the PM feels that the data scientist is ignoring other business logic.</li>
<li>The data science team interviews a candidate with impressive math modeling and engineering skills. Once hired, the candidate is embedded in a vertical product team that needs simple business analytics. The data scientist is bored and not utilizing their skills.</li>
</ul>
<p>I framed these as organizational failures in my original post, but they can also be described as various participants being overly focused on just one slice of the complex system that makes up a full data product. These are failures of communication and goal alignment between different parts of the data product pipeline.</p>
<h2 id="steps" class="anchored">
So, what do machine learning practitioners do?
</h2>
<p>As suggested above, building a machine learning product is a multi-faceted and complex task. Here are some of the things that machine learning practitioners may need to do during the process:</p>
<p><strong>Understanding the context</strong>:</p>
<ul>
<li>identify areas of the business that could benefit from machine learning</li>
<li>communicate with other stakeholders about what machine learning is and is not capable of (there are often many misconceptions)</li>
<li>develop understanding of business strategy, risks, and goals to make sure everyone is on the same page</li>
<li>identify what kind of data the organization has</li>
<li>appropriately frame and scope the task</li>
<li>understand operational constraints (e.g.&nbsp;what data is actually available at inference time)</li>
<li>proactively identify ethical risks, including how your work could be mis-used by harassers, trolls, authoritarian governments, or for propaganda/disinformation campaigns (and plan how to reduce these risks)</li>
<li>identify <a href="https://www.infoq.com/presentations/unconscious-bias-machine-learning">potential biases and potential negative feedback loops</a></li>
</ul>
<p><strong>Data</strong>: - make plans to collect more of different data (if needed and if possible) - stitch together data from many different sources: often this data has been collected in different formats or with inconsistent conventions - deal with missing or corrupted data - visualize the data - create appropriate <a href="https://www.fast.ai/posts/2017-11-13-validation-sets.html">training, validation, and test sets</a></p>
<p><strong>Modeling</strong>: - choose which model to use - fit model resource needs into constraints (e.g.&nbsp;will the completed model need to run on an edge device, in a low memory or high latency environment, etc) - choose hyperparameters (e.g.&nbsp;in the case of deep learning, this includes choosing an architecture, loss function, and optimizer) - train the model (and debug why it’s not training). This can involve: - adjusting hyperparmeters (e.g.&nbsp;such as the learning rate) - outputing intermediate results to see how the loss, training error, and validation error are changing with time - inspecting the data the model is wrong on to look for patterns - identifying underlying errors or issues with the data - realizing you need to change how you clean and pre-process the data - realizing you need more or different data augmentation - realizing you need more or different data - trying out different models - identifying if you are under- or over-fitting</p>
<p><strong>Productionize</strong>: - creating an API or web app with your model as an endpoint in order to productionize - exporting your model into the needed format - plan for how often your model will need to be retrained with updated data (e.g.&nbsp;perhaps you will retrain nightly or weekly)</p>
<p><strong>Monitor</strong>: - track model performance over time - monitor the input data, to identify if it changes with time in a way that would invalidate your model - communicate your results to the rest of the organization - have a plan in place for how you will monitor and respond to mistakes or unexpected consequences</p>
<p>Certainly, not every machine learning practitioner needs to do all of the above steps, but components of this process will be a part of many machine learning applications. Even if you are working on just a subset of these steps, a familiarity with the rest of the process will help ensure that you are not overlooking considerations that would keep your project from being successful!</p>
<h2 id="hard" class="anchored">
Two of the hardest parts of Machine Learning
</h2>
<p>For myself and many others I know, I would highlight two of the most time-consuming and frustrating aspects of machine learning (in particular, deep learning) as:</p>
<ol type="1">
<li>Dealing with <strong>data formatting, inconsistencies, and errors</strong> is often a messy and tedious process.</li>
<li>Training deep learning models is a <strong>notoriously brittle process</strong> right now.</li>
</ol>
<h3 id="cleaning" class="anchored">
Is cleaning data really part of ML? Yes.
</h3>
<p>Dealing with data formatting, inconsistencies, and errors is often a messy and tedious process. People will sometimes describe machine learning as separate from data science, as though for machine learning, you can just begin with your nicely cleaned, formatted data set. However, in my experience, the process of cleaning a data set and training a model are usually interwoven: I frequently find issues in the model training that cause me to go back and change the pre-processing for the input data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rachel.fast.ai/posts/2018-07-12-automl1/floppy-disks.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Dealing with messy and inconsistent data is necessary</figcaption><p></p>
</figure>
</div>
<h3 id="training" class="anchored">
Training Deep Learning Models is Brittle and Finicky (for now)
</h3>
<p>The difficulty of getting models to train deters many beginners, who often wind up feeling discouraged. Even experts frequently complain of <strong>how frustrating and fickle</strong> the training process can be. One AI researcher at <a href="https://www.fast.ai/posts/2018-04-10-stanford-salon.html">Stanford told me</a>, <em>I taught a course on deep learning and had all the students do their own projects. It was so hard. The students couldn’t get their models to train, and we were like “well, that’s deep learning”.</em> Ali Rahimi, an AI researcher with over a decade of experience and winner of the NIPS 2017 Test of Time Award, complained about the brittleness of training in his <a href="https://www.youtube.com/watch?v=ORHFOnaEzPc">NeurIPS award speech</a>. <em>How many of you have designed a deep net from scratch, built it from the ground up, architecture and all, and when it didn’t work, you felt bad about yourself?</em> Rahimi asked the audience of AI researchers, and many raised their hands. Rahimi continued, <em>This happens to me about every 3 months.</em></p>
<p>The fact that even AI experts sometimes have trouble training new models implies that the process has yet to be automated in a way where it could be incorporated into a general-purpose product. <strong>Some of the biggest advances in deep learning will come through discovering more robust training methods</strong>. We have already seen this some with advances like dropout, <a href="https://arxiv.org/abs/1708.07120">super convergence</a>, and transfer learning, all of which make training easier. Through the power of transfer learning (to be discussed in Part 3) training can be a robust process when defined for a narrow enough problem domain; however, we still have a ways to go in making training more robust in general.</p>
<h2 id="academic" class="anchored">
For Academic Researchers
</h2>
<p>Even if you are working on theoretical machine learning research, it is useful to understand the process that machine learning practitioners working on practical problems go through, as that might provide insights on what the most relevant or high-impact areas of research are.</p>
<p>As Googler engineers <a href="https://ai.google/research/pubs/pub43146">D. Sculley et al.&nbsp;wrote</a>, <i>Technical debt is an issue that both engineers and researchers need to be aware of. <b>Research solutions that provide a tiny accuracy benefit at the cost of massive increases in system complexity are rarely wise practice</b>… Paying down technical debt is not always as exciting as proving a new theorem, but it is a critical part of consistently strong innovation. And developing holistic, elegant solutions for complex machine learning systems is deeply rewarding work.</i> (emphasis mine)</p>
<h2 id="automl" class="anchored">
AutoML
</h2>
<p>Now that we have an overview of some of the tasks that machine learning practitioners do as part of their work, we are ready to evaluate attempts to automate this work. As it’s name suggests, <em>AutoML</em> is one field in particular that has focused on automating machine learning, and a subfield of AutoML called <em>neural architecture search</em> is currently receiving a ton of attention. In part 2, I will explain what AutoML and neural architecture search are, and in part 3, look at Google’s AutoML in particular.</p>
<p><strong>Be sure to check out Part 2: <a href="http://www.fast.ai/2018/07/16/auto-ml2/">An Opinionated Introduction to AutoML Neural Architecture Search</a>, and Part 3: <a href="https://www.fast.ai/2018/07/23/auto-ml-3/">Google’s AutoML: Cutting Through the Hype</a></strong></p>



 ]]></description>
  <category>machine learning</category>
  <category>advice</category>
  <category>work</category>
  <guid>https://rachel.fast.ai/posts/2018-07-12-automl1/index.html</guid>
  <pubDate>Wed, 11 Jul 2018 14:00:00 GMT</pubDate>
  <media:content url="https://rachel.fast.ai/posts/2018-07-12-automl1/woc4.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
